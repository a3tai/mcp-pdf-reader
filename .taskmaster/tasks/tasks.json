{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Setup Go Project Structure and MCP Server Foundation",
        "description": "Initialize the Go project with proper module structure, implement basic MCP server protocol handler, and establish the core architecture for PDF processing",
        "details": "Create Go module with go.mod (require go 1.21+). Implement MCP server following the Model Context Protocol specification:\n\n```go\n// main.go\npackage main\n\nimport (\n    \"github.com/yourusername/mcp-pdf-reader/internal/server\"\n    \"github.com/yourusername/mcp-pdf-reader/internal/pdf\"\n)\n\n// internal/server/mcp.go\ntype MCPServer struct {\n    pdfEngine *pdf.Engine\n    tools     map[string]Tool\n}\n\n// Implement MCP protocol methods: Initialize, ListTools, CallTool\n```\n\nSetup directory structure:\n- /cmd/mcp-pdf-reader/\n- /internal/server/ (MCP protocol handling)\n- /internal/pdf/ (PDF processing engine)\n- /internal/extractors/ (content extractors)\n- /pkg/models/ (data models)\n- /docs/examples/ (test PDFs)\n- /test/",
        "testStrategy": "Unit tests for MCP message handling, integration tests for server startup/shutdown, validate tool registration and basic protocol compliance. Test with mock PDF operations to ensure server responds correctly to MCP requests.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement Core PDF Parser with Validation",
        "description": "Build the foundational PDF parsing engine that can read, validate, and parse basic PDF file structure according to PDF 1.4/1.7 specifications",
        "details": "Implement PDF parser using pure Go (no CGO dependencies):\n\n```go\n// internal/pdf/parser.go\ntype PDFParser struct {\n    file     io.ReadSeeker\n    xref     *CrossReferenceTable\n    trailer  map[string]Object\n}\n\nfunc (p *PDFParser) Parse() error {\n    // 1. Read PDF header (%PDF-1.x)\n    // 2. Parse cross-reference table\n    // 3. Read trailer dictionary\n    // 4. Build object catalog\n}\n\n// internal/pdf/validator.go\nfunc ValidatePDF(path string) (*ValidationResult, error) {\n    // Check file signature\n    // Verify xref table\n    // Validate object structure\n}\n```\n\nConsider using pdfcpu (Apache 2.0 license) as a base library or reference implementation. Implement pdf_validate_file MCP tool.",
        "testStrategy": "Test with various PDF versions (1.4, 1.7), corrupted files, and edge cases. Validate against example PDFs in docs/examples/. Ensure proper error handling for malformed PDFs.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Build Content Stream Parser for Text Extraction",
        "description": "Implement PDF content stream parsing to extract basic text content, handling PDF operators and text positioning commands",
        "details": "Parse PDF content streams and implement text extraction:\n\n```go\n// internal/pdf/content_stream.go\ntype ContentStreamParser struct {\n    stream []byte\n    state  GraphicsState\n    text   []ExtractedText\n}\n\n// Handle PDF operators: BT, ET, Tf, Tm, Tj, TJ\nfunc (p *ContentStreamParser) parseOperator(op string, operands []Object) {\n    switch op {\n    case \"BT\": // Begin text\n    case \"Tf\": // Set font\n    case \"Tm\": // Text matrix\n    case \"Tj\": // Show text\n    }\n}\n\n// internal/extractors/text.go\nfunc ExtractText(page *PDFPage) (string, error) {\n    // Parse content stream\n    // Extract text runs\n    // Join into coherent text\n}\n```\n\nImplement pdf_read_file MCP tool for basic text extraction.",
        "testStrategy": "Test text extraction accuracy against known PDFs with various fonts, encodings, and layouts. Compare output with expected text content. Verify handling of special characters and Unicode.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Implement Coordinate System and Positioned Text Extraction",
        "description": "Add coordinate tracking, transformation matrix handling, and positioned text extraction with bounding boxes and formatting information",
        "details": "Enhance text extraction with positioning:\n\n```go\n// internal/pdf/coordinates.go\ntype TransformMatrix [6]float64\n\nfunc (tm TransformMatrix) Transform(x, y float64) (float64, float64) {\n    // Apply transformation matrix\n    return tm[0]*x + tm[2]*y + tm[4], tm[1]*x + tm[3]*y + tm[5]\n}\n\n// pkg/models/text.go\ntype ExtractedText struct {\n    Text   string      `json:\"text\"`\n    Page   int         `json:\"page\"`\n    Bounds BoundingBox `json:\"bounds\"`\n    Font   FontInfo    `json:\"font\"`\n}\n\n// internal/extractors/positioned_text.go\nfunc ExtractStructuredText(page *PDFPage) ([]ExtractedText, error) {\n    // Track current transformation matrix\n    // Calculate absolute positions\n    // Group into words/lines\n    // Preserve font information\n}\n```\n\nImplement pdf_extract_structured MCP tool.",
        "testStrategy": "Validate coordinate accuracy by extracting known positioned elements. Test transformation matrix calculations. Verify bounding boxes align with visual representation.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Develop Line and Paragraph Detection Algorithms",
        "description": "Implement intelligent grouping of positioned text into logical lines and paragraphs based on spatial analysis and proximity",
        "details": "Group text elements into semantic units:\n\n```go\n// internal/extractors/layout.go\ntype LineDetector struct {\n    tolerance float64 // Y-axis tolerance for same line\n}\n\nfunc (ld *LineDetector) GroupIntoLines(texts []ExtractedText) []TextLine {\n    // Sort by Y coordinate (top to bottom)\n    // Group texts with similar Y values\n    // Sort each line by X coordinate\n    // Handle RTL text if needed\n}\n\ntype ParagraphDetector struct {\n    lineSpacing float64\n    indentSize  float64\n}\n\nfunc (pd *ParagraphDetector) GroupIntoParagraphs(lines []TextLine) []Paragraph {\n    // Analyze line spacing\n    // Detect paragraph breaks\n    // Identify indentation patterns\n    // Group related lines\n}\n```\n\nEnhance pdf_extract_structured output with line/paragraph information.",
        "testStrategy": "Test with documents having various layouts: multi-column, mixed fonts, different line spacings. Verify correct paragraph detection and line ordering.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Implement Image and Graphics Extraction",
        "description": "Extract embedded images, vector graphics, and other non-text content with positioning information and metadata",
        "details": "Extract images and graphics from PDF:\n\n```go\n// internal/extractors/images.go\ntype ImageExtractor struct {\n    decoder map[string]ImageDecoder\n}\n\nfunc (ie *ImageExtractor) ExtractImages(page *PDFPage) ([]ExtractedImage, error) {\n    // Find image XObjects\n    // Decode image data (JPEG, PNG, etc.)\n    // Calculate positioning\n    // Extract metadata\n}\n\n// pkg/models/image.go\ntype ExtractedImage struct {\n    Page     int         `json:\"page\"`\n    Bounds   BoundingBox `json:\"bounds\"`\n    Format   string      `json:\"format\"`\n    Data     []byte      `json:\"data,omitempty\"`\n    DataURL  string      `json:\"dataUrl\"`\n    Metadata ImageMeta   `json:\"metadata\"`\n}\n\n// Handle inline images (BI/EI) and XObject images (Do)\n```\n\nImplement pdf_extract_images MCP tool.",
        "testStrategy": "Test with PDFs containing various image formats (JPEG, PNG, TIFF). Verify correct positioning and data extraction. Test inline vs XObject images.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Build Form Field Detection and Extraction",
        "description": "Detect and extract interactive form fields including text fields, checkboxes, radio buttons, and dropdowns with their values and properties",
        "details": "Extract form fields from AcroForms:\n\n```go\n// internal/extractors/forms.go\ntype FormExtractor struct {\n    catalog *PDFCatalog\n}\n\nfunc (fe *FormExtractor) ExtractForms(doc *PDFDocument) ([]FormField, error) {\n    // Get AcroForm dictionary\n    // Parse field tree\n    // Extract field properties\n    // Get current values\n}\n\n// pkg/models/form.go\ntype FormField struct {\n    Name       string      `json:\"name\"`\n    Type       string      `json:\"type\"` // text, checkbox, radio, select\n    Value      interface{} `json:\"value\"`\n    Options    []string    `json:\"options,omitempty\"`\n    Required   bool        `json:\"required\"`\n    Bounds     BoundingBox `json:\"bounds\"`\n    Validation Validation  `json:\"validation,omitempty\"`\n}\n```\n\nImplement pdf_extract_forms MCP tool.",
        "testStrategy": "Test with fillable PDF forms, both empty and filled. Verify field type detection, value extraction, and validation rules. Test nested field hierarchies.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement AcroForm Dictionary Parser",
            "description": "Create functionality to locate and parse the AcroForm dictionary from the PDF catalog, including handling of field tree structures and inheritance",
            "dependencies": [],
            "details": "Implement methods to: 1) Locate the AcroForm entry in the PDF catalog dictionary, 2) Parse the Fields array to build the field tree hierarchy, 3) Handle field inheritance where child fields inherit properties from parent fields, 4) Extract default appearance settings and form-level properties like NeedAppearances flag",
            "status": "done",
            "testStrategy": "Create unit tests with sample PDF catalog structures containing various AcroForm configurations, including nested field hierarchies and inherited properties"
          },
          {
            "id": 2,
            "title": "Build Field Type Detection and Property Extraction",
            "description": "Implement logic to identify field types (text, checkbox, radio, dropdown) and extract their specific properties including flags, default values, and appearance characteristics",
            "dependencies": [
              1
            ],
            "details": "Create type detection based on FT (Field Type) entry: 1) Tx for text fields, 2) Btn for buttons (checkboxes/radio), 3) Ch for choice fields (dropdowns/lists). Extract field flags to determine multiline, password, file select properties. Parse field dictionaries for properties like MaxLen, default value (DV), current value (V), and field name (T)",
            "status": "done",
            "testStrategy": "Test with PDFs containing each field type, verify correct type identification and property extraction including edge cases like missing optional properties"
          },
          {
            "id": 3,
            "title": "Implement Field Value and Options Extraction",
            "description": "Extract current values from form fields and parse options for choice fields (dropdowns, lists) including export values and display text",
            "dependencies": [
              2
            ],
            "details": "Parse V (value) entries handling different data types: strings for text fields, names for checkboxes/radio buttons, strings or arrays for choice fields. For choice fields, parse Opt array to extract available options as pairs of export values and display text. Handle special cases like multiple selection in list boxes",
            "status": "done",
            "testStrategy": "Create test cases with pre-filled forms, empty forms, and forms with various option configurations including multi-select lists"
          },
          {
            "id": 4,
            "title": "Extract Field Positioning and Validation Rules",
            "description": "Parse field appearance rectangles for positioning information and extract validation rules including format, range, and custom JavaScript validation",
            "dependencies": [
              2
            ],
            "details": "Extract Rect array from field or widget annotation to determine field bounds on page. Parse validation dictionary (V) for format (AFNumber_Format, AFDate_Format), keystroke validation, and range checks. Extract JavaScript actions from AA (additional actions) dictionary for custom validation. Map page coordinates to consistent coordinate system",
            "status": "done",
            "testStrategy": "Test with forms containing various validation rules, ensure correct coordinate extraction and transformation, verify JavaScript action detection"
          },
          {
            "id": 5,
            "title": "Implement pdf_extract_forms MCP Tool",
            "description": "Create the MCP tool interface that orchestrates the form extraction process and returns structured form field data in the specified JSON format",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Implement the pdf_extract_forms function that: 1) Accepts PDF document input, 2) Calls FormExtractor.ExtractForms to process the document, 3) Transforms extracted data into the FormField model structure, 4) Handles errors gracefully with appropriate error messages, 5) Returns JSON response with array of form fields including all properties, bounds, and validation rules",
            "status": "done",
            "testStrategy": "Integration tests with complete PDF forms, verify end-to-end extraction and JSON output format, test error handling for malformed PDFs"
          }
        ]
      },
      {
        "id": 8,
        "title": "Develop Table Detection Algorithm",
        "description": "Implement spatial analysis algorithms to detect and extract table structures with rows, columns, and cell content",
        "details": "Detect tables using spatial analysis:\n\n```go\n// internal/extractors/tables.go\ntype TableDetector struct {\n    minCellGap   float64\n    alignTolerance float64\n}\n\nfunc (td *TableDetector) DetectTables(texts []ExtractedText) ([]Table, error) {\n    // 1. Find aligned text clusters\n    // 2. Detect column boundaries\n    // 3. Identify row separations\n    // 4. Build cell matrix\n    // 5. Handle merged cells\n}\n\n// pkg/models/table.go\ntype Table struct {\n    Page       int         `json:\"page\"`\n    Bounds     BoundingBox `json:\"bounds\"`\n    Rows       int         `json:\"rows\"`\n    Columns    int         `json:\"columns\"`\n    Cells      [][]Cell    `json:\"cells\"`\n    Confidence float64     `json:\"confidence\"`\n}\n\n// Use heuristics: vertical alignment, consistent spacing, grid patterns\n```\n\nImplement pdf_extract_tables MCP tool.",
        "testStrategy": "Test with various table layouts: simple grids, merged cells, nested tables. Measure detection accuracy and confidence scores. Compare with ground truth annotations.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Implement Advanced Query Engine",
        "description": "Build a query system that allows searching and filtering PDF content by type, position, formatting, and text patterns",
        "details": "Create flexible query interface:\n\n```go\n// internal/query/engine.go\ntype QueryEngine struct {\n    index ContentIndex\n}\n\ntype Query struct {\n    Type      string      `json:\"type,omitempty\"` // text, image, table\n    Pattern   string      `json:\"pattern,omitempty\"`\n    Page      *int        `json:\"page,omitempty\"`\n    Region    *BoundingBox `json:\"region,omitempty\"`\n    FontSize  *float64    `json:\"fontSize,omitempty\"`\n}\n\nfunc (qe *QueryEngine) Query(doc *PDFDocument, query Query) ([]QueryResult, error) {\n    // Parse query parameters\n    // Filter by content type\n    // Apply spatial filters\n    // Match text patterns\n    // Score and rank results\n}\n\n// Support regex patterns, fuzzy matching, proximity search\n```\n\nImplement pdf_query_content MCP tool.",
        "testStrategy": "Test various query combinations: spatial queries, pattern matching, multi-criteria filters. Verify performance with large documents. Test edge cases and invalid queries.",
        "priority": "low",
        "dependencies": [
          6,
          8
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Create Document Intelligence and Analysis Layer",
        "description": "Implement document type classification, structure analysis, and comprehensive document intelligence features",
        "details": "Build document understanding capabilities:\n\n```go\n// internal/intelligence/analyzer.go\ntype DocumentAnalyzer struct {\n    extractors map[string]Extractor\n    classifier *DocumentClassifier\n}\n\nfunc (da *DocumentAnalyzer) Analyze(doc *PDFDocument) (*DocumentAnalysis, error) {\n    // Extract all content types\n    // Detect document structure\n    // Classify document type\n    // Map relationships\n    // Generate insights\n}\n\n// pkg/models/analysis.go\ntype DocumentAnalysis struct {\n    Type       string           `json:\"type\"` // invoice, report, form, etc.\n    Sections   []Section        `json:\"sections\"`\n    Statistics ContentStats     `json:\"statistics\"`\n    Quality    QualityMetrics   `json:\"quality\"`\n    Suggestions []string        `json:\"suggestions\"`\n}\n\n// Use heuristics and patterns for classification\n// Future: integrate ML models for better accuracy\n```\n\nImplement pdf_analyze_document MCP tool.",
        "testStrategy": "Test with diverse document types: invoices, reports, academic papers. Validate classification accuracy. Test section detection and relationship mapping.",
        "priority": "low",
        "dependencies": [
          8,
          9
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Document Content Extractors",
            "description": "Create modular extractors for different content types (text, tables, images, forms) that can parse and structure raw PDF content into analyzable formats",
            "dependencies": [],
            "details": "Create an Extractor interface and implement concrete extractors: TextExtractor for paragraphs and headers, TableExtractor for tabular data, ImageExtractor for embedded images, and FormExtractor for form fields. Each extractor should return structured data with position information and confidence scores.",
            "status": "done",
            "testStrategy": "Unit test each extractor with sample PDF content, verify extraction accuracy and position mapping, test edge cases like rotated text and complex tables"
          },
          {
            "id": 2,
            "title": "Build Document Structure Detection System",
            "description": "Implement algorithms to detect document sections, hierarchies, and logical structure including headers, paragraphs, lists, and their relationships",
            "dependencies": [
              1
            ],
            "details": "Create a StructureDetector that analyzes extracted content to identify document sections, build a hierarchical tree of content relationships, detect reading order, and map spatial relationships between elements. Use font sizes, positioning, and styling to infer structure.",
            "status": "done",
            "testStrategy": "Test with various document types (reports, articles, forms), verify correct hierarchy detection, validate reading order accuracy"
          },
          {
            "id": 3,
            "title": "Create Document Classification Engine",
            "description": "Develop a rule-based document classifier that can identify document types (invoice, report, form, contract, etc.) based on content patterns and structure",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement DocumentClassifier with pattern matching for keywords, structure signatures, and content heuristics. Create classification rules for common document types, implement confidence scoring, and support custom classification rules. Design for future ML model integration.",
            "status": "done",
            "testStrategy": "Test classification accuracy across document types, verify confidence scores, test with ambiguous documents"
          },
          {
            "id": 4,
            "title": "Implement Document Analysis and Insights Generation",
            "description": "Build the core DocumentAnalyzer that orchestrates extractors, structure detection, and classification to produce comprehensive document analysis with statistics and quality metrics",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Implement the DocumentAnalyzer.Analyze method to coordinate all components, calculate content statistics (word count, table count, image count), assess document quality metrics (readability, completeness, formatting consistency), and generate actionable suggestions for document improvement.",
            "status": "done",
            "testStrategy": "Integration test the full analysis pipeline, verify statistics accuracy, validate quality metrics against known documents"
          },
          {
            "id": 5,
            "title": "Create pdf_analyze_document MCP Tool",
            "description": "Implement the Model Context Protocol tool that exposes document analysis capabilities with proper request/response handling and error management",
            "dependencies": [
              4
            ],
            "details": "Create the MCP tool handler that accepts PDF document input, invokes the DocumentAnalyzer, formats the DocumentAnalysis response as JSON, handles errors gracefully, and provides detailed analysis results including type, sections, statistics, quality metrics, and suggestions.",
            "status": "done",
            "testStrategy": "Test MCP tool with various PDF inputs, verify JSON response structure, test error handling for invalid inputs"
          }
        ]
      },
      {
        "id": 11,
        "title": "Research and Integrate PDF Form Library (pdfcpu)",
        "description": "Research and integrate pdfcpu or similar Go library that provides proper AcroForm access according to PDF 1.4/1.7 standards, enabling robust form field extraction capabilities",
        "details": "Research and integrate a PDF library with AcroForm support:\n\n```go\n// internal/pdf/forms/library_adapter.go\ntype FormLibraryAdapter interface {\n    LoadPDF(reader io.Reader) error\n    GetAcroForm() (*AcroForm, error)\n    ExtractFormFields() ([]RawFormField, error)\n}\n\n// internal/pdf/forms/pdfcpu_adapter.go\ntype PDFCPUAdapter struct {\n    ctx *pdfcpu.Context\n}\n\nfunc (p *PDFCPUAdapter) LoadPDF(reader io.Reader) error {\n    // Initialize pdfcpu context\n    // Validate PDF structure\n    // Load form dictionary\n}\n\nfunc (p *PDFCPUAdapter) GetAcroForm() (*AcroForm, error) {\n    // Access AcroForm dictionary\n    // Parse field hierarchy\n    // Extract form metadata\n}\n\n// internal/pdf/forms/field_parser.go\ntype FieldParser struct {\n    adapter FormLibraryAdapter\n}\n\nfunc (fp *FieldParser) ParseField(dict pdfcpu.Dict) (*RawFormField, error) {\n    // Extract field type (FT)\n    // Get field name (T)\n    // Parse field flags (Ff)\n    // Extract default value (DV)\n    // Get current value (V)\n    // Parse appearance streams\n}\n```\n\nResearch considerations:\n1. Evaluate pdfcpu vs other libraries (unipdf, pdfium-go bindings)\n2. Ensure pure Go implementation (no CGO) for portability\n3. Verify PDF 1.4/1.7 standard compliance\n4. Check support for all form field types:\n   - Text fields (Tx)\n   - Checkboxes (Btn with checkbox flag)\n   - Radio buttons (Btn with radio flag)\n   - Dropdown/combo boxes (Ch)\n   - Signature fields (Sig)\n5. Validate handling of field inheritance and merged dictionaries\n6. Test performance with large forms\n\nIntegration approach:\n```go\n// internal/extractors/forms_enhanced.go\ntype EnhancedFormExtractor struct {\n    parser    *pdf.PDFParser\n    adapter   FormLibraryAdapter\n}\n\nfunc (efe *EnhancedFormExtractor) Extract(doc *PDFDocument) ([]FormField, error) {\n    // Use existing parser for basic structure\n    // Leverage library for AcroForm access\n    // Map library fields to our FormField model\n    // Handle field positioning and appearance\n}\n```",
        "testStrategy": "Create comprehensive test suite for library integration:\n\n1. Library evaluation tests:\n   - Test pdfcpu with sample forms from PDF reference\n   - Verify field extraction accuracy\n   - Benchmark performance vs existing implementation\n\n2. Integration tests:\n   - Test with forms containing all field types\n   - Verify field hierarchy parsing\n   - Test inherited field properties\n   - Validate appearance stream handling\n\n3. Compatibility tests:\n   - Test with PDF 1.4 and 1.7 forms\n   - Verify handling of XFA forms (if supported)\n   - Test encrypted/protected forms\n   - Validate Unicode field names and values\n\n4. Edge case tests:\n   - Malformed AcroForm dictionaries\n   - Missing required field properties\n   - Circular field references\n   - Large forms (1000+ fields)\n\n5. Regression tests:\n   - Ensure existing PDF parsing still works\n   - Verify no performance degradation\n   - Test memory usage with large forms",
        "status": "done",
        "dependencies": [
          2,
          7
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Implement AcroForm Dictionary Parser",
        "description": "Build a specialized parser to extract and process the AcroForm dictionary from PDF catalog, handling the complete form field hierarchy including field inheritance and widget annotations according to PDF 1.7 section 12.7",
        "details": "Implement comprehensive AcroForm dictionary parsing:\n\n```go\n// internal/pdf/forms/acroform_parser.go\ntype AcroFormParser struct {\n    catalog    *PDFCatalog\n    resolver   ObjectResolver\n}\n\ntype AcroForm struct {\n    Fields          []FieldDict    `json:\"fields\"`\n    NeedAppearances bool          `json:\"needAppearances\"`\n    SigFlags        int           `json:\"sigFlags\"`\n    CO              []ObjectRef   `json:\"co,omitempty\"`\n    DR              ResourceDict  `json:\"dr,omitempty\"`\n    DA              string        `json:\"da,omitempty\"`\n    Q               int           `json:\"q,omitempty\"`\n}\n\nfunc (p *AcroFormParser) ParseAcroForm() (*AcroForm, error) {\n    // 1. Get AcroForm entry from catalog\n    acroFormRef := p.catalog.Get(\"AcroForm\")\n    if acroFormRef == nil {\n        return nil, ErrNoAcroForm\n    }\n    \n    // 2. Resolve indirect reference\n    acroFormDict := p.resolver.Resolve(acroFormRef)\n    \n    // 3. Parse AcroForm dictionary entries\n    form := &AcroForm{}\n    \n    // Parse Fields array (required)\n    if fields := acroFormDict.Get(\"Fields\"); fields != nil {\n        form.Fields = p.parseFieldArray(fields)\n    }\n    \n    // Parse optional entries\n    form.NeedAppearances = acroFormDict.GetBool(\"NeedAppearances\", false)\n    form.SigFlags = acroFormDict.GetInt(\"SigFlags\", 0)\n    form.DA = acroFormDict.GetString(\"DA\", \"\")\n    form.Q = acroFormDict.GetInt(\"Q\", 0)\n    \n    return form, nil\n}\n\n// internal/pdf/forms/field_parser.go\ntype FieldDict struct {\n    Type       string              `json:\"type\"`       // FT entry\n    Parent     *ObjectRef          `json:\"parent,omitempty\"`\n    Kids       []FieldDict         `json:\"kids,omitempty\"`\n    T          string              `json:\"t\"`          // Partial field name\n    TU         string              `json:\"tu,omitempty\"` // Alternate name\n    TM         string              `json:\"tm,omitempty\"` // Mapping name\n    Ff         uint32              `json:\"ff,omitempty\"` // Field flags\n    V          interface{}         `json:\"v,omitempty\"`  // Field value\n    DV         interface{}         `json:\"dv,omitempty\"` // Default value\n    AA         map[string]Action   `json:\"aa,omitempty\"` // Additional actions\n    Widgets    []WidgetAnnotation  `json:\"widgets,omitempty\"`\n}\n\nfunc (p *AcroFormParser) parseFieldArray(fieldsObj Object) []FieldDict {\n    var fields []FieldDict\n    \n    if array, ok := fieldsObj.(ArrayObject); ok {\n        for _, fieldRef := range array {\n            field := p.parseFieldDict(fieldRef)\n            if field != nil {\n                fields = append(fields, *field)\n            }\n        }\n    }\n    \n    return fields\n}\n\nfunc (p *AcroFormParser) parseFieldDict(fieldRef Object) *FieldDict {\n    // Resolve indirect reference\n    fieldObj := p.resolver.Resolve(fieldRef)\n    dict, ok := fieldObj.(DictObject)\n    if !ok {\n        return nil\n    }\n    \n    field := &FieldDict{}\n    \n    // Parse field type (may be inherited)\n    field.Type = p.getInheritedValue(dict, \"FT\").(string)\n    \n    // Parse field name components\n    field.T = dict.GetString(\"T\", \"\")\n    field.TU = dict.GetString(\"TU\", \"\")\n    field.TM = dict.GetString(\"TM\", \"\")\n    \n    // Parse field flags (inheritable)\n    if ff := p.getInheritedValue(dict, \"Ff\"); ff != nil {\n        field.Ff = uint32(ff.(int))\n    }\n    \n    // Parse value and default value\n    field.V = p.parseFieldValue(dict.Get(\"V\"), field.Type)\n    field.DV = p.parseFieldValue(dict.Get(\"DV\"), field.Type)\n    \n    // Handle field hierarchy\n    if kids := dict.Get(\"Kids\"); kids != nil {\n        field.Kids = p.parseFieldArray(kids)\n    } else {\n        // Terminal field - parse widget annotations\n        field.Widgets = p.parseWidgetAnnotations(dict)\n    }\n    \n    return field\n}\n\n// Handle field inheritance according to PDF spec\nfunc (p *AcroFormParser) getInheritedValue(field DictObject, key string) interface{} {\n    // Check current field\n    if val := field.Get(key); val != nil {\n        return val\n    }\n    \n    // Check parent hierarchy\n    parent := field.Get(\"Parent\")\n    for parent != nil {\n        parentDict := p.resolver.Resolve(parent).(DictObject)\n        if val := parentDict.Get(key); val != nil {\n            return val\n        }\n        parent = parentDict.Get(\"Parent\")\n    }\n    \n    return nil\n}\n\n// internal/pdf/forms/widget_parser.go\ntype WidgetAnnotation struct {\n    Rect       Rectangle          `json:\"rect\"`\n    Page       int                `json:\"page\"`\n    AP         AppearanceDict     `json:\"ap,omitempty\"`\n    AS         string             `json:\"as,omitempty\"`\n    Border     []float64          `json:\"border,omitempty\"`\n    C          []float64          `json:\"c,omitempty\"`\n    StructParent int              `json:\"structParent,omitempty\"`\n}\n\nfunc (p *AcroFormParser) parseWidgetAnnotations(field DictObject) []WidgetAnnotation {\n    var widgets []WidgetAnnotation\n    \n    // Check if field is merged with widget\n    if rect := field.Get(\"Rect\"); rect != nil {\n        widget := p.parseWidgetAnnotation(field)\n        widgets = append(widgets, widget)\n    }\n    \n    // Check for separate widget annotations\n    if annots := field.Get(\"Kids\"); annots != nil {\n        // Parse child widgets\n        for _, annotRef := range annots.(ArrayObject) {\n            annotDict := p.resolver.Resolve(annotRef).(DictObject)\n            if annotDict.GetName(\"Subtype\") == \"Widget\" {\n                widget := p.parseWidgetAnnotation(annotDict)\n                widgets = append(widgets, widget)\n            }\n        }\n    }\n    \n    return widgets\n}\n\n// Field value parsing based on field type\nfunc (p *AcroFormParser) parseFieldValue(val Object, fieldType string) interface{} {\n    if val == nil {\n        return nil\n    }\n    \n    switch fieldType {\n    case \"Tx\": // Text field\n        return p.resolver.ResolveString(val)\n    case \"Ch\": // Choice field\n        if array, ok := val.(ArrayObject); ok {\n            var options []string\n            for _, opt := range array {\n                options = append(options, p.resolver.ResolveString(opt))\n            }\n            return options\n        }\n        return p.resolver.ResolveString(val)\n    case \"Btn\": // Button field\n        if name, ok := val.(NameObject); ok {\n            return string(name)\n        }\n        return nil\n    default:\n        return val\n    }\n}\n```\n\nKey implementation considerations:\n1. Handle field inheritance properly - FT, Ff, V, DV can be inherited from ancestors\n2. Support merged fields (field dict contains widget properties) and separate widgets\n3. Parse all field types: text (Tx), button (Btn), choice (Ch), signature (Sig)\n4. Handle field naming hierarchy with fully qualified names\n5. Support field collections and calculation order (CO array)\n6. Parse default resources (DR) and default appearance (DA)",
        "testStrategy": "Comprehensive testing for AcroForm dictionary parsing:\n\n1. **Basic AcroForm parsing tests**:\n   - Test with PDFs containing valid AcroForm dictionaries\n   - Verify correct parsing of all AcroForm entries (Fields, NeedAppearances, SigFlags, etc.)\n   - Test with missing optional entries\n   - Test with PDFs that have no forms (should return appropriate error)\n\n2. **Field hierarchy tests**:\n   - Create test PDFs with nested field hierarchies\n   - Verify field inheritance works correctly (FT, Ff, V, DV)\n   - Test fully qualified field name construction\n   - Test with deeply nested field trees (3+ levels)\n\n3. **Field type specific tests**:\n   - Text fields: single line, multiline, password, file select\n   - Button fields: push buttons, checkboxes, radio buttons\n   - Choice fields: list boxes, combo boxes, multi-select\n   - Signature fields with various states\n\n4. **Widget annotation tests**:\n   - Test merged field/widget dictionaries\n   - Test separate widget annotations in Kids array\n   - Verify correct page association for widgets\n   - Test appearance streams (AP dictionary)\n\n5. **Edge cases and error handling**:\n   - Circular references in field hierarchy\n   - Invalid field types\n   - Malformed field dictionaries\n   - Missing required entries\n   - Test with PDF 1.4 and 1.7 form variations\n\n6. **Integration tests**:\n   - Parse real-world PDF forms from Adobe, government forms, etc.\n   - Compare output with reference implementations\n   - Verify all field properties are correctly extracted\n   - Test performance with forms containing 100+ fields\n\nTest data should include example PDFs from PDF reference documentation and real-world forms with various complexity levels.",
        "status": "done",
        "dependencies": [
          2,
          11
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Create PDF Library Wrapper Interface",
        "description": "Design and implement an abstraction layer that provides a unified interface for PDF operations, allowing seamless switching between ledongthuc/pdf, pdfcpu, or custom implementations",
        "details": "Create a flexible PDF library wrapper that abstracts common operations:\n\n```go\n// internal/pdf/wrapper/interface.go\ntype PDFLibrary interface {\n    // Core operations\n    Open(reader io.Reader) (PDFDocument, error)\n    OpenFile(path string) (PDFDocument, error)\n    Validate() error\n    Close() error\n    \n    // Metadata operations\n    GetMetadata() (*Metadata, error)\n    GetPageCount() (int, error)\n    GetVersion() (string, error)\n    \n    // Content extraction\n    ExtractText(pageNum int) ([]TextElement, error)\n    ExtractImages(pageNum int) ([]ImageElement, error)\n    ExtractForms() ([]FormField, error)\n    \n    // Advanced operations\n    GetContentStream(pageNum int) ([]byte, error)\n    GetPageResources(pageNum int) (*Resources, error)\n    GetCatalog() (*Catalog, error)\n}\n\n// internal/pdf/wrapper/factory.go\ntype LibraryType string\n\nconst (\n    LibraryCustom    LibraryType = \"custom\"\n    LibraryPDFCPU    LibraryType = \"pdfcpu\"\n    LibraryLedongthuc LibraryType = \"ledongthuc\"\n)\n\ntype PDFLibraryFactory struct {\n    defaultLib LibraryType\n}\n\nfunc (f *PDFLibraryFactory) Create(libType LibraryType) (PDFLibrary, error) {\n    switch libType {\n    case LibraryCustom:\n        return &CustomPDFLibrary{}, nil\n    case LibraryPDFCPU:\n        return &PDFCPULibrary{}, nil\n    case LibraryLedongthuc:\n        return &LedongthucLibrary{}, nil\n    default:\n        return nil, fmt.Errorf(\"unknown library type: %s\", libType)\n    }\n}\n\n// internal/pdf/wrapper/custom.go\ntype CustomPDFLibrary struct {\n    parser *PDFParser\n    doc    *PDFDocument\n}\n\nfunc (c *CustomPDFLibrary) Open(reader io.Reader) (PDFDocument, error) {\n    // Use existing custom parser implementation\n    c.parser = NewPDFParser(reader)\n    return c.parser.Parse()\n}\n\n// internal/pdf/wrapper/pdfcpu.go\ntype PDFCPULibrary struct {\n    ctx *pdfcpu.Context\n}\n\nfunc (p *PDFCPULibrary) Open(reader io.Reader) (PDFDocument, error) {\n    // Wrap pdfcpu operations\n    config := pdfcpu.NewDefaultConfiguration()\n    ctx, err := pdfcpu.Read(reader, config)\n    if err != nil {\n        return nil, err\n    }\n    p.ctx = ctx\n    return p.wrapDocument(), nil\n}\n\n// internal/pdf/wrapper/ledongthuc.go\ntype LedongthucLibrary struct {\n    pdf *ledongthuc.PDF\n}\n\nfunc (l *LedongthucLibrary) Open(reader io.Reader) (PDFDocument, error) {\n    // Wrap ledongthuc/pdf operations\n    pdf, err := ledongthuc.NewPDF(reader)\n    if err != nil {\n        return nil, err\n    }\n    l.pdf = pdf\n    return l.wrapDocument(), nil\n}\n\n// Common types for unified interface\ntype PDFDocument interface {\n    GetPage(num int) (PDFPage, error)\n    GetPageCount() int\n    GetMetadata() map[string]string\n}\n\ntype TextElement struct {\n    Text     string\n    Position Point\n    Font     FontInfo\n    Size     float64\n}\n\n// Configuration for library selection\ntype WrapperConfig struct {\n    PreferredLibrary LibraryType\n    Fallbacks        []LibraryType\n    Features         []string // Required features for library selection\n}\n\n// Smart library selector based on document characteristics\nfunc SelectOptimalLibrary(doc io.Reader, config WrapperConfig) (PDFLibrary, error) {\n    // Analyze document to determine best library\n    // Check for specific features (forms, encryption, etc.)\n    // Return most suitable implementation\n}\n```\n\nImplement adapter pattern for each library to ensure consistent behavior and error handling across implementations.",
        "testStrategy": "Comprehensive testing strategy for the wrapper interface:\n\n1. **Interface compliance tests**:\n   - Verify all implementations satisfy PDFLibrary interface\n   - Test method signatures and return types\n   - Ensure consistent error types across implementations\n\n2. **Functional equivalence tests**:\n   - Create test suite that runs against all three implementations\n   - Compare extracted text, images, and metadata\n   - Verify identical results for same PDF inputs\n   - Test with PDFs from docs/examples/\n\n3. **Performance benchmarks**:\n   - Benchmark each implementation for common operations\n   - Compare memory usage and processing speed\n   - Test with large PDFs (100+ pages)\n\n4. **Feature coverage matrix**:\n   - Test form extraction capabilities per library\n   - Verify image extraction support\n   - Check Unicode and encoding handling\n   - Test encrypted PDF support\n\n5. **Fallback mechanism tests**:\n   - Test automatic fallback when preferred library fails\n   - Verify graceful degradation of features\n   - Test library selection logic\n\n6. **Integration tests**:\n   - Test wrapper with existing extractors\n   - Verify MCP server works with all implementations\n   - Test hot-swapping of libraries at runtime",
        "status": "done",
        "dependencies": [
          2,
          11
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Implement pdfcpu Backend for PDF Wrapper",
        "description": "Create a pdfcpu-based implementation of the PDFLibrary interface that provides robust AcroForm extraction capabilities with proper field type detection, value extraction, and property handling according to PDF specifications",
        "details": "Implement a complete pdfcpu backend for the PDF wrapper interface with focus on AcroForm functionality:\n\n```go\n// internal/pdf/wrapper/pdfcpu_backend.go\npackage wrapper\n\nimport (\n    \"github.com/pdfcpu/pdfcpu/pkg/api\"\n    \"github.com/pdfcpu/pdfcpu/pkg/pdfcpu\"\n    \"github.com/pdfcpu/pdfcpu/pkg/pdfcpu/model\"\n)\n\ntype PDFCPUBackend struct {\n    ctx      *model.Context\n    reader   io.Reader\n    filePath string\n}\n\nfunc NewPDFCPUBackend() *PDFCPUBackend {\n    return &PDFCPUBackend{}\n}\n\nfunc (p *PDFCPUBackend) Open(reader io.Reader) (PDFDocument, error) {\n    // Read PDF into memory\n    data, err := io.ReadAll(reader)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to read PDF: %w\", err)\n    }\n    \n    // Parse with pdfcpu\n    ctx, err := api.ReadContext(bytes.NewReader(data), pdfcpu.NewDefaultConfiguration())\n    if err != nil {\n        return nil, fmt.Errorf(\"pdfcpu parse error: %w\", err)\n    }\n    \n    p.ctx = ctx\n    p.reader = bytes.NewReader(data)\n    return p, nil\n}\n\n// Implement AcroForm extraction with full field support\nfunc (p *PDFCPUBackend) ExtractFormFields() ([]FormField, error) {\n    if p.ctx.AcroForm == nil {\n        return []FormField{}, nil\n    }\n    \n    fields := []FormField{}\n    \n    // Process field tree recursively\n    for _, fieldRef := range p.ctx.AcroForm.Fields {\n        field, err := p.processFieldTree(fieldRef)\n        if err != nil {\n            continue // Log error but continue processing\n        }\n        fields = append(fields, field...)\n    }\n    \n    return fields, nil\n}\n\nfunc (p *PDFCPUBackend) processFieldTree(fieldRef model.Object) ([]FormField, error) {\n    fieldDict, err := p.ctx.DereferenceDict(fieldRef)\n    if err != nil {\n        return nil, err\n    }\n    \n    fields := []FormField{}\n    \n    // Check if this is a terminal field or intermediate node\n    if kids := fieldDict.ArrayEntry(\"Kids\"); kids != nil {\n        // Process children\n        for _, kidRef := range kids {\n            childFields, err := p.processFieldTree(kidRef)\n            if err != nil {\n                continue\n            }\n            fields = append(fields, childFields...)\n        }\n    } else {\n        // Terminal field - extract properties\n        field, err := p.extractFieldProperties(fieldDict)\n        if err != nil {\n            return nil, err\n        }\n        fields = append(fields, *field)\n    }\n    \n    return fields, nil\n}\n\nfunc (p *PDFCPUBackend) extractFieldProperties(dict model.Dict) (*FormField, error) {\n    field := &FormField{\n        Properties: make(map[string]interface{}),\n    }\n    \n    // Extract field name (handle inheritance)\n    field.Name = p.getInheritedFieldName(dict)\n    \n    // Determine field type\n    ft := dict.NameEntry(\"FT\")\n    if ft == nil {\n        ft = p.getInheritedEntry(dict, \"FT\")\n    }\n    \n    switch ft.String() {\n    case \"Tx\":\n        field.Type = \"text\"\n        field.Properties[\"multiline\"] = dict.BooleanEntry(\"Ff\")&4096 != 0\n        field.Properties[\"password\"] = dict.BooleanEntry(\"Ff\")&8192 != 0\n        field.Properties[\"maxLength\"] = dict.IntEntry(\"MaxLen\")\n    case \"Btn\":\n        flags := dict.IntEntry(\"Ff\")\n        if flags&65536 != 0 { // Pushbutton\n            field.Type = \"button\"\n        } else if flags&32768 != 0 { // Radio\n            field.Type = \"radio\"\n        } else { // Checkbox\n            field.Type = \"checkbox\"\n        }\n    case \"Ch\":\n        flags := dict.IntEntry(\"Ff\")\n        if flags&131072 != 0 { // Combo\n            field.Type = \"combobox\"\n        } else {\n            field.Type = \"listbox\"\n        }\n        field.Properties[\"multiSelect\"] = flags&2097152 != 0\n        \n        // Extract options\n        if opt := dict.ArrayEntry(\"Opt\"); opt != nil {\n            field.Options = p.extractOptions(opt)\n        }\n    case \"Sig\":\n        field.Type = \"signature\"\n    }\n    \n    // Extract value\n    field.Value = p.extractFieldValue(dict, field.Type)\n    \n    // Extract common properties\n    field.Properties[\"readOnly\"] = dict.IntEntry(\"Ff\")&1 != 0\n    field.Properties[\"required\"] = dict.IntEntry(\"Ff\")&2 != 0\n    field.Properties[\"noExport\"] = dict.IntEntry(\"Ff\")&4 != 0\n    \n    // Extract appearance and position\n    if widgets := p.getWidgetAnnotations(dict); len(widgets) > 0 {\n        // Use first widget for position\n        widget := widgets[0]\n        if rect := widget.ArrayEntry(\"Rect\"); rect != nil && len(rect) == 4 {\n            field.Bounds = &BoundingBox{\n                X1: rect[0].(model.Float),\n                Y1: rect[1].(model.Float),\n                X2: rect[2].(model.Float),\n                Y2: rect[3].(model.Float),\n            }\n        }\n    }\n    \n    // Extract default value\n    if dv := dict.Entry(\"DV\"); dv != nil {\n        field.DefaultValue = p.objectToValue(dv)\n    }\n    \n    // Extract tooltip/alternate text\n    if tu := dict.StringEntry(\"TU\"); tu != nil {\n        field.Properties[\"tooltip\"] = *tu\n    }\n    \n    return field, nil\n}\n\nfunc (p *PDFCPUBackend) extractFieldValue(dict model.Dict, fieldType string) interface{} {\n    v := dict.Entry(\"V\")\n    if v == nil {\n        return nil\n    }\n    \n    switch fieldType {\n    case \"checkbox\":\n        // Check for /Yes or checked state\n        if name, ok := v.(model.Name); ok {\n            return name.String() == \"Yes\"\n        }\n    case \"radio\":\n        // Return the selected option name\n        if name, ok := v.(model.Name); ok {\n            return name.String()\n        }\n    case \"text\", \"combobox\":\n        // Return string value\n        if str := dict.StringEntry(\"V\"); str != nil {\n            return *str\n        }\n    case \"listbox\":\n        // Can be single or multiple values\n        if arr, ok := v.(model.Array); ok {\n            values := []string{}\n            for _, item := range arr {\n                if str, ok := item.(model.StringLiteral); ok {\n                    values = append(values, str.Value())\n                }\n            }\n            return values\n        } else if str := dict.StringEntry(\"V\"); str != nil {\n            return []string{*str}\n        }\n    }\n    \n    return p.objectToValue(v)\n}\n\n// Helper to convert PDF objects to Go values\nfunc (p *PDFCPUBackend) objectToValue(obj model.Object) interface{} {\n    switch v := obj.(type) {\n    case model.StringLiteral:\n        return v.Value()\n    case model.Name:\n        return v.String()\n    case model.Integer:\n        return int(v)\n    case model.Float:\n        return float64(v)\n    case model.Boolean:\n        return bool(v)\n    case model.Array:\n        arr := []interface{}{}\n        for _, item := range v {\n            arr = append(arr, p.objectToValue(item))\n        }\n        return arr\n    default:\n        return nil\n    }\n}\n\n// Implement other PDFLibrary interface methods\nfunc (p *PDFCPUBackend) ExtractText(pageNum int) ([]ExtractedText, error) {\n    // Use pdfcpu's text extraction\n    text, err := api.ExtractPageContent(p.ctx, pageNum)\n    if err != nil {\n        return nil, err\n    }\n    \n    // Convert to our format\n    // Note: pdfcpu may not provide positioned text, so this might need enhancement\n    return []ExtractedText{\n        {\n            Text: text,\n            Page: pageNum,\n        },\n    }, nil\n}\n\nfunc (p *PDFCPUBackend) GetMetadata() (*Metadata, error) {\n    info := p.ctx.Info\n    if info == nil {\n        return &Metadata{}, nil\n    }\n    \n    return &Metadata{\n        Title:        info.Title,\n        Author:       info.Author,\n        Subject:      info.Subject,\n        Keywords:     info.Keywords,\n        Creator:      info.Creator,\n        Producer:     info.Producer,\n        CreationDate: info.CreationDate,\n        ModDate:      info.ModDate,\n    }, nil\n}\n```\n\nAdditional implementation considerations:\n\n1. **Field inheritance handling**: PDF form fields can inherit properties from parent fields\n```go\nfunc (p *PDFCPUBackend) getInheritedFieldName(dict model.Dict) string {\n    parts := []string{}\n    current := dict\n    \n    for current != nil {\n        if t := current.StringEntry(\"T\"); t != nil {\n            parts = append([]string{*t}, parts...)\n        }\n        \n        // Move to parent\n        if parent := current.DictEntry(\"Parent\"); parent != nil {\n            current = parent\n        } else {\n            break\n        }\n    }\n    \n    return strings.Join(parts, \".\")\n}\n```\n\n2. **Widget annotation handling**: Form fields can have multiple widget annotations\n```go\nfunc (p *PDFCPUBackend) getWidgetAnnotations(fieldDict model.Dict) []model.Dict {\n    widgets := []model.Dict{}\n    \n    // Check if field dict is also a widget (merged)\n    if fieldDict.Entry(\"Subtype\") == model.Name(\"Widget\") {\n        widgets = append(widgets, fieldDict)\n    }\n    \n    // Check Kids for widgets\n    if kids := fieldDict.ArrayEntry(\"Kids\"); kids != nil {\n        for _, kid := range kids {\n            if kidDict, err := p.ctx.DereferenceDict(kid); err == nil {\n                if kidDict.Entry(\"Subtype\") == model.Name(\"Widget\") {\n                    widgets = append(widgets, kidDict)\n                }\n            }\n        }\n    }\n    \n    return widgets\n}\n```\n\n3. **Configuration and optimization**:\n```go\nfunc (p *PDFCPUBackend) Configure(opts PDFOptions) error {\n    config := pdfcpu.NewDefaultConfiguration()\n    config.ValidationMode = pdfcpu.ValidationRelaxed // For better compatibility\n    config.OptimizeDuplicateContentStreams = true\n    \n    // Apply custom options\n    if opts.StrictValidation {\n        config.ValidationMode = pdfcpu.ValidationStrict\n    }\n    \n    p.config = config\n    return nil\n}\n```",
        "testStrategy": "Comprehensive testing strategy for pdfcpu backend implementation:\n\n1. **Unit tests for field extraction**:\n   - Test each field type (text, checkbox, radio, combobox, listbox, signature)\n   - Verify correct value extraction for filled and empty fields\n   - Test field property extraction (required, readonly, multiline, etc.)\n   - Test field name inheritance from parent fields\n   - Test default value extraction\n\n2. **Integration tests with real PDFs**:\n   - Use IRS tax forms (complex field hierarchies)\n   - Test with Adobe sample forms from PDF reference\n   - Test with forms created by different PDF generators\n   - Verify extraction matches expected field structure\n\n3. **Edge case testing**:\n   - Forms with no AcroForm dictionary\n   - Malformed field dictionaries\n   - Fields with missing required entries\n   - Deeply nested field hierarchies\n   - Fields with multiple widget annotations\n   - Unicode field names and values\n\n4. **Performance benchmarks**:\n   - Compare extraction speed with native implementation\n   - Memory usage profiling for large forms\n   - Concurrent form extraction testing\n\n5. **Compatibility testing**:\n   - Test with PDF 1.4 through 2.0 forms\n   - XFA forms (should gracefully handle as unsupported)\n   - Hybrid AcroForm/XFA documents\n   - Encrypted PDFs with form fields\n\n6. **Validation against PDF specification**:\n   - Verify field flag interpretation matches PDF 1.7 spec\n   - Test appearance stream handling\n   - Validate field value formatting\n   - Check calculation order (CO array) handling\n\nTest implementation example:\n```go\nfunc TestPDFCPUFormExtraction(t *testing.T) {\n    backend := NewPDFCPUBackend()\n    \n    testCases := []struct {\n        name     string\n        pdfPath  string\n        expected []FormField\n    }{\n        {\n            name:    \"simple_text_form\",\n            pdfPath: \"testdata/forms/simple_text.pdf\",\n            expected: []FormField{\n                {\n                    Name:  \"name\",\n                    Type:  \"text\",\n                    Value: \"John Doe\",\n                    Properties: map[string]interface{}{\n                        \"maxLength\": 50,\n                    },\n                },\n            },\n        },\n        // More test cases...\n    }\n    \n    for _, tc := range testCases {\n        t.Run(tc.name, func(t *testing.T) {\n            file, _ := os.Open(tc.pdfPath)\n            defer file.Close()\n            \n            doc, err := backend.Open(file)\n            require.NoError(t, err)\n            \n            fields, err := doc.ExtractFormFields()\n            require.NoError(t, err)\n            \n            assert.Equal(t, tc.expected, fields)\n        })\n    }\n}\n```",
        "status": "done",
        "dependencies": [
          11,
          12,
          13
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Design and Implement Custom PDF Parser from Scratch",
        "description": "Build a complete PDF parser from scratch following PDF 1.4/1.7 specifications, providing full control over form extraction and document parsing without relying on external libraries",
        "details": "Implement a comprehensive PDF parser with complete control over parsing logic:\n\n```go\n// internal/pdf/custom/parser.go\npackage custom\n\ntype CustomPDFParser struct {\n    reader      io.ReadSeeker\n    version     string\n    xrefTable   *CrossReferenceTable\n    trailer     Dictionary\n    catalog     *Catalog\n    pageTree    *PageTree\n    objectCache map[ObjectID]PDFObject\n}\n\n// Core parsing methods\nfunc (p *CustomPDFParser) Parse() error {\n    // 1. Parse PDF header\n    if err := p.parseHeader(); err != nil {\n        return fmt.Errorf(\"header parse failed: %w\", err)\n    }\n    \n    // 2. Locate and parse xref table\n    if err := p.parseXRefTable(); err != nil {\n        return fmt.Errorf(\"xref parse failed: %w\", err)\n    }\n    \n    // 3. Parse trailer dictionary\n    if err := p.parseTrailer(); err != nil {\n        return fmt.Errorf(\"trailer parse failed: %w\", err)\n    }\n    \n    // 4. Load document catalog\n    if err := p.loadCatalog(); err != nil {\n        return fmt.Errorf(\"catalog load failed: %w\", err)\n    }\n    \n    return nil\n}\n\n// internal/pdf/custom/lexer.go\ntype PDFLexer struct {\n    reader   *bufio.Reader\n    position int64\n    buffer   []byte\n}\n\nfunc (l *PDFLexer) NextToken() (Token, error) {\n    // Skip whitespace\n    // Identify token type: number, string, name, array, dict, etc.\n    // Return parsed token\n}\n\n// internal/pdf/custom/objects.go\ntype PDFObject interface {\n    Type() ObjectType\n    String() string\n}\n\ntype Dictionary map[Name]PDFObject\ntype Array []PDFObject\ntype Stream struct {\n    Dict Dictionary\n    Data []byte\n}\n\n// internal/pdf/custom/acroform.go\ntype AcroFormParser struct {\n    parser     *CustomPDFParser\n    formDict   Dictionary\n    fieldCache map[string]*FormField\n}\n\nfunc (a *AcroFormParser) ParseAcroForm(catalog Dictionary) (*AcroForm, error) {\n    // Extract AcroForm dictionary\n    acroFormObj := catalog.Get(\"AcroForm\")\n    if acroFormObj == nil {\n        return nil, nil // No forms\n    }\n    \n    // Parse form dictionary\n    formDict, err := a.parser.resolveIndirectObject(acroFormObj)\n    if err != nil {\n        return err\n    }\n    \n    // Parse field tree\n    fields, err := a.parseFieldTree(formDict.Get(\"Fields\"))\n    if err != nil {\n        return err\n    }\n    \n    return &AcroForm{\n        Fields:          fields,\n        NeedAppearances: formDict.GetBool(\"NeedAppearances\"),\n        SigFlags:        formDict.GetInt(\"SigFlags\"),\n        CO:              a.parseCO(formDict.Get(\"CO\")),\n        DR:              a.parseResources(formDict.Get(\"DR\")),\n        DA:              formDict.GetString(\"DA\"),\n        Q:               formDict.GetInt(\"Q\"),\n    }, nil\n}\n\nfunc (a *AcroFormParser) parseFieldTree(fieldsObj PDFObject) ([]*FormField, error) {\n    // Handle field inheritance\n    // Parse field dictionaries recursively\n    // Resolve widget annotations\n    // Extract field values and properties\n}\n\n// internal/pdf/custom/content_stream.go\ntype ContentStreamParser struct {\n    lexer    *PDFLexer\n    graphics *GraphicsState\n    resources Dictionary\n}\n\nfunc (c *ContentStreamParser) Parse(stream []byte) ([]ContentObject, error) {\n    // Parse PDF content stream operators\n    // Handle text, graphics, and image operators\n    // Track graphics state changes\n    // Extract positioned content\n}\n\n// internal/pdf/custom/filters.go\ntype FilterDecoder interface {\n    Decode(data []byte, params Dictionary) ([]byte, error)\n}\n\nvar filterDecoders = map[string]FilterDecoder{\n    \"FlateDecode\":    &FlateDecoder{},\n    \"ASCIIHexDecode\": &ASCIIHexDecoder{},\n    \"ASCII85Decode\":  &ASCII85Decoder{},\n    \"LZWDecode\":      &LZWDecoder{},\n    \"RunLengthDecode\": &RunLengthDecoder{},\n}\n\n// internal/pdf/custom/encryption.go\ntype EncryptionHandler struct {\n    algorithm string\n    keyLength int\n    permissions uint32\n}\n\nfunc (e *EncryptionHandler) DecryptObject(obj PDFObject, objID ObjectID) (PDFObject, error) {\n    // Handle standard security handler\n    // Support RC4 and AES encryption\n    // Decrypt strings and streams\n}\n\n// Integration with wrapper interface\ntype CustomPDFBackend struct {\n    parser *CustomPDFParser\n}\n\nfunc (c *CustomPDFBackend) Open(reader io.Reader) (PDFDocument, error) {\n    seeker, ok := reader.(io.ReadSeeker)\n    if !ok {\n        // Buffer the reader if not seekable\n        data, err := io.ReadAll(reader)\n        if err != nil {\n            return nil, err\n        }\n        seeker = bytes.NewReader(data)\n    }\n    \n    parser := &CustomPDFParser{\n        reader:      seeker,\n        objectCache: make(map[ObjectID]PDFObject),\n    }\n    \n    if err := parser.Parse(); err != nil {\n        return nil, err\n    }\n    \n    return &customDocument{parser: parser}, nil\n}\n\nfunc (c *CustomPDFBackend) ExtractFormFields() ([]FormField, error) {\n    acroParser := &AcroFormParser{\n        parser:     c.parser,\n        fieldCache: make(map[string]*FormField),\n    }\n    \n    acroForm, err := acroParser.ParseAcroForm(c.parser.catalog)\n    if err != nil {\n        return nil, err\n    }\n    \n    if acroForm == nil {\n        return []FormField{}, nil\n    }\n    \n    return acroParser.ConvertToFormFields(acroForm.Fields)\n}",
        "testStrategy": "Comprehensive testing strategy for custom PDF parser:\n\n1. **PDF structure parsing tests**:\n   - Test header parsing for various PDF versions (1.0-1.7)\n   - Verify cross-reference table parsing (standard and compressed)\n   - Test trailer dictionary extraction\n   - Validate object parsing for all PDF object types\n\n2. **AcroForm parsing tests**:\n   - Test with PDF reference example forms\n   - Verify field inheritance handling\n   - Test all field types: text, checkbox, radio, combo, list, signature\n   - Validate field value extraction and formatting\n   - Test with nested field hierarchies\n\n3. **Content stream parsing tests**:\n   - Test all text operators (BT, ET, Tf, Tm, Tj, TJ, etc.)\n   - Verify graphics state tracking\n   - Test coordinate transformations\n   - Validate font and encoding handling\n\n4. **Filter and encryption tests**:\n   - Test all standard filters (Flate, ASCII85, etc.)\n   - Verify encryption/decryption with test PDFs\n   - Test password-protected documents\n\n5. **Integration tests**:\n   - Compare output with pdfcpu for same documents\n   - Verify form field extraction matches expected values\n   - Test with real-world PDF forms\n   - Performance benchmarks vs other implementations\n\n6. **Error handling tests**:\n   - Test with malformed PDFs\n   - Verify graceful handling of missing objects\n   - Test recovery from parsing errors",
        "status": "done",
        "dependencies": [
          2,
          12,
          13,
          "14"
        ],
        "priority": "low",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Implement Code Signing for Release Binaries",
        "description": "Set up automated code signing infrastructure for all platform binaries (macOS, Windows, Linux) with certificate management and GitHub Actions integration to ensure release artifacts are properly signed and trusted.",
        "details": "Implement comprehensive code signing solution for multi-platform releases:\n\n```yaml\n# .github/workflows/release.yml\nname: Release with Code Signing\non:\n  push:\n    tags:\n      - 'v*'\n\njobs:\n  sign-macos:\n    runs-on: macos-latest\n    steps:\n      - name: Import Apple Developer Certificate\n        env:\n          APPLE_CERT_BASE64: ${{ secrets.APPLE_CERT_BASE64 }}\n          APPLE_CERT_PASSWORD: ${{ secrets.APPLE_CERT_PASSWORD }}\n        run: |\n          echo \"$APPLE_CERT_BASE64\" | base64 --decode > certificate.p12\n          security create-keychain -p actions temp.keychain\n          security import certificate.p12 -k temp.keychain -P \"$APPLE_CERT_PASSWORD\" -T /usr/bin/codesign\n          security set-key-partition-list -S apple-tool:,apple:,codesign: -s -k actions temp.keychain\n      \n      - name: Sign macOS Binary\n        run: |\n          codesign --deep --force --verify --verbose \\\n            --sign \"${{ secrets.APPLE_DEVELOPER_ID }}\" \\\n            --options runtime \\\n            --entitlements entitlements.plist \\\n            ./dist/pdfextract-darwin-amd64\n          \n      - name: Notarize macOS Binary\n        run: |\n          xcrun altool --notarize-app \\\n            --primary-bundle-id \"com.pdfextract.cli\" \\\n            --username \"${{ secrets.APPLE_ID }}\" \\\n            --password \"${{ secrets.APPLE_APP_PASSWORD }}\" \\\n            --file ./dist/pdfextract-darwin-amd64.zip\n\n  sign-windows:\n    runs-on: windows-latest\n    steps:\n      - name: Setup Windows Code Signing\n        env:\n          WINDOWS_CERT_BASE64: ${{ secrets.WINDOWS_CERT_BASE64 }}\n          WINDOWS_CERT_PASSWORD: ${{ secrets.WINDOWS_CERT_PASSWORD }}\n        run: |\n          $cert = [System.Convert]::FromBase64String($env:WINDOWS_CERT_BASE64)\n          [System.IO.File]::WriteAllBytes(\"certificate.pfx\", $cert)\n          \n      - name: Sign Windows Binary\n        run: |\n          & \"C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64\\signtool.exe\" sign `\n            /f certificate.pfx `\n            /p $env:WINDOWS_CERT_PASSWORD `\n            /t http://timestamp.digicert.com `\n            /fd SHA256 `\n            /v .\\dist\\pdfextract-windows-amd64.exe\n\n  sign-linux:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Setup GPG Signing\n        env:\n          GPG_PRIVATE_KEY: ${{ secrets.GPG_PRIVATE_KEY }}\n          GPG_PASSPHRASE: ${{ secrets.GPG_PASSPHRASE }}\n        run: |\n          echo \"$GPG_PRIVATE_KEY\" | gpg --batch --import\n          echo \"$GPG_PASSPHRASE\" | gpg --batch --yes --passphrase-fd 0 --pinentry-mode loopback \\\n            --detach-sign --armor ./dist/pdfextract-linux-amd64\n```\n\nCreate certificate management scripts:\n\n```bash\n# scripts/cert-management/generate-certs.sh\n#!/bin/bash\n\n# Generate self-signed certificate for development\ngenerate_dev_cert() {\n    openssl req -x509 -newkey rsa:4096 -keyout dev-key.pem -out dev-cert.pem \\\n        -days 365 -nodes -subj \"/CN=PDFExtract Development\"\n}\n\n# Convert Apple certificate for GitHub Actions\nprepare_apple_cert() {\n    local p12_file=\"$1\"\n    base64 < \"$p12_file\" > apple-cert-base64.txt\n    echo \"Add contents of apple-cert-base64.txt to APPLE_CERT_BASE64 secret\"\n}\n\n# Convert Windows certificate\nprepare_windows_cert() {\n    local pfx_file=\"$1\"\n    base64 -w 0 < \"$pfx_file\" > windows-cert-base64.txt\n    echo \"Add contents of windows-cert-base64.txt to WINDOWS_CERT_BASE64 secret\"\n}\n```\n\nImplement signing verification:\n\n```go\n// internal/signing/verify.go\npackage signing\n\nimport (\n    \"crypto/x509\"\n    \"encoding/pem\"\n    \"fmt\"\n    \"os/exec\"\n    \"runtime\"\n)\n\ntype SignatureVerifier struct {\n    trustedCerts []*x509.Certificate\n}\n\nfunc (sv *SignatureVerifier) VerifyBinary(path string) error {\n    switch runtime.GOOS {\n    case \"darwin\":\n        return sv.verifyMacOS(path)\n    case \"windows\":\n        return sv.verifyWindows(path)\n    case \"linux\":\n        return sv.verifyLinux(path)\n    default:\n        return fmt.Errorf(\"unsupported platform: %s\", runtime.GOOS)\n    }\n}\n\nfunc (sv *SignatureVerifier) verifyMacOS(path string) error {\n    cmd := exec.Command(\"codesign\", \"--verify\", \"--deep\", \"--strict\", path)\n    output, err := cmd.CombinedOutput()\n    if err != nil {\n        return fmt.Errorf(\"codesign verification failed: %s\", output)\n    }\n    \n    // Check notarization status\n    cmd = exec.Command(\"spctl\", \"-a\", \"-v\", path)\n    output, err = cmd.CombinedOutput()\n    if err != nil {\n        return fmt.Errorf(\"notarization check failed: %s\", output)\n    }\n    return nil\n}\n\nfunc (sv *SignatureVerifier) verifyWindows(path string) error {\n    cmd := exec.Command(\"signtool\", \"verify\", \"/pa\", \"/v\", path)\n    output, err := cmd.CombinedOutput()\n    if err != nil {\n        return fmt.Errorf(\"signtool verification failed: %s\", output)\n    }\n    return nil\n}\n\nfunc (sv *SignatureVerifier) verifyLinux(path string) error {\n    // Verify GPG signature\n    sigPath := path + \".asc\"\n    cmd := exec.Command(\"gpg\", \"--verify\", sigPath, path)\n    output, err := cmd.CombinedOutput()\n    if err != nil {\n        return fmt.Errorf(\"GPG verification failed: %s\", output)\n    }\n    return nil\n}\n```\n\nCreate entitlements for macOS:\n\n```xml\n<!-- entitlements.plist -->\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>com.apple.security.cs.allow-unsigned-executable-memory</key>\n    <true/>\n    <key>com.apple.security.cs.disable-library-validation</key>\n    <true/>\n</dict>\n</plist>\n```\n\nDocumentation for certificate setup:\n\n```markdown\n# Code Signing Setup Guide\n\n## Prerequisites\n1. Apple Developer Account (for macOS)\n2. Windows Code Signing Certificate\n3. GPG Key (for Linux)\n\n## GitHub Actions Secrets Required\n\n### macOS\n- `APPLE_CERT_BASE64`: Base64 encoded .p12 certificate\n- `APPLE_CERT_PASSWORD`: Certificate password\n- `APPLE_DEVELOPER_ID`: Developer ID Application certificate name\n- `APPLE_ID`: Apple ID for notarization\n- `APPLE_APP_PASSWORD`: App-specific password\n\n### Windows\n- `WINDOWS_CERT_BASE64`: Base64 encoded .pfx certificate\n- `WINDOWS_CERT_PASSWORD`: Certificate password\n\n### Linux\n- `GPG_PRIVATE_KEY`: ASCII armored GPG private key\n- `GPG_PASSPHRASE`: GPG key passphrase\n\n## Certificate Generation Commands\n\n### macOS Development Certificate\n```bash\nsecurity create-keychain -p password build.keychain\nsecurity default-keychain -s build.keychain\nsecurity unlock-keychain -p password build.keychain\n```\n\n### Windows Self-Signed Certificate (Development)\n```powershell\nNew-SelfSignedCertificate -Type CodeSigningCert -Subject \"CN=PDFExtract Dev\" -KeyExportPolicy Exportable -CertStoreLocation Cert:\\CurrentUser\\My\n```\n\n### Linux GPG Key\n```bash\ngpg --full-generate-key\ngpg --armor --export-secret-keys YOUR_KEY_ID > private.asc\n```\n```",
        "testStrategy": "Comprehensive testing strategy for code signing implementation:\n\n1. **Certificate Management Tests**:\n   - Test certificate import/export scripts with dummy certificates\n   - Verify base64 encoding/decoding for all certificate types\n   - Test keychain creation and certificate import on macOS CI\n   - Verify certificate installation on Windows CI\n   - Test GPG key import on Linux CI\n\n2. **Signing Process Tests**:\n   - Create test binaries for each platform\n   - Test signing workflow locally with development certificates\n   - Verify signed binaries can be executed without security warnings\n   - Test notarization workflow on macOS (may require real Apple Developer account)\n   - Verify Windows Authenticode signatures with signtool verify\n   - Test GPG signature creation and verification on Linux\n\n3. **GitHub Actions Integration Tests**:\n   - Create test workflow that runs on push to test branch\n   - Verify secrets are properly accessed and decoded\n   - Test signing steps with self-signed certificates in CI\n   - Verify artifact upload includes signed binaries\n   - Test failure scenarios (missing secrets, invalid certificates)\n\n4. **Verification Tests**:\n   - Implement automated verification for each platform\n   - Test SignatureVerifier.VerifyBinary() with signed and unsigned binaries\n   - Verify error handling for tampered binaries\n   - Test cross-platform verification (e.g., verify Windows binary on Linux)\n\n5. **End-to-End Release Tests**:\n   - Create test release tag to trigger full workflow\n   - Download and test signed binaries on each platform\n   - Verify macOS Gatekeeper acceptance\n   - Test Windows SmartScreen behavior\n   - Verify GPG signature with public key\n\n6. **Security Tests**:\n   - Ensure certificates are not exposed in logs\n   - Verify temporary files are cleaned up\n   - Test certificate rotation procedures\n   - Verify signing timestamps for long-term validity",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Set Up Certificate Management Scripts",
            "description": "Create scripts to manage and convert certificates for macOS, Windows, and Linux platforms.",
            "dependencies": [],
            "details": "Implement scripts to generate development certificates and convert them to base64 for GitHub Actions.",
            "status": "done",
            "testStrategy": "Run the scripts to ensure certificates are generated and converted correctly."
          },
          {
            "id": 2,
            "title": "Configure GitHub Actions for Code Signing",
            "description": "Set up GitHub Actions workflows to automate code signing for macOS, Windows, and Linux binaries.",
            "dependencies": [
              1
            ],
            "details": "Implement workflows in .github/workflows/release.yml to handle code signing for each platform.",
            "status": "done",
            "testStrategy": "Trigger a release workflow and verify that binaries are signed correctly."
          },
          {
            "id": 3,
            "title": "Implement Signing Verification",
            "description": "Develop a verification system to ensure that signed binaries are valid and trusted.",
            "dependencies": [
              2
            ],
            "details": "Create a Go module to verify signatures on macOS, Windows, and Linux binaries.",
            "status": "done",
            "testStrategy": "Run verification tests on signed binaries to ensure they pass signature checks."
          },
          {
            "id": 4,
            "title": "Create macOS Entitlements",
            "description": "Define entitlements for macOS binaries to ensure proper execution and security compliance.",
            "dependencies": [
              2
            ],
            "details": "Develop an entitlements.plist file with necessary permissions for macOS applications.",
            "status": "done",
            "testStrategy": "Test the signed macOS binary to ensure it runs with the specified entitlements."
          },
          {
            "id": 5,
            "title": "Document Code Signing Setup",
            "description": "Create comprehensive documentation for setting up and managing code signing across platforms.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Write a guide detailing prerequisites, setup steps, and GitHub Actions secrets required for code signing.",
            "status": "done",
            "testStrategy": "Review the documentation for completeness and accuracy, ensuring it covers all necessary steps."
          }
        ]
      },
      {
        "id": 17,
        "title": "Fix GitHub Actions Release Workflow for Tag-Based Releases",
        "description": "Update the GitHub Actions release workflow to properly handle tag-triggered releases by replacing existing release information, automatically generating release notes from commit history between tags, and implementing proper draft/pre-release state management.",
        "details": "Implement comprehensive fixes to the GitHub Actions release workflow to handle tag-based releases properly:\n\n```yaml\n# .github/workflows/release.yml\nname: Release\non:\n  push:\n    tags:\n      - 'v*'\n\njobs:\n  create-release:\n    runs-on: ubuntu-latest\n    outputs:\n      release_id: ${{ steps.create_release.outputs.id }}\n      upload_url: ${{ steps.create_release.outputs.upload_url }}\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0  # Full history for changelog generation\n\n      - name: Get previous tag\n        id: prev_tag\n        run: |\n          CURRENT_TAG=${GITHUB_REF#refs/tags/}\n          PREVIOUS_TAG=$(git describe --tags --abbrev=0 $CURRENT_TAG^ 2>/dev/null || echo \"\")\n          echo \"current_tag=$CURRENT_TAG\" >> $GITHUB_OUTPUT\n          echo \"previous_tag=$PREVIOUS_TAG\" >> $GITHUB_OUTPUT\n\n      - name: Generate release notes\n        id: release_notes\n        run: |\n          CURRENT_TAG=${{ steps.prev_tag.outputs.current_tag }}\n          PREVIOUS_TAG=${{ steps.prev_tag.outputs.previous_tag }}\n          \n          # Generate changelog from commits\n          if [ -z \"$PREVIOUS_TAG\" ]; then\n            CHANGELOG=$(git log --pretty=format:\"- %s (%h)\" --reverse)\n          else\n            CHANGELOG=$(git log --pretty=format:\"- %s (%h)\" --reverse ${PREVIOUS_TAG}..${CURRENT_TAG})\n          fi\n          \n          # Group commits by type\n          FEATURES=$(echo \"$CHANGELOG\" | grep -E \"^- (feat|feature):\" || true)\n          FIXES=$(echo \"$CHANGELOG\" | grep -E \"^- (fix|bugfix):\" || true)\n          DOCS=$(echo \"$CHANGELOG\" | grep -E \"^- (docs|documentation):\" || true)\n          OTHER=$(echo \"$CHANGELOG\" | grep -vE \"^- (feat|feature|fix|bugfix|docs|documentation):\" || true)\n          \n          # Build release notes\n          NOTES=\"## What's Changed\"\n          \n          if [ -n \"$FEATURES\" ]; then\n            NOTES=\"$NOTES\\n\\n###  Features\\n$FEATURES\"\n          fi\n          \n          if [ -n \"$FIXES\" ]; then\n            NOTES=\"$NOTES\\n\\n###  Bug Fixes\\n$FIXES\"\n          fi\n          \n          if [ -n \"$DOCS\" ]; then\n            NOTES=\"$NOTES\\n\\n###  Documentation\\n$DOCS\"\n          fi\n          \n          if [ -n \"$OTHER\" ]; then\n            NOTES=\"$NOTES\\n\\n###  Other Changes\\n$OTHER\"\n          fi\n          \n          # Add comparison link\n          if [ -n \"$PREVIOUS_TAG\" ]; then\n            NOTES=\"$NOTES\\n\\n**Full Changelog**: https://github.com/${{ github.repository }}/compare/${PREVIOUS_TAG}...${CURRENT_TAG}\"\n          fi\n          \n          # Save to file to handle multiline content\n          echo -e \"$NOTES\" > release_notes.md\n          echo \"notes_file=release_notes.md\" >> $GITHUB_OUTPUT\n\n      - name: Check if release exists\n        id: check_release\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n          TAG=${{ steps.prev_tag.outputs.current_tag }}\n          RELEASE_ID=$(gh api repos/${{ github.repository }}/releases/tags/$TAG --jq '.id' 2>/dev/null || echo \"\")\n          echo \"release_id=$RELEASE_ID\" >> $GITHUB_OUTPUT\n          echo \"exists=$([[ -n \"$RELEASE_ID\" ]] && echo \"true\" || echo \"false\")\" >> $GITHUB_OUTPUT\n\n      - name: Delete existing release\n        if: steps.check_release.outputs.exists == 'true'\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n          gh api -X DELETE repos/${{ github.repository }}/releases/${{ steps.check_release.outputs.release_id }}\n          echo \"Deleted existing release for tag ${{ steps.prev_tag.outputs.current_tag }}\"\n\n      - name: Determine release type\n        id: release_type\n        run: |\n          TAG=${{ steps.prev_tag.outputs.current_tag }}\n          \n          # Check if pre-release (contains alpha, beta, rc, etc.)\n          if [[ \"$TAG\" =~ -(alpha|beta|rc|pre|preview|dev)\\. ]]; then\n            echo \"prerelease=true\" >> $GITHUB_OUTPUT\n            echo \"draft=false\" >> $GITHUB_OUTPUT\n          # Check if draft (contains draft or ends with -draft)\n          elif [[ \"$TAG\" =~ -draft$ ]] || [[ \"$TAG\" =~ -draft\\. ]]; then\n            echo \"prerelease=false\" >> $GITHUB_OUTPUT\n            echo \"draft=true\" >> $GITHUB_OUTPUT\n          else\n            echo \"prerelease=false\" >> $GITHUB_OUTPUT\n            echo \"draft=false\" >> $GITHUB_OUTPUT\n          fi\n\n      - name: Create release\n        id: create_release\n        uses: actions/create-release@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          tag_name: ${{ steps.prev_tag.outputs.current_tag }}\n          release_name: Release ${{ steps.prev_tag.outputs.current_tag }}\n          body_path: ${{ steps.release_notes.outputs.notes_file }}\n          draft: ${{ steps.release_type.outputs.draft }}\n          prerelease: ${{ steps.release_type.outputs.prerelease }}\n\n  build-and-upload:\n    needs: create-release\n    strategy:\n      matrix:\n        include:\n          - os: ubuntu-latest\n            asset_name: app-linux-amd64\n          - os: macos-latest\n            asset_name: app-darwin-amd64\n          - os: windows-latest\n            asset_name: app-windows-amd64.exe\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Build\n        run: |\n          # Build commands here\n          echo \"Building for ${{ matrix.os }}\"\n          \n      - name: Upload Release Asset\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          upload_url: ${{ needs.create-release.outputs.upload_url }}\n          asset_path: ./build/${{ matrix.asset_name }}\n          asset_name: ${{ matrix.asset_name }}\n          asset_content_type: application/octet-stream\n```\n\nAdditional considerations for the implementation:\n\n1. **Release Notes Generation Script** (optional standalone script):\n```bash\n#!/bin/bash\n# scripts/generate-release-notes.sh\n\nCURRENT_TAG=$1\nPREVIOUS_TAG=$2\nOUTPUT_FORMAT=${3:-markdown}\n\ngenerate_conventional_changelog() {\n    local from=$1\n    local to=$2\n    \n    # Define commit type mappings\n    declare -A type_headers=(\n        [\"feat\"]=\" Features\"\n        [\"fix\"]=\" Bug Fixes\"\n        [\"docs\"]=\" Documentation\"\n        [\"style\"]=\" Styles\"\n        [\"refactor\"]=\" Code Refactoring\"\n        [\"perf\"]=\" Performance Improvements\"\n        [\"test\"]=\" Tests\"\n        [\"build\"]=\" Build System\"\n        [\"ci\"]=\" Continuous Integration\"\n        [\"chore\"]=\" Chores\"\n        [\"revert\"]=\" Reverts\"\n    )\n    \n    # Extract and group commits\n    for type in \"${!type_headers[@]}\"; do\n        commits=$(git log --pretty=format:\"- %s (%h)\" --grep=\"^${type}:\" ${from}..${to})\n        if [ -n \"$commits\" ]; then\n            echo \"### ${type_headers[$type]}\"\n            echo \"$commits\"\n            echo \"\"\n        fi\n    done\n}\n```\n\n2. **GitHub API Integration for Advanced Features**:\n```yaml\n- name: Update release with additional metadata\n  env:\n    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n  run: |\n    # Add custom fields to release\n    gh api -X PATCH repos/${{ github.repository }}/releases/${{ steps.create_release.outputs.id }} \\\n      --field discussion_category_name=\"Releases\" \\\n      --field generate_release_notes=false\n```\n\n3. **Error Handling and Retry Logic**:\n```yaml\n- name: Create release with retry\n  uses: nick-invision/retry@v2\n  with:\n    timeout_minutes: 5\n    max_attempts: 3\n    retry_on: error\n    command: |\n      gh release create ${{ steps.prev_tag.outputs.current_tag }} \\\n        --title \"Release ${{ steps.prev_tag.outputs.current_tag }}\" \\\n        --notes-file release_notes.md \\\n        ${{ steps.release_type.outputs.draft == 'true' && '--draft' || '' }} \\\n        ${{ steps.release_type.outputs.prerelease == 'true' && '--prerelease' || '' }}\n```",
        "testStrategy": "Comprehensive testing strategy for the GitHub Actions release workflow:\n\n1. **Tag Detection and Parsing Tests**:\n   - Test with various tag formats: `v1.0.0`, `v2.1.0-beta.1`, `v3.0.0-rc.1`, `v1.0.0-draft`\n   - Verify correct detection of current and previous tags\n   - Test edge cases: first release (no previous tag), non-sequential tags\n   - Validate tag pattern matching for triggering workflow\n\n2. **Release Notes Generation Tests**:\n   - Create test repository with conventional commits between tags\n   - Verify correct grouping of commits by type (feat, fix, docs, etc.)\n   - Test with empty commit ranges and single commit releases\n   - Validate markdown formatting and special character escaping\n   - Test commit message parsing with various formats\n\n3. **Release Management Tests**:\n   - Test detection of existing releases for a tag\n   - Verify successful deletion of existing releases before recreation\n   - Test API error handling when release doesn't exist\n   - Validate release creation with correct metadata\n\n4. **Draft/Pre-release Logic Tests**:\n   - Test tags with pre-release identifiers: `-alpha`, `-beta`, `-rc`, `-pre`\n   - Verify draft detection for tags ending with `-draft`\n   - Test standard release tags create non-draft, non-prerelease\n   - Validate boolean flag propagation to release creation\n\n5. **Integration Tests**:\n   - Create test workflow that triggers on push to test tags\n   - Verify complete workflow execution from tag push to release creation\n   - Test with multiple platform builds uploading to same release\n   - Validate asset upload URLs are correctly passed between jobs\n\n6. **Error Handling Tests**:\n   - Test behavior when GitHub API rate limits are hit\n   - Verify graceful handling of network failures\n   - Test with invalid GitHub tokens\n   - Validate workflow behavior when previous steps fail\n\n7. **Manual Testing Checklist**:\n   - Push a new version tag and verify release is created\n   - Push same tag again and verify old release is replaced\n   - Test with pre-release tag (e.g., `v1.0.0-beta.1`)\n   - Test with draft tag (e.g., `v1.0.0-draft`)\n   - Verify release notes contain commits since last tag\n   - Check that release assets are properly uploaded\n   - Validate release appears correctly in GitHub UI",
        "status": "done",
        "dependencies": [
          16
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Release Workflow to Handle Existing Releases",
            "description": "Modify the GitHub Actions workflow to check for existing releases and delete them if necessary before creating a new release.",
            "dependencies": [],
            "details": "Implement a step in the workflow to check if a release already exists for the current tag and delete it if it does.",
            "status": "done",
            "testStrategy": "Test by creating a release with an existing tag and ensure it is deleted and replaced with a new release."
          },
          {
            "id": 2,
            "title": "Implement Release Notes Generation",
            "description": "Enhance the workflow to automatically generate release notes from commit history between tags.",
            "dependencies": [
              1
            ],
            "details": "Use git log to extract commit messages and categorize them into features, fixes, and other changes for release notes.",
            "status": "done",
            "testStrategy": "Verify that release notes are correctly generated and formatted based on commit messages."
          },
          {
            "id": 3,
            "title": "Add Draft and Pre-release State Management",
            "description": "Implement logic to determine if a release should be a draft or pre-release based on the tag name.",
            "dependencies": [
              2
            ],
            "details": "Use regex to check the tag name for indicators of draft or pre-release status and set the release state accordingly.",
            "status": "done",
            "testStrategy": "Test with various tag names to ensure the correct release state is applied."
          },
          {
            "id": 4,
            "title": "Integrate GitHub API for Metadata Updates",
            "description": "Use the GitHub API to add additional metadata to releases, such as discussion categories.",
            "dependencies": [
              3
            ],
            "details": "Implement a step in the workflow to update the release with additional metadata using the GitHub API.",
            "status": "done",
            "testStrategy": "Check that the release metadata is correctly updated by inspecting the release details on GitHub."
          },
          {
            "id": 5,
            "title": "Implement Error Handling and Retry Logic",
            "description": "Add error handling and retry logic to the release creation process to ensure robustness.",
            "dependencies": [
              4
            ],
            "details": "Use a retry mechanism to handle transient errors during release creation, with configurable timeout and retry attempts.",
            "status": "done",
            "testStrategy": "Simulate errors during release creation and verify that the retry logic successfully handles them."
          }
        ]
      },
      {
        "id": 18,
        "title": "Create Automated Release Preparation System",
        "description": "Design and implement a comprehensive release automation system that handles version tagging, changelog generation from commit history, release note compilation, and orchestrates the complete release process including pre-release checks and post-release notifications.",
        "details": "Implement a complete release preparation system with multiple components:\n\n```go\n// internal/release/manager.go\npackage release\n\ntype ReleaseManager struct {\n    git         *GitClient\n    changelog   *ChangelogGenerator\n    versioning  *VersionManager\n    validator   *ReleaseValidator\n    notifier    *ReleaseNotifier\n}\n\nfunc (rm *ReleaseManager) PrepareRelease(version string, releaseType ReleaseType) (*Release, error) {\n    // 1. Validate version format\n    if err := rm.versioning.ValidateVersion(version); err != nil {\n        return nil, fmt.Errorf(\"invalid version: %w\", err)\n    }\n    \n    // 2. Run pre-release checks\n    if err := rm.validator.RunPreReleaseChecks(); err != nil {\n        return nil, fmt.Errorf(\"pre-release checks failed: %w\", err)\n    }\n    \n    // 3. Generate changelog\n    changelog, err := rm.changelog.Generate(version)\n    if err != nil {\n        return nil, fmt.Errorf(\"changelog generation failed: %w\", err)\n    }\n    \n    // 4. Create release object\n    release := &Release{\n        Version:     version,\n        Type:        releaseType,\n        Changelog:   changelog,\n        CreatedAt:   time.Now(),\n    }\n    \n    return release, nil\n}\n\n// internal/release/changelog.go\ntype ChangelogGenerator struct {\n    git    *GitClient\n    parser *CommitParser\n}\n\nfunc (cg *ChangelogGenerator) Generate(targetVersion string) (*Changelog, error) {\n    // Get previous version tag\n    prevTag, err := cg.git.GetPreviousTag(targetVersion)\n    if err != nil {\n        prevTag = \"\" // First release\n    }\n    \n    // Get commits between tags\n    commits, err := cg.git.GetCommitsBetween(prevTag, \"HEAD\")\n    if err != nil {\n        return nil, err\n    }\n    \n    // Parse and categorize commits\n    changelog := &Changelog{\n        Version:  targetVersion,\n        Date:     time.Now(),\n        Sections: make(map[string][]ChangeEntry),\n    }\n    \n    for _, commit := range commits {\n        entry := cg.parser.ParseCommit(commit)\n        if entry != nil {\n            changelog.AddEntry(entry)\n        }\n    }\n    \n    return changelog, nil\n}\n\n// internal/release/version.go\ntype VersionManager struct {\n    strategy VersionStrategy\n}\n\nfunc (vm *VersionManager) NextVersion(currentVersion string, bumpType BumpType) (string, error) {\n    current, err := semver.Parse(currentVersion)\n    if err != nil {\n        return \"\", err\n    }\n    \n    switch bumpType {\n    case BumpMajor:\n        return fmt.Sprintf(\"v%d.0.0\", current.Major+1), nil\n    case BumpMinor:\n        return fmt.Sprintf(\"v%d.%d.0\", current.Major, current.Minor+1), nil\n    case BumpPatch:\n        return fmt.Sprintf(\"v%d.%d.%d\", current.Major, current.Minor, current.Patch+1), nil\n    }\n    \n    return \"\", fmt.Errorf(\"unknown bump type: %v\", bumpType)\n}\n\n// internal/release/validator.go\ntype ReleaseValidator struct {\n    checks []ReleaseCheck\n}\n\nfunc (rv *ReleaseValidator) RunPreReleaseChecks() error {\n    for _, check := range rv.checks {\n        if err := check.Run(); err != nil {\n            return fmt.Errorf(\"check '%s' failed: %w\", check.Name(), err)\n        }\n    }\n    return nil\n}\n\n// Implement various checks\ntype TestsPassCheck struct{}\nfunc (t *TestsPassCheck) Run() error {\n    // Run go test ./...\n    cmd := exec.Command(\"go\", \"test\", \"./...\")\n    if err := cmd.Run(); err != nil {\n        return fmt.Errorf(\"tests failed\")\n    }\n    return nil\n}\n\ntype BuildCheck struct{}\nfunc (b *BuildCheck) Run() error {\n    // Verify build succeeds\n    cmd := exec.Command(\"go\", \"build\", \"./...\")\n    return cmd.Run()\n}\n\n// cmd/release/main.go\npackage main\n\nimport (\n    \"github.com/spf13/cobra\"\n)\n\nfunc main() {\n    rootCmd := &cobra.Command{\n        Use:   \"release\",\n        Short: \"Automated release preparation tool\",\n    }\n    \n    prepareCmd := &cobra.Command{\n        Use:   \"prepare [version]\",\n        Short: \"Prepare a new release\",\n        RunE: func(cmd *cobra.Command, args []string) error {\n            manager := release.NewManager()\n            \n            version := args[0]\n            releaseType, _ := cmd.Flags().GetString(\"type\")\n            \n            rel, err := manager.PrepareRelease(version, releaseType)\n            if err != nil {\n                return err\n            }\n            \n            // Output release information\n            fmt.Printf(\"Release %s prepared successfully\\n\", rel.Version)\n            fmt.Printf(\"Changelog:\\n%s\\n\", rel.Changelog.Format())\n            \n            return nil\n        },\n    }\n    \n    rootCmd.AddCommand(prepareCmd)\n    rootCmd.Execute()\n}\n\n// .github/workflows/release-automation.yml\nname: Automated Release Process\non:\n  workflow_dispatch:\n    inputs:\n      version:\n        description: 'Release version (e.g., v1.2.3)'\n        required: true\n      release_type:\n        description: 'Release type'\n        type: choice\n        options:\n          - stable\n          - beta\n          - rc\n\njobs:\n  prepare-release:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n          \n      - name: Setup Go\n        uses: actions/setup-go@v4\n        with:\n          go-version: '1.21'\n          \n      - name: Run Release Preparation\n        run: |\n          go run cmd/release/main.go prepare ${{ github.event.inputs.version }} \\\n            --type ${{ github.event.inputs.release_type }}\n            \n      - name: Create Release PR\n        uses: peter-evans/create-pull-request@v5\n        with:\n          title: \"Release ${{ github.event.inputs.version }}\"\n          body: |\n            Automated release preparation for version ${{ github.event.inputs.version }}\n            \n            ## Checklist\n            - [ ] Tests passing\n            - [ ] Changelog reviewed\n            - [ ] Version bumped\n            - [ ] Documentation updated\n          branch: release/${{ github.event.inputs.version }}\n\n// internal/release/templates/changelog.tmpl\n# Changelog\n\n## [{{ .Version }}] - {{ .Date.Format \"2006-01-02\" }}\n\n{{ range $section, $entries := .Sections }}\n### {{ $section }}\n{{ range $entries }}\n- {{ .Description }} ({{ .CommitHash }})\n{{ end }}\n{{ end }}\n\n## Previous Releases\n{{ .PreviousContent }}\n\n// pkg/models/release.go\ntype Release struct {\n    Version     string\n    Type        ReleaseType\n    Changelog   *Changelog\n    Assets      []ReleaseAsset\n    CreatedAt   time.Time\n    PublishedAt *time.Time\n}\n\ntype Changelog struct {\n    Version  string\n    Date     time.Time\n    Sections map[string][]ChangeEntry\n}\n\ntype ChangeEntry struct {\n    Type        string // feat, fix, docs, etc.\n    Scope       string\n    Description string\n    CommitHash  string\n    Author      string\n    Breaking    bool\n}",
        "testStrategy": "Comprehensive testing strategy for the release preparation system:\n\n1. **Version Management Tests**:\n   - Test semantic version parsing and validation\n   - Verify version bumping logic (major, minor, patch)\n   - Test pre-release version handling (beta, rc)\n   - Validate version format compliance\n\n2. **Changelog Generation Tests**:\n   - Mock git repository with various commit patterns\n   - Test conventional commit parsing (feat:, fix:, docs:, etc.)\n   - Verify commit categorization and grouping\n   - Test changelog formatting with different templates\n   - Handle edge cases: first release, no commits, merge commits\n\n3. **Pre-release Validation Tests**:\n   - Mock successful and failing test runs\n   - Verify build check execution\n   - Test custom validation rules\n   - Ensure proper error reporting and aggregation\n\n4. **Integration Tests**:\n   - Test complete release preparation flow\n   - Verify file generation (CHANGELOG.md, VERSION)\n   - Test git operations (tag creation, commit)\n   - Validate GitHub Actions workflow execution\n\n5. **CLI Tests**:\n   - Test command parsing and validation\n   - Verify output formatting\n   - Test interactive mode prompts\n   - Validate error handling and help text\n\n6. **End-to-End Tests**:\n   - Create test repository with sample commits\n   - Run full release preparation\n   - Verify all artifacts are created correctly\n   - Test rollback on failure",
        "status": "done",
        "dependencies": [
          16,
          17
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Version Validation and Management",
            "description": "Develop the version validation and management component to ensure correct version formats and calculate the next version based on the bump type.",
            "dependencies": [],
            "details": "Create methods to validate version formats and determine the next version using semantic versioning rules.",
            "status": "done",
            "testStrategy": "Write unit tests to validate version formats and ensure correct version bumping."
          },
          {
            "id": 2,
            "title": "Develop Pre-Release Validation Checks",
            "description": "Implement pre-release validation checks to ensure code quality and build integrity before proceeding with the release.",
            "dependencies": [],
            "details": "Create checks such as running tests and verifying build success to ensure the codebase is ready for release.",
            "status": "done",
            "testStrategy": "Simulate various scenarios where checks fail and pass to ensure robustness."
          },
          {
            "id": 3,
            "title": "Create Changelog Generation System",
            "description": "Design and implement a system to generate changelogs from commit history, categorizing changes appropriately.",
            "dependencies": [
              1
            ],
            "details": "Use commit history to generate a structured changelog, categorizing changes by type and scope.",
            "status": "done",
            "testStrategy": "Test with different commit histories to ensure accurate and comprehensive changelog generation."
          },
          {
            "id": 4,
            "title": "Integrate Release Preparation Workflow",
            "description": "Integrate all components into a cohesive release preparation workflow that automates the release process.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Combine version management, validation checks, and changelog generation into a single automated workflow.",
            "status": "done",
            "testStrategy": "Perform end-to-end testing of the release preparation process to ensure all components work together seamlessly."
          },
          {
            "id": 5,
            "title": "Implement Post-Release Notification System",
            "description": "Develop a notification system to inform stakeholders of release completion and provide release details.",
            "dependencies": [
              4
            ],
            "details": "Create a notification mechanism to send release details and changelogs to relevant stakeholders after release completion.",
            "status": "done",
            "testStrategy": "Test notifications with various stakeholders to ensure timely and accurate information delivery."
          }
        ]
      },
      {
        "id": 19,
        "title": "Implement Streaming PDF Processing for Large Files",
        "description": "Build a streaming PDF processor that handles large documents exceeding memory limits by processing them in chunks, implementing efficient memory management and progressive parsing techniques.",
        "details": "Implement a comprehensive streaming PDF processing system that can handle arbitrarily large PDF files without loading them entirely into memory:\n\n```go\n// internal/pdf/streaming/stream_parser.go\npackage streaming\n\nimport (\n    \"bufio\"\n    \"io\"\n    \"github.com/yourusername/pdfextract/internal/pdf\"\n)\n\ntype StreamParser struct {\n    reader      *bufio.Reader\n    chunkSize   int\n    offset      int64\n    xrefCache   *LRUCache // Limited size xref cache\n    objectCache *LRUCache // Limited size object cache\n}\n\nfunc NewStreamParser(r io.ReadSeeker, opts ...Option) *StreamParser {\n    return &StreamParser{\n        reader:      bufio.NewReaderSize(r, 64*1024), // 64KB buffer\n        chunkSize:   1024 * 1024, // 1MB chunks\n        xrefCache:   NewLRUCache(1000), // Cache last 1000 xref entries\n        objectCache: NewLRUCache(500),  // Cache last 500 objects\n    }\n}\n\n// internal/pdf/streaming/chunk_processor.go\ntype ChunkProcessor struct {\n    parser      *StreamParser\n    pageBuffer  *PageBuffer\n    textBuffer  *TextBuffer\n    imageBuffer *ImageBuffer\n}\n\nfunc (cp *ChunkProcessor) ProcessChunk(start, end int64) (*ChunkResult, error) {\n    // Seek to chunk start\n    // Parse objects in chunk\n    // Extract content progressively\n    // Flush buffers when full\n}\n\n// internal/pdf/streaming/page_streamer.go\ntype PageStreamer struct {\n    parser   *StreamParser\n    pageSize int // Max pages in memory\n}\n\nfunc (ps *PageStreamer) StreamPages(callback func(*pdf.Page) error) error {\n    pageNum := 0\n    for {\n        page, err := ps.parser.GetNextPage()\n        if err == io.EOF {\n            break\n        }\n        if err != nil {\n            return err\n        }\n        \n        // Process page\n        if err := callback(page); err != nil {\n            return err\n        }\n        \n        // Free page memory\n        page.Release()\n        pageNum++\n    }\n    return nil\n}\n\n// internal/pdf/streaming/memory_manager.go\ntype MemoryManager struct {\n    maxMemory   int64\n    currentUsed int64\n    gcTrigger   float64 // Trigger GC at % of max\n}\n\nfunc (mm *MemoryManager) AllocateBuffer(size int) ([]byte, error) {\n    if mm.currentUsed + int64(size) > mm.maxMemory {\n        if err := mm.FreeMemory(); err != nil {\n            return nil, err\n        }\n    }\n    \n    buf := make([]byte, size)\n    mm.currentUsed += int64(size)\n    \n    if float64(mm.currentUsed)/float64(mm.maxMemory) > mm.gcTrigger {\n        runtime.GC()\n    }\n    \n    return buf, nil\n}\n\n// pkg/streaming/api.go\ntype StreamingExtractor struct {\n    parser  *StreamParser\n    options StreamOptions\n}\n\ntype StreamOptions struct {\n    MaxMemoryMB    int\n    ChunkSizeMB    int\n    PageBufferSize int\n    EnableCaching  bool\n}\n\nfunc (se *StreamingExtractor) ExtractText(reader io.ReadSeeker, writer io.Writer) error {\n    parser := NewStreamParser(reader, WithMemoryLimit(se.options.MaxMemoryMB))\n    \n    return parser.StreamPages(func(page *pdf.Page) error {\n        text, err := page.ExtractText()\n        if err != nil {\n            return err\n        }\n        \n        _, err = writer.Write([]byte(text))\n        return err\n    })\n}\n\n// internal/pdf/streaming/progressive_parser.go\ntype ProgressiveParser struct {\n    baseParser *StreamParser\n    progress   chan<- Progress\n}\n\ntype Progress struct {\n    BytesProcessed int64\n    TotalBytes     int64\n    PagesProcessed int\n    CurrentPage    int\n    MemoryUsed     int64\n}\n\nfunc (pp *ProgressiveParser) ParseWithProgress(ctx context.Context) error {\n    ticker := time.NewTicker(100 * time.Millisecond)\n    defer ticker.Stop()\n    \n    go func() {\n        for {\n            select {\n            case <-ticker.C:\n                pp.progress <- pp.getProgress()\n            case <-ctx.Done():\n                return\n            }\n        }\n    }()\n    \n    return pp.parse(ctx)\n}\n\n// internal/pdf/streaming/buffered_extractor.go\ntype BufferedExtractor struct {\n    textBuffer  *RingBuffer\n    imageBuffer *RingBuffer\n    tableBuffer *RingBuffer\n    flushSize   int\n}\n\nfunc (be *BufferedExtractor) Extract(page *pdf.Page) error {\n    // Extract content to buffers\n    if err := be.extractText(page); err != nil {\n        return err\n    }\n    \n    // Check if buffers need flushing\n    if be.textBuffer.Size() > be.flushSize {\n        if err := be.flushTextBuffer(); err != nil {\n            return err\n        }\n    }\n    \n    return nil\n}\n\n// internal/pdf/streaming/xref_stream.go\ntype XRefStreamer struct {\n    reader    io.ReadSeeker\n    cache     *XRefCache\n    chunkSize int\n}\n\nfunc (xs *XRefStreamer) GetObject(ref pdf.ObjectRef) (*pdf.Object, error) {\n    // Check cache first\n    if obj, ok := xs.cache.Get(ref); ok {\n        return obj, nil\n    }\n    \n    // Stream to object location\n    offset, err := xs.findObjectOffset(ref)\n    if err != nil {\n        return nil, err\n    }\n    \n    // Read only the object\n    obj, err := xs.readObjectAt(offset)\n    if err != nil {\n        return nil, err\n    }\n    \n    // Cache for future use\n    xs.cache.Put(ref, obj)\n    return obj, nil\n}\n```\n\nKey implementation considerations:\n\n1. **Memory-bounded processing**: Implement strict memory limits with configurable thresholds\n2. **Lazy loading**: Load PDF objects only when needed, not entire file\n3. **Streaming extraction**: Process content as it's read without storing entire document\n4. **Progressive parsing**: Allow partial results and progress reporting\n5. **Efficient caching**: Use LRU caches for frequently accessed objects\n6. **Chunk-based processing**: Divide large files into manageable chunks\n7. **Buffer management**: Use ring buffers and flush strategies\n8. **Resource cleanup**: Ensure proper memory release after processing",
        "testStrategy": "Comprehensive testing strategy for streaming PDF processing:\n\n1. **Memory limit tests**:\n   - Create test PDFs of various sizes (100MB, 500MB, 1GB+)\n   - Monitor memory usage during processing with runtime.MemStats\n   - Verify memory never exceeds configured limits\n   - Test with artificially low memory limits (e.g., 50MB for 1GB file)\n\n2. **Chunk processing tests**:\n   - Test chunk boundary handling (objects split across chunks)\n   - Verify no data loss between chunks\n   - Test different chunk sizes and their impact on performance\n   - Validate chunk overlap handling for cross-references\n\n3. **Streaming accuracy tests**:\n   - Compare streaming extraction results with full-file extraction\n   - Test with complex PDFs (many pages, embedded content, forms)\n   - Verify all content types extracted correctly (text, images, tables)\n   - Test partial extraction (specific page ranges)\n\n4. **Performance benchmarks**:\n   - Measure processing time for various file sizes\n   - Compare memory usage patterns with non-streaming approach\n   - Test concurrent streaming of multiple files\n   - Profile CPU usage and identify bottlenecks\n\n5. **Error handling tests**:\n   - Test with corrupted PDFs that fail mid-stream\n   - Verify graceful handling of memory allocation failures\n   - Test context cancellation during streaming\n   - Validate recovery from partial failures\n\n6. **Cache effectiveness tests**:\n   - Monitor cache hit/miss ratios\n   - Test cache eviction strategies\n   - Verify cache size limits are respected\n   - Test performance impact of different cache sizes\n\n7. **Integration tests**:\n   - Test streaming with existing extractors (text, image, table)\n   - Verify compatibility with form extraction\n   - Test progress reporting accuracy\n   - Validate buffer flushing strategies",
        "status": "done",
        "dependencies": [
          2,
          3,
          4
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Optimize pdf_server_info Tool Performance",
        "description": "Enhance the pdf_server_info tool performance by implementing lazy directory scanning, caching directory contents to avoid repeated filesystem operations, and removing the 5-second timeout that causes unnecessary delays.",
        "details": "Implement comprehensive performance optimizations for the pdf_server_info tool:\n\n```go\n// internal/tools/pdf_server_info.go\npackage tools\n\nimport (\n    \"sync\"\n    \"time\"\n    \"path/filepath\"\n    \"os\"\n)\n\ntype PDFServerInfo struct {\n    cache         *DirectoryCache\n    lazyScanner   *LazyDirectoryScanner\n    mu            sync.RWMutex\n}\n\n// Directory cache with TTL\ntype DirectoryCache struct {\n    entries map[string]*CacheEntry\n    ttl     time.Duration\n    mu      sync.RWMutex\n}\n\ntype CacheEntry struct {\n    files      []PDFFileInfo\n    lastUpdate time.Time\n    scanning   bool\n    scanMu     sync.Mutex\n}\n\n// Lazy directory scanner\ntype LazyDirectoryScanner struct {\n    maxDepth    int\n    fileLimit   int\n    timeLimit   time.Duration\n}\n\nfunc (p *PDFServerInfo) GetInfo(ctx context.Context, path string) (*ServerInfo, error) {\n    // Remove hardcoded 5-second timeout\n    // Use context timeout instead\n    \n    // Check cache first\n    if cached := p.cache.Get(path); cached != nil {\n        return &ServerInfo{\n            PDFFiles: cached.files,\n            FromCache: true,\n            CacheAge: time.Since(cached.lastUpdate),\n        }, nil\n    }\n    \n    // Implement lazy scanning\n    scanner := p.lazyScanner\n    results := make(chan PDFFileInfo, 100)\n    errChan := make(chan error, 1)\n    \n    go func() {\n        defer close(results)\n        if err := scanner.ScanDirectory(ctx, path, results); err != nil {\n            errChan <- err\n        }\n    }()\n    \n    // Collect results with streaming\n    var files []PDFFileInfo\n    for file := range results {\n        files = append(files, file)\n    }\n    \n    // Update cache\n    p.cache.Set(path, files)\n    \n    return &ServerInfo{PDFFiles: files}, nil\n}\n\n// Lazy scanning implementation\nfunc (s *LazyDirectoryScanner) ScanDirectory(ctx context.Context, root string, results chan<- PDFFileInfo) error {\n    visited := make(map[string]bool)\n    fileCount := 0\n    startTime := time.Now()\n    \n    return filepath.Walk(root, func(path string, info os.FileInfo, err error) error {\n        // Check context cancellation\n        select {\n        case <-ctx.Done():\n            return ctx.Err()\n        default:\n        }\n        \n        // Skip if we've hit limits\n        if s.fileLimit > 0 && fileCount >= s.fileLimit {\n            return filepath.SkipDir\n        }\n        \n        if s.timeLimit > 0 && time.Since(startTime) > s.timeLimit {\n            return filepath.SkipDir\n        }\n        \n        // Skip symlinks to avoid loops\n        if info.Mode()&os.ModeSymlink != 0 {\n            return nil\n        }\n        \n        // Check depth limit\n        depth := strings.Count(strings.TrimPrefix(path, root), string(os.PathSeparator))\n        if s.maxDepth > 0 && depth > s.maxDepth {\n            return filepath.SkipDir\n        }\n        \n        // Process PDF files\n        if strings.HasSuffix(strings.ToLower(path), \".pdf\") {\n            fileCount++\n            results <- PDFFileInfo{\n                Path: path,\n                Size: info.Size(),\n                ModTime: info.ModTime(),\n            }\n        }\n        \n        return nil\n    })\n}\n\n// Cache implementation with TTL\nfunc (c *DirectoryCache) Get(path string) *CacheEntry {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    \n    entry, exists := c.entries[path]\n    if !exists {\n        return nil\n    }\n    \n    // Check if cache is still valid\n    if time.Since(entry.lastUpdate) > c.ttl {\n        // Trigger background refresh\n        go c.refreshEntry(path)\n        // Return stale data for now\n    }\n    \n    return entry\n}\n\nfunc (c *DirectoryCache) Set(path string, files []PDFFileInfo) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    c.entries[path] = &CacheEntry{\n        files:      files,\n        lastUpdate: time.Now(),\n    }\n}\n\n// Background cache refresh\nfunc (c *DirectoryCache) refreshEntry(path string) {\n    entry := c.entries[path]\n    if entry == nil {\n        return\n    }\n    \n    // Prevent concurrent refreshes\n    if !entry.scanMu.TryLock() {\n        return\n    }\n    defer entry.scanMu.Unlock()\n    \n    // Rescan directory\n    scanner := &LazyDirectoryScanner{\n        maxDepth:  3,\n        fileLimit: 1000,\n        timeLimit: 5 * time.Second,\n    }\n    \n    results := make(chan PDFFileInfo, 100)\n    go scanner.ScanDirectory(context.Background(), path, results)\n    \n    var files []PDFFileInfo\n    for file := range results {\n        files = append(files, file)\n    }\n    \n    c.Set(path, files)\n}\n\n// Configuration for performance tuning\ntype PerformanceConfig struct {\n    CacheTTL           time.Duration `json:\"cache_ttl\"`\n    MaxScanDepth       int          `json:\"max_scan_depth\"`\n    MaxFilesPerScan    int          `json:\"max_files_per_scan\"`\n    ScanTimeLimit      time.Duration `json:\"scan_time_limit\"`\n    EnableBackgroundRefresh bool     `json:\"enable_background_refresh\"`\n}\n\n// Initialize with config\nfunc NewPDFServerInfo(config PerformanceConfig) *PDFServerInfo {\n    return &PDFServerInfo{\n        cache: &DirectoryCache{\n            entries: make(map[string]*CacheEntry),\n            ttl:     config.CacheTTL,\n        },\n        lazyScanner: &LazyDirectoryScanner{\n            maxDepth:  config.MaxScanDepth,\n            fileLimit: config.MaxFilesPerScan,\n            timeLimit: config.ScanTimeLimit,\n        },\n    }\n}\n```\n\nAdditional optimizations:\n\n```go\n// internal/tools/metrics.go\ntype PerformanceMetrics struct {\n    CacheHits      int64\n    CacheMisses    int64\n    ScanDuration   time.Duration\n    FilesScanned   int64\n    DirectoriesScanned int64\n}\n\n// Add metrics collection\nfunc (p *PDFServerInfo) GetInfoWithMetrics(ctx context.Context, path string) (*ServerInfo, *PerformanceMetrics, error) {\n    metrics := &PerformanceMetrics{}\n    start := time.Now()\n    \n    // Check cache\n    if cached := p.cache.Get(path); cached != nil {\n        atomic.AddInt64(&metrics.CacheHits, 1)\n        return &ServerInfo{\n            PDFFiles: cached.files,\n            FromCache: true,\n        }, metrics, nil\n    }\n    \n    atomic.AddInt64(&metrics.CacheMisses, 1)\n    \n    // Perform scan\n    info, err := p.GetInfo(ctx, path)\n    metrics.ScanDuration = time.Since(start)\n    \n    return info, metrics, err\n}\n\n// Implement directory watching for cache invalidation\ntype DirectoryWatcher struct {\n    watcher *fsnotify.Watcher\n    cache   *DirectoryCache\n}\n\nfunc (d *DirectoryWatcher) Watch(path string) error {\n    return d.watcher.Add(path)\n}\n\nfunc (d *DirectoryWatcher) handleEvents() {\n    for {\n        select {\n        case event := <-d.watcher.Events:\n            if event.Op&(fsnotify.Create|fsnotify.Remove|fsnotify.Rename) != 0 {\n                // Invalidate cache for parent directory\n                dir := filepath.Dir(event.Name)\n                d.cache.Invalidate(dir)\n            }\n        case err := <-d.watcher.Errors:\n            log.Printf(\"Watcher error: %v\", err)\n        }\n    }\n}\n```",
        "testStrategy": "Comprehensive testing strategy for performance optimizations:\n\n1. **Performance benchmarks**:\n   - Create benchmark tests comparing old vs new implementation\n   - Test with directories containing 10, 100, 1000, 10000 PDF files\n   - Measure memory usage and CPU utilization\n   - Verify removal of 5-second timeout delay\n\n2. **Cache functionality tests**:\n   - Test cache hit/miss scenarios\n   - Verify TTL expiration and refresh logic\n   - Test concurrent access to cache\n   - Verify cache invalidation on file system changes\n\n3. **Lazy scanning tests**:\n   - Test scan depth limiting\n   - Verify file count limits are respected\n   - Test time-based scan termination\n   - Verify context cancellation handling\n\n4. **Integration tests**:\n   - Test with real directory structures\n   - Verify symlink handling to prevent infinite loops\n   - Test with directories on different file systems (local, network)\n   - Measure actual performance improvements\n\n5. **Stress tests**:\n   - Test with very large directory trees\n   - Concurrent requests for same/different directories\n   - Memory leak detection with repeated operations\n   - Verify graceful degradation under load\n\n6. **Configuration tests**:\n   - Test different cache TTL values\n   - Verify configurable scan limits\n   - Test background refresh enable/disable\n   - Validate metrics collection accuracy",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Remove Hardcoded Timeout and Implement Context-Based Cancellation",
            "description": "Replace the hardcoded 5-second timeout in the pdf_server_info tool with proper context-based cancellation to allow flexible timeout configuration and immediate response to cancellation requests",
            "dependencies": [],
            "details": "Modify the GetInfo method to use context.WithTimeout or context.WithDeadline instead of hardcoded timeouts. Update all blocking operations to check context.Done() channel. Ensure proper cleanup when context is cancelled. Update the ScanDirectory method to properly propagate context cancellation throughout the file walking process.",
            "status": "done",
            "testStrategy": "Create unit tests that verify context cancellation works correctly at different stages of scanning. Test with various timeout values and ensure scanning stops immediately when context is cancelled. Verify no goroutine leaks occur after cancellation."
          },
          {
            "id": 2,
            "title": "Implement Directory Cache with TTL and Background Refresh",
            "description": "Create a thread-safe caching mechanism for directory contents with configurable TTL and automatic background refresh to avoid repeated filesystem operations",
            "dependencies": [],
            "details": "Implement the DirectoryCache struct with RWMutex for thread safety. Create Get and Set methods with proper locking. Implement TTL checking logic that returns stale data while triggering background refresh. Add the refreshEntry method that prevents concurrent refreshes using TryLock. Ensure cache entries include lastUpdate timestamp and scanning status.",
            "status": "done",
            "testStrategy": "Write tests for concurrent cache access, TTL expiration behavior, and background refresh triggering. Verify that stale data is returned while refresh is in progress. Test cache invalidation and memory cleanup for expired entries."
          },
          {
            "id": 3,
            "title": "Create Lazy Directory Scanner with Configurable Limits",
            "description": "Implement a lazy directory scanner that supports depth limits, file count limits, and time-based scanning limits to prevent excessive resource usage on large directory trees",
            "dependencies": [
              1
            ],
            "details": "Implement the LazyDirectoryScanner struct with configurable maxDepth, fileLimit, and timeLimit. Create the ScanDirectory method that uses filepath.Walk with proper limit checking. Implement symlink detection to avoid infinite loops. Add depth calculation logic and skip directories when limits are reached. Stream results through a channel instead of building a complete list in memory.",
            "status": "done",
            "testStrategy": "Test scanner with directories of various sizes and depths. Verify that limits are respected and scanning stops appropriately. Test symlink handling and circular reference prevention. Measure performance improvements with large directory structures."
          },
          {
            "id": 4,
            "title": "Add Performance Metrics Collection and Monitoring",
            "description": "Implement comprehensive performance metrics collection including cache hit/miss rates, scan durations, and file counts to enable monitoring and optimization",
            "dependencies": [
              2,
              3
            ],
            "details": "Create the PerformanceMetrics struct with atomic counters for thread safety. Implement GetInfoWithMetrics method that wraps GetInfo with metric collection. Add cache hit/miss tracking, scan duration measurement, and file/directory count tracking. Use atomic operations for concurrent metric updates. Create methods to export metrics in a format suitable for monitoring systems.",
            "status": "done",
            "testStrategy": "Verify metrics are accurately collected under concurrent load. Test that cache hits and misses are properly counted. Ensure scan duration measurements are accurate. Create benchmarks to validate metric collection doesn't significantly impact performance."
          },
          {
            "id": 5,
            "title": "Implement Directory Watcher for Cache Invalidation",
            "description": "Create a filesystem watcher that monitors directories for changes and automatically invalidates cache entries when files are added, removed, or renamed",
            "dependencies": [
              2
            ],
            "details": "Implement DirectoryWatcher using fsnotify library. Create Watch method to add directories to monitoring. Implement handleEvents goroutine that processes filesystem events and invalidates appropriate cache entries. Handle Create, Remove, and Rename events by invalidating the parent directory cache. Add proper error handling and logging for watcher errors. Ensure watcher cleanup on shutdown.",
            "status": "done",
            "testStrategy": "Test cache invalidation when files are added, removed, or renamed in watched directories. Verify that only affected cache entries are invalidated. Test watcher behavior under high filesystem activity. Ensure proper cleanup and no resource leaks when watcher is stopped."
          }
        ]
      },
      {
        "id": 21,
        "title": "Update MCP Tool Descriptions for Clarity and Usability",
        "description": "Revise all MCP tool descriptions to be clear, concise, and action-oriented, emphasizing when and why to use each tool with practical examples and real-world use cases that help users quickly understand tool capabilities.",
        "details": "Implement comprehensive updates to all MCP tool descriptions across the codebase:\n\n```go\n// internal/tools/descriptions.go\npackage tools\n\n// Tool description structure for consistency\ntype ToolDescription struct {\n    Name           string\n    Summary        string // One-line action-oriented summary\n    When           string // When to use this tool\n    Why            string // Why this tool is valuable\n    Examples       []Example\n    UseCases       []UseCase\n    Prerequisites  []string\n    Output         string // What the tool returns\n}\n\ntype Example struct {\n    Scenario    string\n    Command     string\n    Expected    string\n}\n\ntype UseCase struct {\n    Title       string\n    Description string\n    Workflow    []string // Step-by-step workflow\n}\n\n// Example: PDF Server Info Tool\nvar PDFServerInfoDesc = ToolDescription{\n    Name:    \"pdf_server_info\",\n    Summary: \"Get real-time server status and PDF processing capabilities\",\n    When:    \"Before starting PDF operations or troubleshooting issues\",\n    Why:     \"Ensures server readiness and helps diagnose configuration problems\",\n    Examples: []Example{\n        {\n            Scenario: \"Check server health before bulk processing\",\n            Command:  \"pdf_server_info\",\n            Expected: \"Server status, available tools, memory usage\",\n        },\n        {\n            Scenario: \"Verify PDF directory configuration\",\n            Command:  \"pdf_server_info --check-dirs\",\n            Expected: \"List of configured directories and file counts\",\n        },\n    },\n    UseCases: []UseCase{\n        {\n            Title: \"Pre-flight Check for Automation\",\n            Description: \"Verify server is ready before automated workflows\",\n            Workflow: []string{\n                \"1. Call pdf_server_info to check status\",\n                \"2. Verify required tools are available\",\n                \"3. Check memory/CPU thresholds\",\n                \"4. Proceed with automation if healthy\",\n            },\n        },\n    },\n    Prerequisites: []string{\"PDF server must be running\"},\n    Output: \"JSON object with server status, capabilities, and metrics\",\n}\n\n// Example: List PDFs Tool\nvar ListPDFsDesc = ToolDescription{\n    Name:    \"list_pdfs\",\n    Summary: \"Discover and filter PDF files across configured directories\",\n    When:    \"Need to find specific PDFs or get an overview of available documents\",\n    Why:     \"Quickly locate documents without manual directory browsing\",\n    Examples: []Example{\n        {\n            Scenario: \"Find all PDFs containing 'invoice' in the name\",\n            Command:  \"list_pdfs --filter 'invoice'\",\n            Expected: \"List of matching PDFs with paths and metadata\",\n        },\n        {\n            Scenario: \"Get PDFs modified in last 7 days\",\n            Command:  \"list_pdfs --modified-since '7d'\",\n            Expected: \"Recent PDFs with timestamps\",\n        },\n    },\n    UseCases: []UseCase{\n        {\n            Title: \"Batch Processing Preparation\",\n            Description: \"Identify PDFs for bulk operations\",\n            Workflow: []string{\n                \"1. Use list_pdfs with filters to find target files\",\n                \"2. Export results to processing queue\",\n                \"3. Iterate through list for batch operations\",\n            },\n        },\n        {\n            Title: \"Document Inventory Management\",\n            Description: \"Regular audits of PDF collections\",\n            Workflow: []string{\n                \"1. Schedule periodic list_pdfs calls\",\n                \"2. Compare results with previous inventory\",\n                \"3. Identify new, modified, or missing files\",\n                \"4. Generate inventory reports\",\n            },\n        },\n    },\n    Prerequisites: []string{\"Configured PDF directories in server\"},\n    Output: \"Array of PDF metadata objects with paths, sizes, dates\",\n}\n\n// Template for consistent formatting\nconst DescriptionTemplate = `\n## {{.Name}}\n\n**What it does:** {{.Summary}}\n\n**When to use:** {{.When}}\n\n**Why it's useful:** {{.Why}}\n\n### Examples\n\n{{range .Examples}}\n**{{.Scenario}}**\n\\`\\`\\`\n{{.Command}}\n\\`\\`\\`\nExpected output: {{.Expected}}\n\n{{end}}\n\n### Use Cases\n\n{{range .UseCases}}\n#### {{.Title}}\n{{.Description}}\n\n**Workflow:**\n{{range .Workflow}}\n{{.}}\n{{end}}\n\n{{end}}\n\n**Prerequisites:** {{join .Prerequisites \", \"}}\n\n**Returns:** {{.Output}}\n`\n\n// Update all tool registration with new descriptions\nfunc RegisterTools() {\n    tools := map[string]ToolDescription{\n        \"pdf_server_info\": PDFServerInfoDesc,\n        \"list_pdfs\":       ListPDFsDesc,\n        \"extract_text\":    ExtractTextDesc,\n        \"extract_images\":  ExtractImagesDesc,\n        \"get_metadata\":    GetMetadataDesc,\n        \"search_pdfs\":     SearchPDFsDesc,\n        \"merge_pdfs\":      MergePDFsDesc,\n        \"split_pdf\":       SplitPDFDesc,\n        // ... all other tools\n    }\n    \n    // Generate documentation\n    for name, desc := range tools {\n        doc := GenerateDocumentation(desc)\n        SaveToolDoc(name, doc)\n    }\n}\n\n// MCP schema integration\nfunc UpdateMCPSchemas(tools map[string]ToolDescription) {\n    for name, desc := range tools {\n        schema := MCPToolSchema{\n            Name:        name,\n            Description: desc.Summary,\n            InputSchema: generateInputSchema(name),\n            Examples:    convertExamples(desc.Examples),\n        }\n        RegisterMCPTool(schema)\n    }\n}",
        "testStrategy": "Comprehensive testing and validation strategy for updated tool descriptions:\n\n1. **Description Quality Tests**:\n   - Verify all descriptions follow action-oriented format (starts with verb)\n   - Check summary length is under 80 characters\n   - Ensure When/Why sections are present and meaningful\n   - Validate at least 2 examples per tool\n   - Confirm at least 1 detailed use case per tool\n\n2. **Consistency Validation**:\n   - Run linter to check all tools use ToolDescription structure\n   - Verify template rendering produces valid Markdown\n   - Check all prerequisites are accurate and testable\n   - Ensure output descriptions match actual tool responses\n\n3. **Example Testing**:\n   - Execute each example command programmatically\n   - Verify expected outputs match actual results\n   - Test examples cover common and edge cases\n   - Validate command syntax is correct\n\n4. **Use Case Validation**:\n   - Review workflows with actual users\n   - Test each workflow step is actionable\n   - Verify use cases address real user needs\n   - Check workflows are complete and logical\n\n5. **Documentation Generation**:\n   - Generate docs for all tools automatically\n   - Verify Markdown formatting is correct\n   - Test documentation site builds successfully\n   - Check cross-references between tools work\n\n6. **MCP Integration Tests**:\n   - Verify MCP schemas include new descriptions\n   - Test tool discovery shows updated information\n   - Check Claude/other assistants see new descriptions\n   - Validate schema validation passes\n\n7. **User Acceptance Testing**:\n   - A/B test old vs new descriptions with users\n   - Measure time to understand tool purpose\n   - Track successful tool usage after reading descriptions\n   - Collect feedback on clarity and usefulness\n\n8. **Automated Checks**:\n   ```go\n   func TestToolDescriptions(t *testing.T) {\n       for name, desc := range AllToolDescriptions {\n           // Check required fields\n           assert.NotEmpty(t, desc.Summary)\n           assert.NotEmpty(t, desc.When)\n           assert.NotEmpty(t, desc.Why)\n           \n           // Verify action-oriented\n           assert.Regexp(t, \"^[A-Z][a-z]+ \", desc.Summary)\n           \n           // Check examples\n           assert.GreaterOrEqual(t, len(desc.Examples), 2)\n           \n           // Validate use cases\n           assert.GreaterOrEqual(t, len(desc.UseCases), 1)\n       }\n   }\n   ```",
        "status": "done",
        "dependencies": [
          20
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 22,
        "title": "Implement Page-Range Extraction Tools for Large PDFs",
        "description": "Create specialized tools that enable efficient extraction of content from specific page ranges in large PDF documents without loading the entire file into memory, leveraging streaming techniques and selective parsing.",
        "details": "Implement a comprehensive page-range extraction system that builds upon the streaming infrastructure:\n\n```go\n// internal/pdf/pagerange/extractor.go\npackage pagerange\n\nimport (\n    \"github.com/yourusername/pdfextract/internal/pdf/streaming\"\n    \"github.com/yourusername/pdfextract/internal/pdf/wrapper\"\n)\n\ntype PageRangeExtractor struct {\n    streamParser *streaming.StreamParser\n    pageIndex    *PageIndex\n    cache        *PageObjectCache\n}\n\ntype PageRange struct {\n    Start int `json:\"start\"`\n    End   int `json:\"end\"`\n}\n\nfunc (pre *PageRangeExtractor) ExtractRange(reader io.ReadSeeker, ranges []PageRange, options ExtractOptions) (*ExtractedContent, error) {\n    // 1. Build minimal page index without parsing all pages\n    if err := pre.buildPageIndex(reader); err != nil {\n        return nil, fmt.Errorf(\"failed to build page index: %w\", err)\n    }\n    \n    // 2. Calculate required objects for requested pages\n    requiredObjects := pre.calculateRequiredObjects(ranges)\n    \n    // 3. Stream parse only necessary objects\n    content := &ExtractedContent{\n        Pages: make(map[int]*PageContent),\n    }\n    \n    for _, objID := range requiredObjects {\n        obj, err := pre.streamParser.ParseObject(reader, objID)\n        if err != nil {\n            return nil, fmt.Errorf(\"failed to parse object %d: %w\", objID, err)\n        }\n        pre.cache.Put(objID, obj)\n    }\n    \n    // 4. Extract content from specific pages\n    for _, r := range ranges {\n        for page := r.Start; page <= r.End && page <= pre.pageIndex.TotalPages; page++ {\n            pageContent, err := pre.extractPageContent(page)\n            if err != nil {\n                return nil, fmt.Errorf(\"failed to extract page %d: %w\", page, err)\n            }\n            content.Pages[page] = pageContent\n        }\n    }\n    \n    return content, nil\n}\n\n// internal/pdf/pagerange/index.go\ntype PageIndex struct {\n    TotalPages   int\n    PageOffsets  map[int]int64      // Page number -> file offset\n    PageObjects  map[int]ObjectRef  // Page number -> object reference\n    Resources    map[int][]ObjectRef // Page number -> required resources\n}\n\nfunc (pre *PageRangeExtractor) buildPageIndex(reader io.ReadSeeker) error {\n    // Parse only page tree structure\n    catalog, err := pre.streamParser.ParseCatalog(reader)\n    if err != nil {\n        return err\n    }\n    \n    // Walk page tree to build index\n    pre.pageIndex = &PageIndex{\n        PageOffsets: make(map[int]int64),\n        PageObjects: make(map[int]ObjectRef),\n        Resources:   make(map[int][]ObjectRef),\n    }\n    \n    return pre.walkPageTree(reader, catalog.Pages, 1)\n}\n\n// internal/tools/extract_page_range.go\ntype ExtractPageRangeTool struct {\n    extractor *pagerange.PageRangeExtractor\n    wrapper   wrapper.PDFLibrary\n}\n\nfunc (t *ExtractPageRangeTool) Execute(args map[string]interface{}) (interface{}, error) {\n    filePath := args[\"file\"].(string)\n    ranges := parseRanges(args[\"ranges\"])\n    contentTypes := args[\"content_types\"].([]string) // text, images, forms, etc.\n    \n    // Open file for streaming\n    file, err := os.Open(filePath)\n    if err != nil {\n        return nil, err\n    }\n    defer file.Close()\n    \n    // Extract specified content from page ranges\n    options := pagerange.ExtractOptions{\n        ContentTypes: contentTypes,\n        PreserveFormatting: args[\"preserve_formatting\"].(bool),\n        IncludeMetadata: args[\"include_metadata\"].(bool),\n    }\n    \n    content, err := t.extractor.ExtractRange(file, ranges, options)\n    if err != nil {\n        return nil, err\n    }\n    \n    return content, nil\n}\n\n// internal/pdf/pagerange/cache.go\ntype PageObjectCache struct {\n    objects  map[ObjectRef]*CachedObject\n    size     int64\n    maxSize  int64\n    lru      *list.List\n    mu       sync.RWMutex\n}\n\ntype CachedObject struct {\n    ref      ObjectRef\n    data     interface{}\n    size     int64\n    accessed time.Time\n    element  *list.Element\n}\n\n// Implement LRU eviction when cache exceeds maxSize\nfunc (c *PageObjectCache) Put(ref ObjectRef, obj interface{}) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    objSize := calculateObjectSize(obj)\n    \n    // Evict if necessary\n    for c.size+objSize > c.maxSize && c.lru.Len() > 0 {\n        c.evictOldest()\n    }\n    \n    // Add new object\n    cached := &CachedObject{\n        ref:      ref,\n        data:     obj,\n        size:     objSize,\n        accessed: time.Now(),\n    }\n    \n    cached.element = c.lru.PushFront(cached)\n    c.objects[ref] = cached\n    c.size += objSize\n}\n\n// Support for complex page range specifications\nfunc parseRanges(input interface{}) []PageRange {\n    // Support formats like:\n    // \"1-5,10,15-20\"\n    // [\"1-5\", \"10\", \"15-20\"]\n    // [{\"start\": 1, \"end\": 5}, {\"start\": 10, \"end\": 10}]\n}\n\n// Memory-efficient content aggregation\ntype ExtractedContent struct {\n    Pages    map[int]*PageContent `json:\"pages\"`\n    Metadata *DocumentMetadata    `json:\"metadata,omitempty\"`\n}\n\ntype PageContent struct {\n    Number     int                    `json:\"number\"`\n    Text       []ExtractedText        `json:\"text,omitempty\"`\n    Images     []ExtractedImage       `json:\"images,omitempty\"`\n    Forms      []FormField            `json:\"forms,omitempty\"`\n    Tables     []ExtractedTable       `json:\"tables,omitempty\"`\n    Bounds     BoundingBox            `json:\"bounds\"`\n}\n```\n\nKey implementation considerations:\n\n1. **Efficient page indexing**: Build a lightweight index of page locations without parsing content\n2. **Selective object loading**: Only load PDF objects required for requested pages\n3. **Resource sharing**: Handle shared resources (fonts, images) efficiently across pages\n4. **Memory management**: Use bounded caches with LRU eviction\n5. **Flexible range specification**: Support various input formats for page ranges\n6. **Content type filtering**: Allow extraction of specific content types only\n7. **Progress reporting**: For large ranges, provide progress callbacks\n8. **Error recovery**: Handle corrupted pages gracefully without failing entire extraction",
        "testStrategy": "Comprehensive testing strategy for page-range extraction:\n\n1. **Performance benchmarks**:\n   - Test extraction of single page from 1000+ page PDF\n   - Measure memory usage stays constant regardless of PDF size\n   - Compare extraction time for first, middle, and last pages\n   - Verify no unnecessary object parsing occurs\n\n2. **Range specification tests**:\n   - Test various range formats: \"1-5\", \"1,3,5\", \"10-\", \"-10\"\n   - Test overlapping ranges: \"1-5,3-7\"\n   - Test invalid ranges: negative numbers, out of bounds\n   - Test edge cases: empty ranges, single page\n\n3. **Memory efficiency tests**:\n   - Monitor memory with runtime.MemStats during extraction\n   - Test with 1GB+ PDFs extracting small page ranges\n   - Verify cache eviction when memory limit reached\n   - Test concurrent extractions from same file\n\n4. **Content accuracy tests**:\n   - Extract known pages and compare with full document extraction\n   - Verify all content types extracted correctly (text, images, forms)\n   - Test pages with shared resources (fonts used across pages)\n   - Validate coordinate systems remain accurate\n\n5. **Integration tests**:\n   - Test with streaming parser for large files\n   - Test with different PDF backends (pdfcpu, ledongthuc)\n   - Test as MCP tool with various parameter combinations\n   - Verify proper error messages for user-facing tool\n\n6. **Edge case tests**:\n   - PDFs with non-sequential page objects\n   - Encrypted PDFs with page-level permissions\n   - PDFs with complex page trees (nested, inherited resources)\n   - Corrupted PDFs with damaged page objects",
        "status": "done",
        "dependencies": [
          19,
          13,
          4
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 23,
        "title": "Implement Intelligent PDF Content Chunking",
        "description": "Build an intelligent content chunking system that automatically splits large PDF documents into logical sections (chapters, sections, paragraphs) based on structural analysis, formatting cues, and semantic boundaries for improved processing of complex documents like technical specifications.",
        "details": "Implement a comprehensive PDF content chunking system that analyzes document structure and intelligently splits content:\n\n```go\n// internal/pdf/chunking/analyzer.go\npackage chunking\n\nimport (\n    \"github.com/yourusername/pdfextract/internal/pdf/streaming\"\n    \"github.com/yourusername/pdfextract/pkg/models\"\n)\n\ntype ContentAnalyzer struct {\n    streamParser *streaming.StreamParser\n    layoutEngine *LayoutAnalysisEngine\n    chunkBuilder *ChunkBuilder\n}\n\ntype StructuralElement struct {\n    Type       ElementType  `json:\"type\"` // chapter, section, subsection, paragraph\n    Level      int         `json:\"level\"`\n    Title      string      `json:\"title,omitempty\"`\n    StartPage  int         `json:\"startPage\"`\n    EndPage    int         `json:\"endPage\"`\n    StartPos   Position    `json:\"startPos\"`\n    EndPos     Position    `json:\"endPos\"`\n    FontInfo   FontMetrics `json:\"fontInfo\"`\n    Children   []StructuralElement `json:\"children,omitempty\"`\n}\n\ntype ChunkingStrategy struct {\n    MaxChunkSize      int     `json:\"maxChunkSize\"`      // Max chars per chunk\n    MinChunkSize      int     `json:\"minChunkSize\"`      // Min chars per chunk\n    PreserveStructure bool    `json:\"preserveStructure\"` // Keep logical units intact\n    SplitTables       bool    `json:\"splitTables\"`       // Allow table splitting\n    HeaderDetection   bool    `json:\"headerDetection\"`   // Detect repeating headers\n    FooterDetection   bool    `json:\"footerDetection\"`   // Detect repeating footers\n}\n\n// internal/pdf/chunking/layout_engine.go\ntype LayoutAnalysisEngine struct {\n    fontAnalyzer    *FontAnalyzer\n    spacingAnalyzer *SpacingAnalyzer\n    indentDetector  *IndentationDetector\n}\n\nfunc (lae *LayoutAnalysisEngine) AnalyzeStructure(pages []ExtractedPage) (*DocumentStructure, error) {\n    // 1. Analyze font usage patterns\n    fontHierarchy := lae.fontAnalyzer.BuildFontHierarchy(pages)\n    \n    // 2. Detect heading levels based on font size/weight\n    headings := lae.detectHeadings(pages, fontHierarchy)\n    \n    // 3. Analyze spacing patterns\n    spacingPatterns := lae.spacingAnalyzer.AnalyzeSpacing(pages)\n    \n    // 4. Build document outline\n    outline := lae.buildDocumentOutline(headings, spacingPatterns)\n    \n    return &DocumentStructure{\n        Outline:         outline,\n        FontHierarchy:   fontHierarchy,\n        SpacingPatterns: spacingPatterns,\n    }, nil\n}\n\n// internal/pdf/chunking/chunk_builder.go\ntype ChunkBuilder struct {\n    strategy ChunkingStrategy\n    merger   *ContentMerger\n}\n\ntype ContentChunk struct {\n    ID            string              `json:\"id\"`\n    SequenceNum   int                `json:\"sequenceNum\"`\n    Type          ChunkType          `json:\"type\"`\n    Title         string             `json:\"title,omitempty\"`\n    Content       string             `json:\"content\"`\n    PageRange     PageRange          `json:\"pageRange\"`\n    StructureInfo StructuralElement  `json:\"structureInfo\"`\n    Metadata      ChunkMetadata      `json:\"metadata\"`\n    Previous      string             `json:\"previous,omitempty\"` // ID of previous chunk\n    Next          string             `json:\"next,omitempty\"`     // ID of next chunk\n}\n\nfunc (cb *ChunkBuilder) BuildChunks(structure *DocumentStructure, content []ExtractedContent) ([]ContentChunk, error) {\n    chunks := []ContentChunk{}\n    \n    // Process based on strategy\n    if cb.strategy.PreserveStructure {\n        // Chunk along structural boundaries\n        chunks = cb.chunkByStructure(structure, content)\n    } else {\n        // Chunk by size with smart splitting\n        chunks = cb.chunkBySize(content)\n    }\n    \n    // Post-process chunks\n    chunks = cb.handleOverflow(chunks)\n    chunks = cb.linkChunks(chunks)\n    \n    return chunks, nil\n}\n\n// internal/pdf/chunking/semantic_splitter.go\ntype SemanticSplitter struct {\n    nlpAnalyzer *NLPAnalyzer\n    sentenceDetector *SentenceDetector\n}\n\nfunc (ss *SemanticSplitter) FindSplitPoints(text string, targetSize int) []int {\n    // 1. Detect sentence boundaries\n    sentences := ss.sentenceDetector.DetectSentences(text)\n    \n    // 2. Find paragraph boundaries\n    paragraphs := ss.findParagraphBoundaries(text)\n    \n    // 3. Score potential split points\n    splitScores := ss.scoreSplitPoints(text, sentences, paragraphs)\n    \n    // 4. Select optimal split points near target size\n    return ss.selectOptimalSplits(splitScores, targetSize)\n}\n\n// pkg/models/chunking.go\ntype ChunkMetadata struct {\n    WordCount      int                 `json:\"wordCount\"`\n    CharCount      int                 `json:\"charCount\"`\n    HasTables      bool               `json:\"hasTables\"`\n    HasImages      bool               `json:\"hasImages\"`\n    HasFormFields  bool               `json:\"hasFormFields\"`\n    Language       string             `json:\"language,omitempty\"`\n    ReadingLevel   float64            `json:\"readingLevel,omitempty\"`\n    Keywords       []string           `json:\"keywords,omitempty\"`\n    CrossRefs      []CrossReference   `json:\"crossRefs,omitempty\"`\n}\n\n// internal/pdf/chunking/table_handler.go\ntype TableChunkHandler struct {\n    tableDetector *TableDetector\n    tableSplitter *TableSplitter\n}\n\nfunc (tch *TableChunkHandler) HandleTableChunking(table *ExtractedTable, maxSize int) []TableChunk {\n    if !tch.shouldSplitTable(table, maxSize) {\n        return []TableChunk{{Table: table}}\n    }\n    \n    // Split table intelligently\n    splitPoints := tch.findTableSplitPoints(table)\n    return tch.tableSplitter.SplitTable(table, splitPoints)\n}\n\n// API endpoint implementation\n// internal/server/chunking_handler.go\nfunc (s *MCPServer) handleChunkPDF(params ChunkingParams) (*ChunkingResult, error) {\n    // Initialize chunking system\n    analyzer := chunking.NewContentAnalyzer(\n        s.streamParser,\n        chunking.ChunkingStrategy{\n            MaxChunkSize:      params.MaxChunkSize,\n            MinChunkSize:      params.MinChunkSize,\n            PreserveStructure: params.PreserveStructure,\n            SplitTables:       params.AllowTableSplit,\n        },\n    )\n    \n    // Analyze document structure\n    structure, err := analyzer.AnalyzeDocument(params.FilePath)\n    if err != nil {\n        return nil, fmt.Errorf(\"structure analysis failed: %w\", err)\n    }\n    \n    // Generate chunks\n    chunks, err := analyzer.GenerateChunks(structure)\n    if err != nil {\n        return nil, fmt.Errorf(\"chunk generation failed: %w\", err)\n    }\n    \n    return &ChunkingResult{\n        Chunks:    chunks,\n        Structure: structure,\n        Stats:     analyzer.GetStatistics(),\n    }, nil\n}",
        "testStrategy": "Comprehensive testing strategy for intelligent PDF content chunking:\n\n1. **Structure Detection Tests**:\n   - Test with PDF specifications (PDF 1.4, PDF 1.7) that have clear chapter/section structure\n   - Verify correct detection of heading hierarchy based on font sizes\n   - Test with documents using various heading styles (numbered, unnumbered, mixed)\n   - Validate outline generation matches table of contents when present\n   - Test with documents lacking clear structure (novels, continuous text)\n\n2. **Chunking Algorithm Tests**:\n   - Test chunk size constraints are respected (min/max sizes)\n   - Verify chunks don't split in middle of sentences when possible\n   - Test paragraph boundary preservation\n   - Validate section boundary preservation when PreserveStructure is enabled\n   - Test handling of edge cases (very small sections, single large paragraph)\n\n3. **Semantic Splitting Tests**:\n   - Test sentence detection with various punctuation patterns\n   - Verify handling of abbreviations and special cases (Dr., Inc., etc.)\n   - Test with multi-language documents\n   - Validate split point scoring algorithm\n   - Test handling of bullet points and numbered lists\n\n4. **Table and Special Content Tests**:\n   - Test table detection and chunking with large tables\n   - Verify table splitting preserves headers when possible\n   - Test handling of images and captions\n   - Validate form field preservation in chunks\n   - Test mixed content (text + tables + images)\n\n5. **Performance and Memory Tests**:\n   - Benchmark chunking speed for documents of various sizes\n   - Test memory usage with streaming for large PDFs (100MB+)\n   - Verify chunking works efficiently with page-range extraction\n   - Test concurrent chunking of multiple documents\n\n6. **Integration Tests**:\n   - Test with real technical documentation (API specs, user manuals)\n   - Verify chunk linking (previous/next) is correct\n   - Test chunk metadata accuracy (word counts, cross-references)\n   - Validate chunk retrieval and reassembly\n   - Test with documents containing headers/footers\n\n7. **Edge Case Tests**:\n   - Test with PDFs having no text content\n   - Test with scanned PDFs (image-based)\n   - Test with encrypted or protected PDFs\n   - Test with malformed structure information\n   - Test with documents using non-standard fonts or encodings",
        "status": "pending",
        "dependencies": [
          19,
          22,
          5,
          9
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 24,
        "title": "Fix pdf_analyze_document Tool Content Processing",
        "description": "Debug and fix the pdf_analyze_document tool that is failing with 'no content elements provided for analysis' error, investigating the workflow from PDF parsing through content extraction to ensure proper document analysis functionality.",
        "details": "Investigate and fix the broken pdf_analyze_document tool by tracing through the entire content processing pipeline:\n\n```go\n// cmd/pdf_analyze_document/main.go\n// First, add comprehensive debugging to trace the issue\nfunc analyzeDocument(pdfPath string) (*DocumentAnalysis, error) {\n    log.Printf(\"Starting document analysis for: %s\", pdfPath)\n    \n    // Step 1: Verify PDF parsing is working\n    parser := pdf.NewParser()\n    doc, err := parser.Parse(pdfPath)\n    if err != nil {\n        return nil, fmt.Errorf(\"PDF parsing failed: %w\", err)\n    }\n    log.Printf(\"PDF parsed successfully, pages: %d\", doc.PageCount)\n    \n    // Step 2: Debug content extraction\n    extractor := extractors.NewContentExtractor()\n    content, err := extractor.ExtractAll(doc)\n    if err != nil {\n        return nil, fmt.Errorf(\"content extraction failed: %w\", err)\n    }\n    log.Printf(\"Content extracted: %d elements\", len(content.Elements))\n    \n    // Step 3: Fix the content element processing\n    if len(content.Elements) == 0 {\n        // This is likely where the issue is\n        log.Printf(\"WARNING: No content elements extracted\")\n        // Check if the extractor is properly initialized\n        // Verify content stream parsing is working\n    }\n    \n    // Step 4: Ensure proper element type detection\n    analyzer := NewDocumentAnalyzer()\n    for _, element := range content.Elements {\n        log.Printf(\"Processing element type: %T, page: %d\", element, element.GetPage())\n    }\n    \n    return analyzer.Analyze(content)\n}\n\n// internal/analysis/document_analyzer.go\n// Fix the content element processing logic\ntype DocumentAnalyzer struct {\n    textExtractor   *extractors.TextExtractor\n    imageExtractor  *extractors.ImageExtractor\n    layoutAnalyzer  *extractors.LayoutAnalyzer\n}\n\nfunc (da *DocumentAnalyzer) Analyze(content *ExtractedContent) (*DocumentAnalysis, error) {\n    if content == nil || len(content.Elements) == 0 {\n        // Instead of returning error, try to extract content directly\n        log.Println(\"No pre-extracted content, attempting direct extraction\")\n        \n        // Fallback to direct extraction\n        elements, err := da.extractContentElements(content.Document)\n        if err != nil {\n            return nil, fmt.Errorf(\"direct extraction failed: %w\", err)\n        }\n        content.Elements = elements\n    }\n    \n    analysis := &DocumentAnalysis{\n        TotalPages:     content.Document.PageCount,\n        ContentSummary: make(map[string]int),\n        Elements:       make([]AnalyzedElement, 0),\n    }\n    \n    // Process each element with proper type checking\n    for _, elem := range content.Elements {\n        analyzed := da.analyzeElement(elem)\n        analysis.Elements = append(analysis.Elements, analyzed)\n        analysis.ContentSummary[analyzed.Type]++\n    }\n    \n    return analysis, nil\n}\n\n// Fix the content extraction workflow\nfunc (da *DocumentAnalyzer) extractContentElements(doc *PDFDocument) ([]ContentElement, error) {\n    var elements []ContentElement\n    \n    for pageNum := 1; pageNum <= doc.PageCount; pageNum++ {\n        page, err := doc.GetPage(pageNum)\n        if err != nil {\n            log.Printf(\"Failed to get page %d: %v\", pageNum, err)\n            continue\n        }\n        \n        // Extract text elements\n        texts, err := da.textExtractor.ExtractFromPage(page)\n        if err != nil {\n            log.Printf(\"Text extraction failed for page %d: %v\", pageNum, err)\n        } else {\n            for _, text := range texts {\n                elements = append(elements, text)\n            }\n        }\n        \n        // Extract images\n        images, err := da.imageExtractor.ExtractFromPage(page)\n        if err != nil {\n            log.Printf(\"Image extraction failed for page %d: %v\", pageNum, err)\n        } else {\n            for _, img := range images {\n                elements = append(elements, img)\n            }\n        }\n        \n        // Extract layout elements (paragraphs, lines)\n        layout, err := da.layoutAnalyzer.AnalyzePage(page)\n        if err != nil {\n            log.Printf(\"Layout analysis failed for page %d: %v\", pageNum, err)\n        } else {\n            elements = append(elements, layout.Elements...)\n        }\n    }\n    \n    return elements, nil\n}\n\n// Ensure proper initialization of extractors\nfunc NewDocumentAnalyzer() *DocumentAnalyzer {\n    return &DocumentAnalyzer{\n        textExtractor:  extractors.NewTextExtractor(),\n        imageExtractor: extractors.NewImageExtractor(),\n        layoutAnalyzer: extractors.NewLayoutAnalyzer(),\n    }\n}\n\n// Add validation to ensure extractors are working\nfunc (da *DocumentAnalyzer) validateExtractors() error {\n    if da.textExtractor == nil {\n        return errors.New(\"text extractor not initialized\")\n    }\n    if da.imageExtractor == nil {\n        return errors.New(\"image extractor not initialized\")\n    }\n    if da.layoutAnalyzer == nil {\n        return errors.New(\"layout analyzer not initialized\")\n    }\n    return nil\n}\n```\n\nAdditional fixes to implement:\n\n1. **Content Stream Processing Fix**:\n```go\n// Ensure content streams are properly decoded\nfunc (p *ContentStreamParser) Parse() ([]ContentElement, error) {\n    // Add proper error handling for empty streams\n    if len(p.stream) == 0 {\n        return nil, nil // Return empty slice, not error\n    }\n    \n    // Ensure proper operator parsing\n    tokens := p.tokenize()\n    for _, token := range tokens {\n        if err := p.processToken(token); err != nil {\n            log.Printf(\"Token processing error: %v\", err)\n            // Continue processing instead of failing\n        }\n    }\n    \n    return p.elements, nil\n}\n```\n\n2. **Integration Test Fix**:\n```go\n// cmd/pdf_analyze_document/main_test.go\nfunc TestAnalyzeDocument(t *testing.T) {\n    // Test with various PDF types\n    testFiles := []string{\n        \"testdata/simple_text.pdf\",\n        \"testdata/complex_layout.pdf\",\n        \"testdata/images_only.pdf\",\n    }\n    \n    for _, file := range testFiles {\n        t.Run(file, func(t *testing.T) {\n            result, err := analyzeDocument(file)\n            assert.NoError(t, err)\n            assert.NotNil(t, result)\n            assert.Greater(t, len(result.Elements), 0)\n        })\n    }\n}\n```",
        "testStrategy": "Comprehensive testing and debugging strategy to fix the pdf_analyze_document tool:\n\n1. **Root Cause Analysis**:\n   - Add verbose logging at each step of the content extraction pipeline\n   - Test with simple PDFs first (single page, text only) to isolate the issue\n   - Use debugger to step through the extraction process\n   - Verify that the PDF parser is correctly identifying content streams\n\n2. **Unit Testing Each Component**:\n   - Test ContentStreamParser with known content streams\n   - Verify TextExtractor returns elements for basic text PDFs\n   - Test ImageExtractor with PDFs containing only images\n   - Ensure LayoutAnalyzer properly groups text elements\n\n3. **Integration Testing**:\n   - Create minimal test PDFs with known content\n   - Test the complete workflow from PDF file to analysis result\n   - Verify that content elements are properly passed between components\n   - Test with PDFs that previously failed with the error\n\n4. **Error Handling Verification**:\n   - Test with empty PDFs\n   - Test with PDFs containing only metadata (no content)\n   - Test with encrypted or corrupted PDFs\n   - Ensure graceful degradation instead of complete failure\n\n5. **Performance Testing**:\n   - Verify the fix doesn't introduce performance regressions\n   - Test with large PDFs to ensure memory usage is reasonable\n   - Profile the extraction process to identify bottlenecks\n\n6. **Regression Testing**:\n   - Create a test suite with PDFs that previously failed\n   - Ensure all test cases pass after the fix\n   - Add new test cases for edge cases discovered during debugging\n   - Set up continuous testing to prevent future regressions",
        "status": "done",
        "dependencies": [
          2,
          3,
          5,
          6,
          9
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Comprehensive Debugging and Logging Infrastructure",
            "description": "Implement detailed logging throughout the PDF analysis pipeline to trace the flow of data and identify where content elements are being lost",
            "dependencies": [],
            "details": "Add log statements in analyzeDocument() function to trace PDF parsing, content extraction, and element processing. Include logging for page counts, extracted element counts, and element types. Implement structured logging with appropriate log levels (DEBUG, INFO, WARN, ERROR) to facilitate troubleshooting.",
            "status": "done",
            "testStrategy": "Create unit tests to verify that logging captures all critical workflow steps and that log output contains expected information for both successful and failed operations"
          },
          {
            "id": 2,
            "title": "Fix Content Stream Parser Empty Stream Handling",
            "description": "Update the ContentStreamParser to properly handle empty content streams without throwing errors, ensuring the parser continues processing other valid content",
            "dependencies": [
              1
            ],
            "details": "Modify ContentStreamParser.Parse() to return an empty slice instead of an error when encountering empty streams. Add error recovery in processToken() to log errors but continue processing. Ensure tokenization handles edge cases gracefully.",
            "status": "done",
            "testStrategy": "Write tests with PDFs containing empty content streams, malformed tokens, and mixed valid/invalid content to ensure robust parsing"
          },
          {
            "id": 3,
            "title": "Implement Direct Content Extraction Fallback",
            "description": "Create a fallback mechanism in DocumentAnalyzer that attempts direct content extraction when pre-extracted content is empty or missing",
            "dependencies": [
              2
            ],
            "details": "Implement extractContentElements() method that directly extracts text, images, and layout elements from each page. Add proper error handling to continue processing even if individual page extraction fails. Ensure all extractor types (text, image, layout) are attempted for each page.",
            "status": "done",
            "testStrategy": "Test with PDFs that have varying content types on different pages, including pages with only text, only images, or mixed content"
          },
          {
            "id": 4,
            "title": "Fix Extractor Initialization and Validation",
            "description": "Ensure all content extractors (text, image, layout) are properly initialized and validated before use in the DocumentAnalyzer",
            "dependencies": [
              3
            ],
            "details": "Update NewDocumentAnalyzer() to properly initialize all extractors with required configurations. Implement validateExtractors() method to check extractor initialization. Add nil checks before using any extractor. Create factory methods for consistent extractor creation.",
            "status": "done",
            "testStrategy": "Unit test extractor initialization, validation logic, and error handling when extractors are not properly initialized"
          },
          {
            "id": 5,
            "title": "Enhance Element Type Detection and Processing",
            "description": "Improve the analyzeElement() method to properly identify and categorize different content element types with appropriate metadata extraction",
            "dependencies": [
              4
            ],
            "details": "Implement type checking for TextElement, ImageElement, and LayoutElement types. Extract relevant metadata for each element type (text content, image dimensions, layout boundaries). Add proper type assertion with error handling. Ensure ContentSummary accurately reflects element type counts.",
            "status": "done",
            "testStrategy": "Create tests with mock content elements of various types to verify correct type detection and metadata extraction"
          },
          {
            "id": 6,
            "title": "Fix Content Extraction Workflow Integration",
            "description": "Ensure the complete workflow from PDF parsing through content extraction to analysis works end-to-end without losing content elements",
            "dependencies": [
              5
            ],
            "details": "Update ExtractAll() method in ContentExtractor to properly aggregate elements from all pages. Ensure proper error propagation without stopping the entire process. Add recovery mechanisms for partial failures. Verify that Elements array is properly populated before analysis.",
            "status": "done",
            "testStrategy": "Integration tests using real PDF files with known content to verify the complete extraction pipeline produces expected results"
          },
          {
            "id": 7,
            "title": "Create Comprehensive Integration Tests",
            "description": "Develop a full test suite that validates the pdf_analyze_document tool works correctly with various PDF types and edge cases",
            "dependencies": [
              6
            ],
            "details": "Create test PDFs covering simple text, complex layouts, image-only documents, and mixed content. Test error scenarios including corrupted PDFs and unsupported formats. Verify output DocumentAnalysis structure contains expected elements and counts. Add benchmarks for performance validation.",
            "status": "done",
            "testStrategy": "Use table-driven tests with multiple PDF samples, validate both successful analysis and proper error handling, include regression tests for the original 'no content elements' error"
          }
        ]
      },
      {
        "id": 25,
        "title": "Fix pdf_get_page_info Tool Page Count and Dimension Extraction",
        "description": "Debug and fix the pdf_get_page_info tool that returns incorrect page counts (0 pages) for multi-page documents and fails to extract page dimensions and layout information, making page analysis unreliable.",
        "details": "Investigate and fix the broken pdf_get_page_info tool by debugging the page enumeration and dimension extraction logic:\n\n```go\n// cmd/pdf_get_page_info/main.go\n// Add comprehensive debugging to identify the root cause\nfunc getPageInfo(pdfPath string) (*PageInfo, error) {\n    log.Printf(\"Getting page info for: %s\", pdfPath)\n    \n    // Step 1: Debug PDF parsing and page tree traversal\n    parser := pdf.NewParser()\n    doc, err := parser.Parse(pdfPath)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to parse PDF: %w\", err)\n    }\n    \n    // Step 2: Fix page count extraction\n    pageCount, err := doc.GetPageCount()\n    log.Printf(\"Raw page count from catalog: %d\", pageCount)\n    \n    // Check if Pages tree is being traversed correctly\n    pagesDict := doc.Catalog.Get(\"Pages\")\n    if pagesDict == nil {\n        log.Printf(\"ERROR: Pages dictionary not found in catalog\")\n        return nil, errors.New(\"invalid PDF structure: missing Pages\")\n    }\n    \n    // Recursively count pages in the page tree\n    actualCount := countPagesRecursive(pagesDict)\n    log.Printf(\"Recursive page count: %d\", actualCount)\n}\n\n// internal/pdf/pageinfo/extractor.go\npackage pageinfo\n\ntype PageInfoExtractor struct {\n    parser *pdf.Parser\n    cache  map[int]*PageDimensions\n}\n\nfunc (pie *PageInfoExtractor) ExtractPageInfo(doc *PDFDocument) (*DocumentPageInfo, error) {\n    info := &DocumentPageInfo{\n        TotalPages: 0,\n        Pages:      make([]PageDimensions, 0),\n    }\n    \n    // Fix 1: Properly traverse the page tree\n    pageTree := doc.Catalog.GetDict(\"Pages\")\n    if pageTree == nil {\n        return nil, errors.New(\"no page tree found\")\n    }\n    \n    // Get count from Pages dictionary\n    if count := pageTree.GetInt(\"Count\"); count != nil {\n        info.TotalPages = *count\n        log.Printf(\"Page count from Pages dict: %d\", *count)\n    }\n    \n    // Fix 2: Recursively collect all page objects\n    pages, err := pie.collectPages(pageTree)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to collect pages: %w\", err)\n    }\n    \n    // Fix 3: Extract dimensions for each page\n    for i, pageObj := range pages {\n        dims, err := pie.extractPageDimensions(pageObj, i+1)\n        if err != nil {\n            log.Printf(\"Warning: failed to extract dimensions for page %d: %v\", i+1, err)\n            // Use default dimensions as fallback\n            dims = &PageDimensions{\n                PageNumber: i + 1,\n                Width:      612.0,  // Letter width in points\n                Height:     792.0,  // Letter height in points\n                Rotation:   0,\n            }\n        }\n        info.Pages = append(info.Pages, *dims)\n    }\n    \n    return info, nil\n}\n\nfunc (pie *PageInfoExtractor) collectPages(node PDFObject) ([]PDFObject, error) {\n    pages := []PDFObject{}\n    \n    nodeType := node.GetName(\"Type\")\n    if nodeType == \"Pages\" {\n        // This is a Pages node, recursively collect from Kids\n        kids := node.GetArray(\"Kids\")\n        if kids == nil {\n            return nil, errors.New(\"Pages node missing Kids array\")\n        }\n        \n        for _, kid := range kids {\n            // Resolve indirect references\n            if ref, ok := kid.(PDFReference); ok {\n                kid = pie.parser.ResolveReference(ref)\n            }\n            \n            childPages, err := pie.collectPages(kid)\n            if err != nil {\n                return nil, err\n            }\n            pages = append(pages, childPages...)\n        }\n    } else if nodeType == \"Page\" {\n        // This is a leaf Page node\n        pages = append(pages, node)\n    }\n    \n    return pages, nil\n}\n\nfunc (pie *PageInfoExtractor) extractPageDimensions(pageObj PDFObject, pageNum int) (*PageDimensions, error) {\n    dims := &PageDimensions{\n        PageNumber: pageNum,\n    }\n    \n    // Fix 4: Handle inherited MediaBox from parent Pages nodes\n    mediaBox := pie.getInheritedRectangle(pageObj, \"MediaBox\")\n    if mediaBox == nil {\n        return nil, errors.New(\"no MediaBox found\")\n    }\n    \n    // MediaBox format: [llx lly urx ury]\n    if len(mediaBox) != 4 {\n        return nil, fmt.Errorf(\"invalid MediaBox format: %v\", mediaBox)\n    }\n    \n    dims.Width = mediaBox[2] - mediaBox[0]\n    dims.Height = mediaBox[3] - mediaBox[1]\n    \n    // Fix 5: Check for CropBox (visible area)\n    cropBox := pie.getInheritedRectangle(pageObj, \"CropBox\")\n    if cropBox != nil && len(cropBox) == 4 {\n        dims.CropBox = &Rectangle{\n            X:      cropBox[0],\n            Y:      cropBox[1],\n            Width:  cropBox[2] - cropBox[0],\n            Height: cropBox[3] - cropBox[1],\n        }\n    }\n    \n    // Fix 6: Handle page rotation\n    rotation := pie.getInheritedInt(pageObj, \"Rotate\")\n    if rotation != nil {\n        dims.Rotation = *rotation % 360\n        // Swap dimensions for 90/270 degree rotations\n        if dims.Rotation == 90 || dims.Rotation == 270 {\n            dims.Width, dims.Height = dims.Height, dims.Width\n        }\n    }\n    \n    return dims, nil\n}\n\n// Helper to get inherited values from parent Pages nodes\nfunc (pie *PageInfoExtractor) getInheritedRectangle(pageObj PDFObject, key string) []float64 {\n    // Check current page first\n    if rect := pageObj.GetArray(key); rect != nil {\n        return pie.parseRectangle(rect)\n    }\n    \n    // Check parent Pages nodes\n    parent := pageObj.GetDict(\"Parent\")\n    if parent != nil {\n        return pie.getInheritedRectangle(parent, key)\n    }\n    \n    return nil\n}\n\n// pkg/models/pageinfo.go\ntype DocumentPageInfo struct {\n    TotalPages int              `json:\"total_pages\"`\n    Pages      []PageDimensions `json:\"pages\"`\n}\n\ntype PageDimensions struct {\n    PageNumber int        `json:\"page_number\"`\n    Width      float64    `json:\"width\"`\n    Height     float64    `json:\"height\"`\n    Rotation   int        `json:\"rotation\"`\n    CropBox    *Rectangle `json:\"crop_box,omitempty\"`\n    Unit       string     `json:\"unit\"` // \"points\", \"inches\", \"mm\"\n}",
        "testStrategy": "Comprehensive testing and debugging strategy to fix the pdf_get_page_info tool:\n\n1. **Root Cause Debugging**:\n   - Add extensive logging to trace page tree traversal\n   - Test with simple single-page PDF first to verify basic functionality\n   - Use PDF debugging tools (qpdf, pdftk) to verify actual page counts\n   - Compare tool output with known PDF metadata\n\n2. **Page Count Tests**:\n   - Test with PDFs containing 0, 1, 10, 100, 1000+ pages\n   - Test with PDFs having complex page trees (nested Pages nodes)\n   - Test with linearized PDFs\n   - Test with PDFs having deleted/removed pages\n   - Verify count matches other PDF tools (pdfinfo, qpdf --show-npages)\n\n3. **Dimension Extraction Tests**:\n   - Test standard page sizes: Letter (612x792), A4 (595x842), Legal\n   - Test custom page sizes and mixed page sizes within same document\n   - Test rotated pages (0, 90, 180, 270)\n   - Test with CropBox, BleedBox, TrimBox, ArtBox\n   - Test inheritance of MediaBox from parent Pages nodes\n\n4. **Edge Cases**:\n   - Test corrupted PDFs with missing Pages dictionary\n   - Test PDFs with circular references in page tree\n   - Test encrypted PDFs (should fail gracefully)\n   - Test PDFs with non-standard page tree structures\n   - Test very large PDFs (memory efficiency)\n\n5. **Integration Tests**:\n   - Verify fixed tool works with pdf_analyze_document workflow\n   - Test with real-world PDFs: books, reports, forms, scanned documents\n   - Benchmark performance on large documents\n   - Verify JSON output format is consistent and parseable",
        "status": "done",
        "dependencies": [
          2,
          3,
          4
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 26,
        "title": "Fix pdf_get_metadata Tool Empty Results Issue",
        "description": "Debug and fix the pdf_get_metadata tool that returns empty results despite PDFs containing rich metadata, while pdf_stats_file successfully extracts the same metadata, indicating a parsing or data mapping issue in the metadata extraction logic.",
        "details": "Investigate and fix the broken pdf_get_metadata tool by analyzing the discrepancy with pdf_stats_file:\n\n```go\n// cmd/pdf_get_metadata/main.go\n// Debug the metadata extraction to identify why it returns empty results\nfunc getMetadata(pdfPath string) (*PDFMetadata, error) {\n    log.Printf(\"Extracting metadata from: %s\", pdfPath)\n    \n    // Step 1: Add debugging to trace metadata extraction\n    parser := pdf.NewParser()\n    doc, err := parser.Parse(pdfPath)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to parse PDF: %w\", err)\n    }\n    \n    // Step 2: Debug Info dictionary extraction\n    log.Printf(\"Document parsed, extracting Info dictionary\")\n    infoDictObj := doc.GetTrailer().Get(\"Info\")\n    if infoDictObj == nil {\n        log.Printf(\"WARNING: No Info dictionary found in trailer\")\n        // Check if metadata is in catalog instead\n        catalog := doc.GetCatalog()\n        if catalog != nil {\n            metadataStream := catalog.Get(\"Metadata\")\n            if metadataStream != nil {\n                log.Printf(\"Found metadata in catalog instead of Info dict\")\n            }\n        }\n    }\n    \n    // Step 3: Compare with pdf_stats_file implementation\n    // The working implementation might be using different extraction methods\n    metadata := &PDFMetadata{\n        Title:        extractString(infoDictObj, \"Title\"),\n        Author:       extractString(infoDictObj, \"Author\"),\n        Subject:      extractString(infoDictObj, \"Subject\"),\n        Keywords:     extractString(infoDictObj, \"Keywords\"),\n        Creator:      extractString(infoDictObj, \"Creator\"),\n        Producer:     extractString(infoDictObj, \"Producer\"),\n        CreationDate: extractDate(infoDictObj, \"CreationDate\"),\n        ModDate:      extractDate(infoDictObj, \"ModDate\"),\n    }\n    \n    // Step 4: Check XMP metadata stream\n    if isEmpty(metadata) {\n        log.Printf(\"Info dict empty, checking XMP metadata\")\n        xmpMetadata, err := extractXMPMetadata(doc)\n        if err == nil && xmpMetadata != nil {\n            mergeMetadata(metadata, xmpMetadata)\n        }\n    }\n    \n    return metadata, nil\n}\n\n// Fix string extraction to handle various encodings\nfunc extractString(dict pdf.Object, key string) string {\n    if dict == nil {\n        return \"\"\n    }\n    \n    obj := dict.Get(key)\n    if obj == nil {\n        return \"\"\n    }\n    \n    // Handle different string encodings\n    switch v := obj.(type) {\n    case *pdf.StringObject:\n        // Check for UTF-16BE BOM\n        if bytes.HasPrefix(v.Bytes(), []byte{0xFE, 0xFF}) {\n            return decodeUTF16BE(v.Bytes()[2:])\n        }\n        // Try UTF-8 first, fall back to Latin-1\n        if utf8.Valid(v.Bytes()) {\n            return string(v.Bytes())\n        }\n        return decodeLatin1(v.Bytes())\n    case *pdf.NameObject:\n        return v.String()\n    default:\n        log.Printf(\"Unexpected type for %s: %T\", key, v)\n        return \"\"\n    }\n}\n\n// Compare with pdf_stats_file implementation\nfunc analyzeImplementationDifferences() {\n    // Key differences to investigate:\n    // 1. pdf_stats_file might be using a different PDF library\n    // 2. It might handle string encoding differently\n    // 3. It might look in additional locations for metadata\n    // 4. It might have better error handling for malformed metadata\n}\n\n// Add comprehensive metadata search\nfunc findAllMetadata(doc *pdf.Document) map[string]interface{} {\n    metadata := make(map[string]interface{})\n    \n    // 1. Check trailer Info dictionary\n    if info := doc.GetTrailer().Get(\"Info\"); info != nil {\n        metadata[\"info\"] = extractInfoDict(info)\n    }\n    \n    // 2. Check catalog metadata stream\n    if catalog := doc.GetCatalog(); catalog != nil {\n        if mdStream := catalog.Get(\"Metadata\"); mdStream != nil {\n            metadata[\"xmp\"] = extractXMPStream(mdStream)\n        }\n    }\n    \n    // 3. Check document information dictionary\n    if docInfo := doc.GetDocumentInfo(); docInfo != nil {\n        metadata[\"docInfo\"] = docInfo\n    }\n    \n    return metadata\n}\n```\n\nKey areas to investigate:\n\n1. **String Encoding Issues**:\n   - PDF strings can be encoded in multiple ways (PDFDocEncoding, UTF-16BE)\n   - The tool might not be handling all encoding types correctly\n   - Add proper encoding detection and conversion\n\n2. **Metadata Location**:\n   - Metadata can be in Info dictionary (trailer)\n   - Metadata can be in XMP stream (catalog)\n   - Some PDFs use non-standard locations\n\n3. **Implementation Comparison**:\n   - Analyze pdf_stats_file source to understand its approach\n   - Identify key differences in parsing logic\n   - Port successful techniques to pdf_get_metadata\n\n4. **Error Handling**:\n   - Add comprehensive error logging\n   - Handle malformed or missing metadata gracefully\n   - Provide partial results when possible",
        "testStrategy": "Comprehensive testing and debugging strategy to fix pdf_get_metadata:\n\n1. **Comparative Analysis**:\n   - Run both pdf_get_metadata and pdf_stats_file on the same test PDFs\n   - Compare the output to identify what pdf_stats_file extracts successfully\n   - Use PDF inspection tools (qpdf --show-all-pages) to verify actual metadata\n\n2. **Test Cases**:\n   - Test with PDFs known to have metadata (Adobe-generated PDFs)\n   - Test with PDFs using different metadata encodings (UTF-16BE, Latin-1)\n   - Test with PDFs having XMP metadata streams\n   - Test with PDFs having metadata in non-standard locations\n\n3. **Debugging Steps**:\n   - Add verbose logging to trace each step of metadata extraction\n   - Log raw bytes of metadata strings before decoding\n   - Compare byte-by-byte with pdf_stats_file extraction\n   - Use hex dumps to identify encoding issues\n\n4. **Validation**:\n   - Verify all standard metadata fields are extracted (Title, Author, Subject, etc.)\n   - Check date parsing handles PDF date format (D:YYYYMMDDHHmmSSOHH'mm)\n   - Ensure Unicode metadata is properly decoded\n   - Test with multilingual metadata (Japanese, Arabic, etc.)\n\n5. **Success Metrics**:\n   - Tool should achieve >90% success rate on test corpus\n   - All metadata fields present in pdf_stats_file output should be extracted\n   - No empty results for PDFs with verified metadata\n   - Performance should be comparable to pdf_stats_file",
        "status": "done",
        "dependencies": [
          2,
          3
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze pdf_stats_file Implementation",
            "description": "Examine the working pdf_stats_file tool's source code to understand its metadata extraction approach and identify key differences with pdf_get_metadata",
            "dependencies": [],
            "details": "Locate and analyze pdf_stats_file source code, focusing on: 1) Which PDF parsing library it uses, 2) How it extracts metadata from PDFs, 3) String encoding handling methods, 4) Metadata location search patterns. Document all findings in comments for reference during implementation.",
            "status": "done",
            "testStrategy": "Create a test PDF with known metadata and verify that pdf_stats_file correctly extracts it while pdf_get_metadata fails, documenting the exact differences in output"
          },
          {
            "id": 2,
            "title": "Implement Comprehensive String Encoding Support",
            "description": "Fix the extractString function to properly handle all PDF string encoding types including PDFDocEncoding, UTF-16BE, and UTF-8",
            "dependencies": [
              1
            ],
            "details": "Enhance extractString function to: 1) Detect UTF-16BE BOM (0xFE 0xFF), 2) Implement proper UTF-16BE decoding, 3) Add PDFDocEncoding to UTF-8 conversion, 4) Handle Latin-1 fallback correctly, 5) Add logging for encoding detection. Create helper functions decodeUTF16BE() and decodeLatin1() as needed.",
            "status": "done",
            "testStrategy": "Create unit tests with PDF strings in various encodings (UTF-8, UTF-16BE with BOM, PDFDocEncoding, Latin-1) and verify correct decoding"
          },
          {
            "id": 3,
            "title": "Add Multiple Metadata Location Search",
            "description": "Implement comprehensive metadata search that checks all possible locations where PDF metadata can be stored",
            "dependencies": [
              1
            ],
            "details": "Implement findAllMetadata function to search: 1) Trailer Info dictionary, 2) Catalog Metadata stream (XMP), 3) Document information dictionary, 4) Any non-standard locations identified from pdf_stats_file analysis. Ensure proper null checking and error handling for each location.",
            "status": "done",
            "testStrategy": "Test with PDFs that store metadata in different locations: traditional Info dict, XMP streams, and hybrid approaches"
          },
          {
            "id": 4,
            "title": "Implement XMP Metadata Extraction",
            "description": "Create robust XMP metadata extraction and parsing functionality to handle PDFs that store metadata in XML format",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement extractXMPMetadata() function to: 1) Extract XMP stream from catalog, 2) Parse XML content safely, 3) Map XMP fields to PDFMetadata struct fields, 4) Handle namespaces (dc:, xmp:, pdf:), 5) Implement mergeMetadata() to combine XMP data with Info dict data.",
            "status": "done",
            "testStrategy": "Test with PDFs containing only XMP metadata, only Info dict metadata, and both types to ensure proper extraction and merging"
          },
          {
            "id": 5,
            "title": "Add Comprehensive Error Handling and Logging",
            "description": "Implement detailed error handling and diagnostic logging throughout the metadata extraction process",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Add structured logging to trace: 1) Each metadata extraction attempt, 2) Encoding detection results, 3) Found vs missing metadata fields, 4) Any parsing errors with context. Implement graceful degradation to return partial results when some metadata is corrupted. Add isEmpty() helper to check if metadata struct is empty.",
            "status": "done",
            "testStrategy": "Test with corrupted PDFs, PDFs with partial metadata, and PDFs with malformed encoding to ensure graceful handling and useful error messages"
          },
          {
            "id": 6,
            "title": "Integration Testing and Performance Optimization",
            "description": "Perform comprehensive integration testing comparing pdf_get_metadata with pdf_stats_file and optimize performance",
            "dependencies": [
              5
            ],
            "details": "1) Create test suite with diverse PDF samples (different creators, versions, metadata locations), 2) Compare outputs between fixed pdf_get_metadata and pdf_stats_file, 3) Ensure 100% metadata extraction parity, 4) Profile and optimize any performance bottlenecks, 5) Update documentation with supported metadata formats.",
            "status": "done",
            "testStrategy": "Run parallel tests comparing both tools on a corpus of 100+ PDFs with various metadata configurations, measuring extraction accuracy and performance"
          }
        ]
      },
      {
        "id": 27,
        "title": "Debug and Fix Form Field Detection in pdf_extract_forms Tool",
        "description": "Investigate and fix the pdf_extract_forms tool's failure to detect form fields in valid PDF forms (tax documents, W-2s, 1099s), addressing the root cause of the tool returning 0 form elements and restoring form processing functionality.",
        "details": "Debug and fix the form field detection issue in the pdf_extract_forms tool by systematically investigating the parsing pipeline:\n\n```go\n// cmd/pdf_extract_forms/main.go\n// Add comprehensive debugging and logging\nfunc main() {\n    // Enable verbose logging for form detection\n    log.SetLevel(log.DebugLevel)\n    \n    // Add diagnostic mode flag\n    diagnosticMode := flag.Bool(\"diagnostic\", false, \"Enable diagnostic output for form detection\")\n    flag.Parse()\n    \n    if *diagnosticMode {\n        enableFormDiagnostics()\n    }\n}\n\n// internal/pdf/forms/debug.go\npackage forms\n\ntype FormDebugger struct {\n    writer io.Writer\n    level  DebugLevel\n}\n\nfunc (fd *FormDebugger) TraceAcroFormLookup(catalog map[string]interface{}) {\n    fd.log(\"Searching for AcroForm in catalog...\")\n    if acroForm, exists := catalog[\"AcroForm\"]; exists {\n        fd.log(\"AcroForm found: %+v\", acroForm)\n    } else {\n        fd.log(\"WARNING: No AcroForm entry in catalog\")\n        fd.log(\"Catalog keys: %v\", keys(catalog))\n    }\n}\n\nfunc (fd *FormDebugger) TraceFieldParsing(field map[string]interface{}) {\n    fd.log(\"Parsing field: FT=%v, T=%v, V=%v\", \n        field[\"FT\"], field[\"T\"], field[\"V\"])\n    \n    // Check for widget annotations\n    if kids, hasKids := field[\"Kids\"]; hasKids {\n        fd.log(\"Field has %d children\", len(kids))\n    }\n}\n\n// internal/pdf/wrapper/form_detection_fix.go\nfunc (p *PDFCPUBackend) ExtractFormFields() ([]FormField, error) {\n    // Fix 1: Handle indirect references properly\n    catalog := p.ctx.Catalog\n    \n    // Check both direct and indirect AcroForm references\n    acroFormObj := catalog[\"AcroForm\"]\n    if indRef, isIndirect := acroFormObj.(model.IndirectRef); isIndirect {\n        // Dereference indirect object\n        acroFormObj, _ = p.ctx.Dereference(indRef)\n    }\n    \n    // Fix 2: Handle XFA forms (common in tax documents)\n    if xfa := p.checkForXFAForms(acroFormObj); xfa != nil {\n        return p.parseXFAForms(xfa)\n    }\n    \n    // Fix 3: Recursive field search\n    fields := p.recursiveFieldSearch(acroFormObj)\n    \n    // Fix 4: Check for fields in page annotations\n    if len(fields) == 0 {\n        fields = p.searchPageAnnotations()\n    }\n    \n    return fields, nil\n}\n\nfunc (p *PDFCPUBackend) checkForXFAForms(acroForm map[string]interface{}) interface{} {\n    // Many tax forms use XFA (XML Forms Architecture)\n    if xfa, exists := acroForm[\"XFA\"]; exists {\n        log.Debug(\"XFA forms detected\")\n        return xfa\n    }\n    return nil\n}\n\nfunc (p *PDFCPUBackend) searchPageAnnotations() []FormField {\n    // Some PDFs store form fields as page annotations\n    var fields []FormField\n    \n    for pageNum := 1; pageNum <= p.ctx.PageCount; pageNum++ {\n        page, _ := p.ctx.Page(pageNum)\n        if annots, exists := page.Dict[\"Annots\"]; exists {\n            // Parse widget annotations\n            fields = append(fields, p.parseWidgetAnnotations(annots)...)\n        }\n    }\n    \n    return fields\n}\n\n// Fix for specific tax document formats\nfunc (p *PDFCPUBackend) handleTaxDocumentQuirks() {\n    // W-2 and 1099 forms often use non-standard field structures\n    // Implement specific handlers for common tax form formats\n    \n    // Check for flattened forms\n    if p.isFlattened() {\n        log.Warn(\"Form appears to be flattened - fields may not be extractable\")\n    }\n}\n```\n\nKey fixes to implement:\n\n1. **Indirect Reference Resolution**: Properly handle indirect object references in the AcroForm dictionary\n2. **XFA Form Support**: Add support for XFA (XML Forms Architecture) used in many tax documents\n3. **Page Annotation Search**: Search for form fields stored as page annotations\n4. **Recursive Field Discovery**: Implement deep recursive search for nested field structures\n5. **Tax Document Quirks**: Handle specific formatting quirks common in W-2s and 1099s\n6. **Flattened Form Detection**: Detect and warn about flattened forms where fields are rendered as static content",
        "testStrategy": "Comprehensive testing strategy to verify form field detection fixes:\n\n1. **Tax Document Test Suite**:\n   - Collect sample W-2, 1099, and other tax forms from multiple sources\n   - Test with forms from major tax software (TurboTax, H&R Block, etc.)\n   - Verify detection of all expected fields (SSN, wages, tax withheld, etc.)\n   - Create regression test suite with known problematic documents\n\n2. **Diagnostic Output Verification**:\n   - Run tool with --diagnostic flag on failing documents\n   - Verify detailed logging shows AcroForm lookup process\n   - Check that indirect references are properly resolved\n   - Confirm XFA detection when present\n\n3. **Field Detection Tests**:\n   - Test with forms containing indirect AcroForm references\n   - Test with XFA-based forms (common in government documents)\n   - Test with forms using page annotations for fields\n   - Test with nested field hierarchies\n   - Test with flattened vs. interactive forms\n\n4. **Integration Testing**:\n   - Run fixed tool against production tax documents\n   - Compare field counts with Adobe Acrobat's form field list\n   - Verify all field types are properly detected (text, checkbox, signature)\n   - Test field value extraction for pre-filled forms\n\n5. **Performance Regression**:\n   - Ensure fixes don't significantly impact processing speed\n   - Test with large forms (50+ fields)\n   - Verify memory usage remains reasonable\n\n6. **Error Handling**:\n   - Test with corrupted form dictionaries\n   - Test with partially valid forms\n   - Verify graceful degradation and helpful error messages",
        "status": "done",
        "dependencies": [
          12,
          14
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Comprehensive Debugging Infrastructure",
            "description": "Create a robust debugging framework with diagnostic mode, trace logging, and detailed form detection analysis capabilities to identify why form fields are not being detected",
            "dependencies": [],
            "details": "Create internal/pdf/forms/debug.go with FormDebugger struct implementing TraceAcroFormLookup, TraceFieldParsing, and other diagnostic methods. Add -diagnostic flag to main.go that enables verbose logging. Implement structured logging that traces catalog inspection, AcroForm lookup, field enumeration, and any parsing failures. Include helper functions to dump PDF structure and identify missing or malformed form elements.",
            "status": "done",
            "testStrategy": "Create unit tests with sample PDFs containing known form structures. Verify diagnostic output correctly identifies presence/absence of AcroForm, fields count, and parsing steps. Test with both valid and malformed PDF structures."
          },
          {
            "id": 2,
            "title": "Fix Indirect Reference Resolution in AcroForm Lookup",
            "description": "Implement proper handling of indirect object references when accessing the AcroForm dictionary, as tax documents often use indirect references that current implementation may not resolve correctly",
            "dependencies": [
              1
            ],
            "details": "In ExtractFormFields method, check if catalog['AcroForm'] returns an IndirectRef type. If so, use ctx.Dereference() to resolve the actual object. Implement error handling for failed dereferences. Add logging to track reference resolution. Handle nested indirect references that may occur in complex PDF structures. Ensure the resolved object is properly type-checked before processing.",
            "status": "done",
            "testStrategy": "Test with PDFs using direct AcroForm references, single indirect references, and nested indirect references. Verify correct resolution in all cases. Use diagnostic mode to confirm reference chains are followed correctly."
          },
          {
            "id": 3,
            "title": "Add XFA Form Support for Tax Documents",
            "description": "Implement parsing support for XFA (XML Forms Architecture) forms commonly used in W-2s, 1099s, and other tax documents, which use XML-based form definitions instead of traditional AcroForm fields",
            "dependencies": [
              2
            ],
            "details": "Create checkForXFAForms method to detect XFA presence in AcroForm dictionary. Implement parseXFAForms to extract XML data from XFA streams and parse form field definitions. Handle both XFA-only and hybrid (XFA+AcroForm) documents. Extract field names, types, and values from XML structure. Map XFA field types to standard FormField structure. Add specific handling for common tax form XFA schemas.",
            "status": "done",
            "testStrategy": "Test with actual W-2 and 1099 PDF samples that use XFA. Verify field extraction matches expected tax form fields. Test hybrid documents to ensure both XFA and AcroForm fields are captured."
          },
          {
            "id": 4,
            "title": "Implement Page Annotation and Recursive Field Search",
            "description": "Add fallback mechanisms to search for form fields in page annotations and implement deep recursive search for nested field structures that may not be directly referenced in AcroForm",
            "dependencies": [
              2
            ],
            "details": "Implement searchPageAnnotations to iterate through all pages and check Annots array for Widget type annotations. Create parseWidgetAnnotations to extract form field data from annotation dictionaries. Implement recursiveFieldSearch to handle fields with Kids arrays and nested field hierarchies. Add support for field inheritance where child fields inherit properties from parents. Handle both terminal fields and non-terminal field nodes in the hierarchy.",
            "status": "done",
            "testStrategy": "Test with PDFs that store fields as page annotations without AcroForm. Verify nested field structures are fully traversed. Test field inheritance scenarios where child fields lack explicit properties."
          },
          {
            "id": 5,
            "title": "Handle Tax Document Quirks and Flattened Forms",
            "description": "Implement specific handlers for common tax document formatting quirks and add detection for flattened forms where interactive fields have been converted to static content",
            "dependencies": [
              3,
              4
            ],
            "details": "Create handleTaxDocumentQuirks method to detect and handle W-2/1099 specific patterns. Implement isFlattened detection by checking for absence of form fields but presence of form-like content patterns. Add heuristics to identify common tax form layouts even when fields are missing. Implement warning system for flattened forms. Add special handling for forms that use non-standard field naming conventions common in tax software. Create mapping tables for common tax form field variations.",
            "status": "done",
            "testStrategy": "Test with variety of real tax documents including flattened PDFs. Verify quirk handlers correctly identify and process non-standard formats. Test warning generation for flattened forms. Validate field name normalization works across different tax software outputs."
          }
        ]
      },
      {
        "id": 28,
        "title": "Fix MediaBox Parsing Errors in pdf_extract_structured Tool",
        "description": "Debug and fix the pdf_extract_structured tool's MediaBox parsing failures that prevent advanced extraction features from working on certain PDF documents, improving the robustness of PDF page dimension parsing.",
        "details": "Investigate and fix the MediaBox parsing errors in the pdf_extract_structured tool by implementing robust page dimension parsing:\n\n```go\n// internal/pdf/parser/page_parser.go\n// Fix MediaBox parsing to handle various PDF specifications\ntype PageParser struct {\n    objectResolver *ObjectResolver\n    logger         *log.Logger\n}\n\nfunc (pp *PageParser) ParseMediaBox(pageDict map[string]interface{}) (*Rectangle, error) {\n    pp.logger.Debug(\"Parsing MediaBox from page dictionary\")\n    \n    // Step 1: Handle MediaBox inheritance from parent pages\n    mediaBox := pp.findInheritedAttribute(pageDict, \"MediaBox\")\n    if mediaBox == nil {\n        // Default to US Letter size if MediaBox is missing\n        pp.logger.Warn(\"MediaBox not found, using default US Letter size\")\n        return &Rectangle{\n            LLX: 0,\n            LLY: 0,\n            URX: 612,  // 8.5 inches * 72 DPI\n            URY: 792,  // 11 inches * 72 DPI\n        }, nil\n    }\n    \n    // Step 2: Handle various MediaBox formats\n    switch v := mediaBox.(type) {\n    case []interface{}:\n        return pp.parseRectangleArray(v)\n    case *PDFArray:\n        return pp.parseRectangleArray(v.Elements)\n    case *PDFIndirectObject:\n        // Resolve indirect reference\n        resolved, err := pp.objectResolver.ResolveObject(v)\n        if err != nil {\n            return nil, fmt.Errorf(\"failed to resolve MediaBox reference: %w\", err)\n        }\n        return pp.ParseMediaBox(map[string]interface{}{\"MediaBox\": resolved})\n    default:\n        return nil, fmt.Errorf(\"unexpected MediaBox type: %T\", v)\n    }\n}\n\nfunc (pp *PageParser) parseRectangleArray(arr []interface{}) (*Rectangle, error) {\n    if len(arr) != 4 {\n        return nil, fmt.Errorf(\"invalid MediaBox array length: %d\", len(arr))\n    }\n    \n    rect := &Rectangle{}\n    coords := []*float64{&rect.LLX, &rect.LLY, &rect.URX, &rect.URY}\n    \n    for i, val := range arr {\n        num, err := pp.parseNumber(val)\n        if err != nil {\n            return nil, fmt.Errorf(\"invalid coordinate at index %d: %w\", i, err)\n        }\n        *coords[i] = num\n    }\n    \n    // Validate rectangle\n    if rect.URX <= rect.LLX || rect.URY <= rect.LLY {\n        return nil, fmt.Errorf(\"invalid MediaBox dimensions: %v\", rect)\n    }\n    \n    return rect, nil\n}\n\nfunc (pp *PageParser) parseNumber(val interface{}) (float64, error) {\n    switch v := val.(type) {\n    case float64:\n        return v, nil\n    case int:\n        return float64(v), nil\n    case int64:\n        return float64(v), nil\n    case *PDFNumber:\n        return v.Value, nil\n    case *PDFInteger:\n        return float64(v.Value), nil\n    case string:\n        // Handle numeric strings\n        return strconv.ParseFloat(v, 64)\n    default:\n        return 0, fmt.Errorf(\"cannot parse number from type %T\", v)\n    }\n}\n\nfunc (pp *PageParser) findInheritedAttribute(pageDict map[string]interface{}, attr string) interface{} {\n    // Check current page\n    if val, exists := pageDict[attr]; exists {\n        return val\n    }\n    \n    // Check parent pages recursively\n    if parent, exists := pageDict[\"Parent\"]; exists {\n        if parentDict, ok := parent.(map[string]interface{}); ok {\n            return pp.findInheritedAttribute(parentDict, attr)\n        }\n    }\n    \n    return nil\n}\n\n// cmd/pdf_extract_structured/main.go\n// Update the tool to use robust MediaBox parsing\nfunc extractStructured(pdfPath string) (*StructuredContent, error) {\n    // Enable debug logging for MediaBox issues\n    logger := log.New(os.Stderr, \"[MediaBox] \", log.LstdFlags)\n    \n    parser := pdf.NewParser(pdf.WithLogger(logger))\n    doc, err := parser.Parse(pdfPath)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to parse PDF: %w\", err)\n    }\n    \n    structured := &StructuredContent{\n        Pages: make([]PageStructure, 0),\n    }\n    \n    // Process each page with error recovery\n    for i, page := range doc.Pages {\n        logger.Printf(\"Processing page %d\", i+1)\n        \n        pageStruct, err := extractPageStructure(page)\n        if err != nil {\n            // Log error but continue processing other pages\n            logger.Printf(\"Warning: Failed to extract page %d: %v\", i+1, err)\n            \n            // Add placeholder page with error info\n            pageStruct = PageStructure{\n                PageNumber: i + 1,\n                Error:      err.Error(),\n                MediaBox:   getDefaultMediaBox(),\n            }\n        }\n        \n        structured.Pages = append(structured.Pages, pageStruct)\n    }\n    \n    return structured, nil\n}\n\n// internal/pdf/recovery/mediabox_recovery.go\n// Implement MediaBox recovery strategies\ntype MediaBoxRecovery struct {\n    logger *log.Logger\n}\n\nfunc (mr *MediaBoxRecovery) RecoverMediaBox(pageObj map[string]interface{}) (*Rectangle, error) {\n    strategies := []func(map[string]interface{}) (*Rectangle, error){\n        mr.tryDirectMediaBox,\n        mr.tryInheritedMediaBox,\n        mr.tryCropBox,\n        mr.tryArtBox,\n        mr.tryDefaultSize,\n    }\n    \n    for _, strategy := range strategies {\n        if rect, err := strategy(pageObj); err == nil && rect != nil {\n            mr.logger.Printf(\"MediaBox recovered using strategy: %v\", rect)\n            return rect, nil\n        }\n    }\n    \n    return nil, fmt.Errorf(\"all MediaBox recovery strategies failed\")\n}\n\nfunc (mr *MediaBoxRecovery) tryDirectMediaBox(pageObj map[string]interface{}) (*Rectangle, error) {\n    if mb, exists := pageObj[\"MediaBox\"]; exists {\n        return parseMediaBoxValue(mb)\n    }\n    return nil, fmt.Errorf(\"no direct MediaBox found\")\n}\n\nfunc (mr *MediaBoxRecovery) tryCropBox(pageObj map[string]interface{}) (*Rectangle, error) {\n    // CropBox can be used as fallback for MediaBox\n    if cb, exists := pageObj[\"CropBox\"]; exists {\n        mr.logger.Warn(\"Using CropBox as MediaBox fallback\")\n        return parseMediaBoxValue(cb)\n    }\n    return nil, fmt.Errorf(\"no CropBox found\")\n}\n\n// pkg/models/structured.go\n// Update structured content model to include error handling\ntype PageStructure struct {\n    PageNumber int         `json:\"pageNumber\"`\n    MediaBox   *Rectangle  `json:\"mediaBox\"`\n    Content    []Content   `json:\"content,omitempty\"`\n    Error      string      `json:\"error,omitempty\"`\n}\n```",
        "testStrategy": "Comprehensive testing strategy to verify MediaBox parsing fixes:\n\n1. **Error Reproduction**:\n   - Collect PDF samples that trigger \"invalid MediaBox\" errors\n   - Create test cases for various MediaBox formats (direct arrays, indirect references, inherited values)\n   - Test with PDFs missing MediaBox entirely\n   - Verify the exact error conditions and stack traces\n\n2. **MediaBox Format Testing**:\n   - Test PDFs with MediaBox as direct array: [0 0 612 792]\n   - Test with indirect object references: \"MediaBox 5 0 R\"\n   - Test with inherited MediaBox from parent page tree nodes\n   - Test with numeric strings vs numeric values\n   - Test with integer vs float coordinates\n\n3. **Edge Case Validation**:\n   - PDFs with rotated pages (MediaBox with Rotate attribute)\n   - Non-standard page sizes (A4, Legal, custom dimensions)\n   - Negative coordinates in MediaBox\n   - MediaBox with CropBox/BleedBox/TrimBox/ArtBox\n   - Malformed MediaBox arrays (wrong number of elements, non-numeric values)\n\n4. **Recovery Testing**:\n   - Verify fallback to CropBox when MediaBox is missing\n   - Test default page size assignment (US Letter)\n   - Ensure tool continues processing remaining pages after MediaBox errors\n   - Validate error reporting includes page numbers and specific issues\n\n5. **Integration Testing**:\n   - Run pdf_extract_structured on previously failing documents\n   - Verify all advanced extraction features work after MediaBox fix\n   - Compare extracted dimensions with PDF viewers (Adobe Reader, Preview)\n   - Test performance impact of robust parsing",
        "status": "done",
        "dependencies": [
          2,
          3,
          4,
          10
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Robust Number Parsing for PDF Values",
            "description": "Create a comprehensive number parsing function that handles all PDF numeric types including integers, floats, PDFNumber objects, PDFInteger objects, and numeric strings",
            "dependencies": [],
            "details": "Implement the parseNumber method in PageParser that safely converts various PDF value types to float64. Handle edge cases like nil values, invalid strings, and type assertions. Add proper error messages for debugging.",
            "status": "done",
            "testStrategy": "Create unit tests with various input types: float64, int, int64, PDFNumber, PDFInteger, numeric strings, and invalid inputs. Verify correct conversion and error handling."
          },
          {
            "id": 2,
            "title": "Build Rectangle Array Parser with Validation",
            "description": "Implement the parseRectangleArray function to convert PDF array representations into Rectangle structs with proper coordinate validation",
            "dependencies": [
              1
            ],
            "details": "Parse 4-element arrays into Rectangle structs (LLX, LLY, URX, URY). Validate that upper-right coordinates are greater than lower-left coordinates. Handle both []interface{} and PDFArray types. Use the parseNumber function for each coordinate.",
            "status": "done",
            "testStrategy": "Test with valid rectangles, invalid array lengths, negative dimensions, and various numeric formats. Verify error messages are descriptive."
          },
          {
            "id": 3,
            "title": "Implement MediaBox Inheritance Resolution",
            "description": "Create the findInheritedAttribute method to traverse the PDF page tree and inherit MediaBox values from parent pages when not directly specified",
            "dependencies": [],
            "details": "Recursively check parent pages for MediaBox attribute when not found in current page. Handle circular references and missing parent links. Support generic attribute inheritance for reusability with other inherited properties.",
            "status": "done",
            "testStrategy": "Test with nested page trees, missing MediaBox at various levels, circular parent references, and verify correct inheritance behavior."
          },
          {
            "id": 4,
            "title": "Create MediaBox Recovery Strategies",
            "description": "Implement the MediaBoxRecovery class with multiple fallback strategies for recovering page dimensions when standard parsing fails",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Implement recovery strategies in order: direct MediaBox, inherited MediaBox, CropBox fallback, ArtBox fallback, and default US Letter size. Log which strategy succeeded for debugging. Each strategy should return nil on failure to try the next one.",
            "status": "done",
            "testStrategy": "Test each recovery strategy independently and verify the fallback chain works correctly. Include PDFs with missing MediaBox, only CropBox, and completely missing dimension data."
          },
          {
            "id": 5,
            "title": "Integrate Robust Parsing into Main Tool",
            "description": "Update the pdf_extract_structured tool's main function to use the new robust MediaBox parsing with error recovery and continue processing on failures",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Modify extractStructured to use the new PageParser with MediaBoxRecovery. Add debug logging for MediaBox parsing. Implement graceful degradation where pages with parsing errors still get processed with default dimensions and error information.",
            "status": "done",
            "testStrategy": "Test with problematic PDFs that previously failed. Verify all pages are processed even when some have MediaBox errors. Check that error information is properly included in output."
          },
          {
            "id": 6,
            "title": "Add Comprehensive Error Handling and Logging",
            "description": "Enhance error handling throughout the MediaBox parsing pipeline with detailed logging and update the PageStructure model to include error information",
            "dependencies": [
              5
            ],
            "details": "Add structured logging at each parsing step. Update PageStructure model to include an Error field. Ensure all errors are logged but don't stop processing. Include page numbers and parsing context in error messages for easier debugging.",
            "status": "done",
            "testStrategy": "Run against a test suite of malformed PDFs. Verify logs contain sufficient information to diagnose issues. Ensure JSON output includes error details for failed pages."
          }
        ]
      },
      {
        "id": 29,
        "title": "Standardize Element Counting Across PDF Extraction Modes",
        "description": "Implement consistent element counting and normalization across all PDF extraction modes (complete, semantic, structured) to ensure predictable and comparable results when processing the same document.",
        "details": "Implement a unified element counting and normalization system that provides consistent results across different extraction modes:\n\n```go\n// internal/pdf/extraction/element_normalizer.go\npackage extraction\n\nimport (\n    \"github.com/yourusername/pdfextract/pkg/models\"\n)\n\ntype ElementNormalizer struct {\n    mode           ExtractionMode\n    countingRules  *CountingRules\n    mappingEngine  *ElementMappingEngine\n}\n\ntype CountingRules struct {\n    // Define what constitutes an \"element\" in each mode\n    CompleteMode   ElementDefinition\n    SemanticMode   ElementDefinition\n    StructuredMode ElementDefinition\n}\n\ntype ElementDefinition struct {\n    CountTextBlocks      bool\n    CountImages         bool\n    CountTables         bool\n    CountFormFields     bool\n    CountAnnotations    bool\n    MergeAdjacentText   bool\n    MinTextLength       int\n    GroupingThreshold   float64\n}\n\n// Normalize elements to ensure consistent counting\nfunc (en *ElementNormalizer) NormalizeElements(raw []interface{}) ([]models.Element, error) {\n    normalized := make([]models.Element, 0)\n    \n    switch en.mode {\n    case CompleteMode:\n        // In complete mode, count every distinct object\n        for _, item := range raw {\n            if elem := en.processCompleteElement(item); elem != nil {\n                normalized = append(normalized, elem)\n            }\n        }\n        \n    case SemanticMode:\n        // In semantic mode, group related elements\n        grouped := en.groupSemanticElements(raw)\n        for _, group := range grouped {\n            if elem := en.processSemanticGroup(group); elem != nil {\n                normalized = append(normalized, elem)\n            }\n        }\n        \n    case StructuredMode:\n        // In structured mode, only count high-level structures\n        structures := en.extractStructures(raw)\n        for _, struct := range structures {\n            if elem := en.processStructure(struct); elem != nil {\n                normalized = append(normalized, elem)\n            }\n        }\n    }\n    \n    return normalized, nil\n}\n\n// internal/pdf/extraction/mode_reconciler.go\ntype ModeReconciler struct {\n    normalizers map[ExtractionMode]*ElementNormalizer\n    validator   *ConsistencyValidator\n}\n\nfunc (mr *ModeReconciler) ReconcileResults(results map[ExtractionMode][]interface{}) (*ReconciliationReport, error) {\n    report := &ReconciliationReport{\n        Modes: make(map[ExtractionMode]*ModeResult),\n    }\n    \n    // Normalize each mode's results\n    for mode, rawElements := range results {\n        normalizer := mr.normalizers[mode]\n        normalized, err := normalizer.NormalizeElements(rawElements)\n        if err != nil {\n            return nil, fmt.Errorf(\"normalization failed for %s mode: %w\", mode, err)\n        }\n        \n        report.Modes[mode] = &ModeResult{\n            ElementCount:     len(normalized),\n            Elements:         normalized,\n            NormalizationLog: normalizer.GetLog(),\n        }\n    }\n    \n    // Validate consistency\n    validation := mr.validator.ValidateConsistency(report)\n    report.ConsistencyScore = validation.Score\n    report.Discrepancies = validation.Discrepancies\n    \n    return report, nil\n}\n\n// internal/pdf/extraction/consistency_validator.go\ntype ConsistencyValidator struct {\n    tolerances ConsistencyTolerances\n}\n\ntype ConsistencyTolerances struct {\n    // Acceptable variance between modes\n    TextCountVariance      float64 // e.g., 0.1 = 10% variance allowed\n    StructureCountVariance float64\n    TotalElementVariance   float64\n}\n\nfunc (cv *ConsistencyValidator) ValidateConsistency(report *ReconciliationReport) *ValidationResult {\n    result := &ValidationResult{\n        Score: 1.0,\n        Discrepancies: make([]Discrepancy, 0),\n    }\n    \n    // Compare element counts between modes\n    counts := make(map[ExtractionMode]int)\n    for mode, modeResult := range report.Modes {\n        counts[mode] = modeResult.ElementCount\n    }\n    \n    // Check if counts are within acceptable variance\n    if variance := cv.calculateVariance(counts); variance > cv.tolerances.TotalElementVariance {\n        result.Score -= 0.3\n        result.Discrepancies = append(result.Discrepancies, Discrepancy{\n            Type:        \"element_count_mismatch\",\n            Description: fmt.Sprintf(\"Element count variance %.2f exceeds tolerance %.2f\", variance, cv.tolerances.TotalElementVariance),\n            Severity:    \"high\",\n            Details:     counts,\n        })\n    }\n    \n    // Validate element mapping between modes\n    mapping := cv.mapElementsBetweenModes(report)\n    for _, issue := range mapping.Issues {\n        result.Discrepancies = append(result.Discrepancies, issue)\n        result.Score -= 0.1\n    }\n    \n    return result\n}\n\n// pkg/models/extraction.go\ntype Element interface {\n    GetID() string\n    GetType() ElementType\n    GetContent() interface{}\n    GetBounds() *BoundingBox\n    GetPage() int\n}\n\ntype ElementType string\n\nconst (\n    ElementTypeText       ElementType = \"text\"\n    ElementTypeImage      ElementType = \"image\"\n    ElementTypeTable      ElementType = \"table\"\n    ElementTypeForm       ElementType = \"form\"\n    ElementTypeAnnotation ElementType = \"annotation\"\n    ElementTypeStructure  ElementType = \"structure\"\n)\n\n// Update extraction modes to use normalized counting\nfunc (e *Extractor) Extract(doc *PDFDocument, mode ExtractionMode) (*ExtractionResult, error) {\n    // Extract raw elements based on mode\n    rawElements := e.extractRawElements(doc, mode)\n    \n    // Normalize elements for consistent counting\n    normalizer := NewElementNormalizer(mode)\n    normalized, err := normalizer.NormalizeElements(rawElements)\n    if err != nil {\n        return nil, fmt.Errorf(\"element normalization failed: %w\", err)\n    }\n    \n    return &ExtractionResult{\n        Mode:         mode,\n        Elements:     normalized,\n        ElementCount: len(normalized),\n        Metadata:     e.extractMetadata(doc),\n    }, nil\n}\n```\n\nKey implementation considerations:\n\n1. **Element Definition Standardization**:\n   - Define clear rules for what constitutes an \"element\" in each mode\n   - Complete mode: Every distinct PDF object (text run, image, annotation)\n   - Semantic mode: Logically grouped content (paragraphs, sections, tables)\n   - Structured mode: High-level document structures only\n\n2. **Normalization Rules**:\n   - Merge adjacent text runs in semantic/structured modes\n   - Group related form fields as single elements\n   - Count table cells vs entire tables based on mode\n   - Handle empty or whitespace-only elements consistently\n\n3. **Cross-Mode Mapping**:\n   - Maintain element IDs that allow tracking across modes\n   - Create mapping tables showing how elements in one mode relate to others\n   - Provide clear documentation on expected count differences\n\n4. **Consistency Validation**:\n   - Implement validators to ensure reasonable variance between modes\n   - Flag unexpected discrepancies for investigation\n   - Provide detailed reports on element counting decisions",
        "testStrategy": "Comprehensive testing strategy for consistent element counting across extraction modes:\n\n1. **Baseline Document Tests**:\n   - Create test PDFs with known element counts (e.g., 10 paragraphs, 5 images, 3 tables)\n   - Extract using all three modes and verify normalized counts\n   - Document should show predictable count relationships (e.g., complete >= semantic >= structured)\n   - Test with simple single-page documents first\n\n2. **Element Type Coverage**:\n   - Test documents with only text elements\n   - Test documents with mixed content (text, images, tables, forms)\n   - Verify each element type is counted consistently according to mode rules\n   - Test edge cases: empty paragraphs, single-character text runs, invisible elements\n\n3. **Mode-Specific Validation**:\n   - Complete mode: Verify every PDF object is counted\n   - Semantic mode: Verify logical grouping (adjacent text merged, table cells grouped)\n   - Structured mode: Verify only high-level structures counted\n   - Compare counts and ensure they follow expected patterns\n\n4. **Regression Testing**:\n   - Test with the document mentioned in the issue (116 vs 1 vs 0 elements)\n   - Verify all modes now return reasonable, explainable counts\n   - Document the normalization decisions that lead to final counts\n   - Ensure no mode returns 0 elements for non-empty documents\n\n5. **Performance Testing**:\n   - Verify normalization doesn't significantly impact extraction performance\n   - Test with large documents (1000+ pages)\n   - Monitor memory usage during normalization\n   - Ensure element mapping doesn't create memory leaks\n\n6. **Integration Testing**:\n   - Test with existing extraction workflows\n   - Verify downstream consumers handle normalized elements correctly\n   - Test with pdf_analyze_document tool to ensure compatibility\n   - Validate that element IDs remain consistent for cross-referencing",
        "status": "pending",
        "dependencies": [
          2,
          3,
          4,
          5,
          9,
          10
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 30,
        "title": "Enhance Error Messaging and Configuration Validation",
        "description": "Implement comprehensive error message improvements and configuration parameter validation across all PDF extraction tools, providing clear, actionable guidance to users when errors occur and ensuring configuration parameters are properly validated and applied.",
        "details": "Implement a robust error handling and configuration validation system that provides clear, actionable error messages and validates all configuration parameters:\n\n```go\n// internal/errors/user_errors.go\npackage errors\n\nimport (\n    \"fmt\"\n    \"strings\"\n)\n\ntype UserError struct {\n    Code      string\n    Message   string\n    Details   string\n    Guidance  string\n    Context   map[string]interface{}\n}\n\nfunc (e *UserError) Error() string {\n    var sb strings.Builder\n    sb.WriteString(fmt.Sprintf(\"Error %s: %s\\n\", e.Code, e.Message))\n    if e.Details != \"\" {\n        sb.WriteString(fmt.Sprintf(\"Details: %s\\n\", e.Details))\n    }\n    if e.Guidance != \"\" {\n        sb.WriteString(fmt.Sprintf(\"How to fix: %s\\n\", e.Guidance))\n    }\n    if len(e.Context) > 0 {\n        sb.WriteString(\"Context:\\n\")\n        for k, v := range e.Context {\n            sb.WriteString(fmt.Sprintf(\"  %s: %v\\n\", k, v))\n        }\n    }\n    return sb.String()\n}\n\n// Common error constructors\nfunc NoContentElementsError() *UserError {\n    return &UserError{\n        Code:    \"PDF_NO_CONTENT\",\n        Message: \"No content elements provided for extraction\",\n        Details: \"The PDF document appears to be empty or contains no extractable content\",\n        Guidance: \"Please ensure:\\n\" +\n            \"1. The PDF file is not corrupted\\n\" +\n            \"2. The PDF contains actual content (text, images, or forms)\\n\" +\n            \"3. The PDF is not password-protected\\n\" +\n            \"4. Try using --mode=complete for comprehensive extraction\",\n    }\n}\n\nfunc InvalidConfigurationError(param string, value interface{}, reason string) *UserError {\n    return &UserError{\n        Code:    \"CONFIG_INVALID\",\n        Message: fmt.Sprintf(\"Invalid configuration parameter: %s\", param),\n        Details: reason,\n        Context: map[string]interface{}{\n            \"parameter\": param,\n            \"value\":     value,\n        },\n        Guidance: getConfigGuidance(param),\n    }\n}\n\n// internal/config/validator.go\npackage config\n\nimport (\n    \"github.com/yourusername/pdfextract/internal/errors\"\n)\n\ntype ConfigValidator struct {\n    rules map[string]ValidationRule\n}\n\ntype ValidationRule struct {\n    Type        string\n    Required    bool\n    MinValue    interface{}\n    MaxValue    interface{}\n    ValidValues []interface{}\n    Validator   func(interface{}) error\n}\n\nfunc NewConfigValidator() *ConfigValidator {\n    return &ConfigValidator{\n        rules: map[string]ValidationRule{\n            \"mode\": {\n                Type:        \"string\",\n                Required:    false,\n                ValidValues: []interface{}{\"complete\", \"semantic\", \"structured\"},\n            },\n            \"page_range\": {\n                Type:      \"string\",\n                Required:  false,\n                Validator: validatePageRange,\n            },\n            \"output_format\": {\n                Type:        \"string\",\n                Required:    false,\n                ValidValues: []interface{}{\"json\", \"xml\", \"csv\", \"text\"},\n            },\n            \"chunk_size\": {\n                Type:     \"int\",\n                Required: false,\n                MinValue: 100,\n                MaxValue: 10000,\n            },\n            \"timeout\": {\n                Type:     \"duration\",\n                Required: false,\n                MinValue: \"1s\",\n                MaxValue: \"1h\",\n            },\n        },\n    }\n}\n\nfunc (cv *ConfigValidator) Validate(config map[string]interface{}) error {\n    appliedParams := make([]string, 0)\n    \n    for param, value := range config {\n        rule, exists := cv.rules[param]\n        if !exists {\n            return errors.InvalidConfigurationError(param, value, \n                fmt.Sprintf(\"Unknown parameter '%s'. Valid parameters are: %s\", \n                    param, cv.getValidParams()))\n        }\n        \n        if err := cv.validateParam(param, value, rule); err != nil {\n            return err\n        }\n        \n        appliedParams = append(appliedParams, param)\n    }\n    \n    // Log applied configuration\n    if len(appliedParams) > 0 {\n        log.Info(\"Applied configuration parameters: %s\", strings.Join(appliedParams, \", \"))\n    }\n    \n    return nil\n}\n\n// internal/pdf/extraction/error_context.go\npackage extraction\n\ntype ErrorContext struct {\n    Operation   string\n    PageNumber  int\n    ElementType string\n    Position    *BoundingBox\n    Metadata    map[string]interface{}\n}\n\nfunc (ec *ErrorContext) WrapError(err error) error {\n    if userErr, ok := err.(*errors.UserError); ok {\n        // Enhance existing user error with context\n        if userErr.Context == nil {\n            userErr.Context = make(map[string]interface{})\n        }\n        userErr.Context[\"operation\"] = ec.Operation\n        if ec.PageNumber > 0 {\n            userErr.Context[\"page\"] = ec.PageNumber\n        }\n        if ec.ElementType != \"\" {\n            userErr.Context[\"element_type\"] = ec.ElementType\n        }\n        return userErr\n    }\n    \n    // Convert technical errors to user-friendly errors\n    return ec.convertToUserError(err)\n}\n\n// Update extraction tools to use enhanced error handling\n// cmd/pdf_extract_text/main.go\nfunc main() {\n    // ... existing code ...\n    \n    // Validate configuration early\n    validator := config.NewConfigValidator()\n    if err := validator.Validate(extractConfig); err != nil {\n        fmt.Fprintf(os.Stderr, \"%v\\n\", err)\n        os.Exit(1)\n    }\n    \n    // Enhanced error handling during extraction\n    result, err := extractor.Extract(doc)\n    if err != nil {\n        if result != nil && result.ElementCount == 0 {\n            err = errors.NoContentElementsError()\n        }\n        \n        // Add file context to error\n        if userErr, ok := err.(*errors.UserError); ok {\n            userErr.Context[\"file\"] = inputFile\n            userErr.Context[\"file_size\"] = getFileSize(inputFile)\n        }\n        \n        fmt.Fprintf(os.Stderr, \"%v\\n\", err)\n        os.Exit(1)\n    }\n}\n\n// internal/cli/help.go\npackage cli\n\ntype ParameterHelp struct {\n    Name        string\n    Type        string\n    Default     string\n    Description string\n    Examples    []string\n}\n\nfunc GetParameterHelp() []ParameterHelp {\n    return []ParameterHelp{\n        {\n            Name:        \"mode\",\n            Type:        \"string\",\n            Default:     \"complete\",\n            Description: \"Extraction mode determining what content to extract\",\n            Examples: []string{\n                \"--mode=complete  # Extract all content types\",\n                \"--mode=semantic  # Extract with semantic structure\",\n                \"--mode=structured  # Extract with full document structure\",\n            },\n        },\n        {\n            Name:        \"page_range\",\n            Type:        \"string\",\n            Default:     \"all\",\n            Description: \"Pages to extract (1-based indexing)\",\n            Examples: []string{\n                \"--page_range=1-10  # First 10 pages\",\n                \"--page_range=5     # Only page 5\",\n                \"--page_range=1,3,5-7  # Pages 1, 3, 5, 6, and 7\",\n            },\n        },\n        // ... more parameters\n    }\n}",
        "testStrategy": "Comprehensive testing strategy for error messaging and configuration validation:\n\n1. **Error Message Clarity Tests**:\n   - Test \"no content elements\" error with empty PDFs, image-only PDFs, and corrupted files\n   - Verify error messages include specific guidance for each scenario\n   - Test that file context (name, size) is included in error output\n   - Verify error codes are consistent and documented\n\n2. **Configuration Validation Tests**:\n   - Test each configuration parameter with valid and invalid values\n   - Verify unknown parameters are rejected with list of valid options\n   - Test boundary values (min/max for numeric parameters)\n   - Verify type validation (string vs int vs duration)\n   - Test that valid configurations log \"Applied configuration parameters\" message\n\n3. **Integration Tests**:\n   - Test all extraction tools (text, forms, images, structured) with invalid configs\n   - Verify consistent error format across all tools\n   - Test parameter combinations that might conflict\n   - Verify --help displays all parameters with examples\n\n4. **User Experience Tests**:\n   - Have users unfamiliar with the tool attempt common tasks\n   - Document confusing error messages they encounter\n   - Test error messages in different terminal environments\n   - Verify guidance is actionable and resolves the issue\n\n5. **Regression Tests**:\n   - Create test suite with previously confusing errors\n   - Ensure \"no content elements provided\" includes proper guidance\n   - Test that configuration parameters are actually applied (not just validated)\n   - Verify error context includes relevant debugging information",
        "status": "pending",
        "dependencies": [
          2,
          3,
          4,
          5,
          9,
          10,
          27,
          28
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 31,
        "title": "Implement Comprehensive PDF Tool Testing Framework",
        "description": "Build an automated testing framework that validates all PDF extraction tools against diverse document types, formats, and generators to ensure reliability and prevent regressions based on production testing feedback.",
        "details": "Create a comprehensive testing framework that systematically validates all PDF tools with various document types and edge cases:\n\n```go\n// internal/testing/framework/pdf_test_framework.go\npackage framework\n\nimport (\n    \"github.com/yourusername/pdfextract/internal/pdf\"\n    \"github.com/yourusername/pdfextract/internal/testing/corpus\"\n    \"github.com/yourusername/pdfextract/internal/testing/validators\"\n)\n\ntype PDFTestFramework struct {\n    corpus      *TestCorpus\n    validators  map[string]Validator\n    reporter    *TestReporter\n    config      *TestConfig\n}\n\ntype TestCorpus struct {\n    Documents []TestDocument\n    Categories map[string][]TestDocument\n}\n\ntype TestDocument struct {\n    Path         string\n    Generator    string // Adobe, pdfcpu, wkhtmltopdf, Chrome, etc.\n    Version      string // PDF version\n    Features     []string // forms, images, tables, encryption\n    ExpectedData map[string]interface{}\n}\n\n// internal/testing/framework/test_runner.go\ntype TestRunner struct {\n    framework *PDFTestFramework\n    tools     map[string]PDFTool\n}\n\nfunc (tr *TestRunner) RunComprehensiveTests() (*TestReport, error) {\n    report := NewTestReport()\n    \n    // Test each tool against all document types\n    for toolName, tool := range tr.tools {\n        for _, category := range tr.framework.corpus.Categories {\n            results := tr.testToolWithCategory(tool, category)\n            report.AddResults(toolName, results)\n        }\n    }\n    \n    return report, nil\n}\n\n// internal/testing/corpus/builder.go\ntype CorpusBuilder struct {\n    sources []DocumentSource\n}\n\nfunc (cb *CorpusBuilder) BuildTestCorpus() (*TestCorpus, error) {\n    corpus := &TestCorpus{\n        Categories: map[string][]TestDocument{\n            \"simple_text\": {},\n            \"complex_forms\": {},\n            \"mixed_content\": {},\n            \"large_documents\": {},\n            \"encrypted\": {},\n            \"malformed\": {},\n            \"edge_cases\": {},\n        },\n    }\n    \n    // Populate with diverse test documents\n    corpus.AddDocument(\"simple_text\", TestDocument{\n        Path: \"testdata/generators/adobe/simple_text.pdf\",\n        Generator: \"Adobe Acrobat DC\",\n        Version: \"1.7\",\n        Features: []string{\"text\"},\n        ExpectedData: map[string]interface{}{\n            \"page_count\": 1,\n            \"text_content\": \"Expected text content...\",\n        },\n    })\n    \n    return corpus, nil\n}\n\n// internal/testing/validators/content_validator.go\ntype ContentValidator struct {\n    tolerances map[string]float64\n}\n\nfunc (cv *ContentValidator) ValidateTextExtraction(actual, expected interface{}) ValidationResult {\n    // Compare extracted text with expected\n    // Handle encoding differences\n    // Allow for minor formatting variations\n}\n\nfunc (cv *ContentValidator) ValidateFormData(actual, expected interface{}) ValidationResult {\n    // Validate form field extraction\n    // Check field types, values, properties\n    // Handle different form representations\n}\n\n// internal/testing/framework/regression_tests.go\ntype RegressionTestSuite struct {\n    baseline map[string]TestBaseline\n    differ   *ContentDiffer\n}\n\nfunc (rts *RegressionTestSuite) RunRegressionTests(tool PDFTool) []RegressionResult {\n    results := []RegressionResult{}\n    \n    for docID, baseline := range rts.baseline {\n        current := tool.Extract(baseline.Document)\n        diff := rts.differ.Compare(baseline.Expected, current)\n        \n        if diff.HasChanges() {\n            results = append(results, RegressionResult{\n                DocumentID: docID,\n                Baseline: baseline,\n                Current: current,\n                Differences: diff,\n            })\n        }\n    }\n    \n    return results\n}\n\n// internal/testing/generators/test_generator.go\ntype TestPDFGenerator struct {\n    generators map[string]PDFGenerator\n}\n\nfunc (tg *TestPDFGenerator) GenerateTestSuite() error {\n    // Generate PDFs with different tools\n    generators := map[string]PDFGenerator{\n        \"pdfcpu\": NewPDFCPUGenerator(),\n        \"reportlab\": NewReportLabGenerator(),\n        \"wkhtmltopdf\": NewWkHTMLToPDFGenerator(),\n        \"chrome\": NewChromeGenerator(),\n    }\n    \n    // Create test PDFs with various features\n    for name, gen := range generators {\n        // Simple text document\n        gen.CreateTextDocument(\"testdata/generated/\" + name + \"_text.pdf\")\n        \n        // Form with all field types\n        gen.CreateFormDocument(\"testdata/generated/\" + name + \"_forms.pdf\")\n        \n        // Mixed content (text, images, tables)\n        gen.CreateMixedDocument(\"testdata/generated/\" + name + \"_mixed.pdf\")\n        \n        // Edge cases\n        gen.CreateEdgeCaseDocument(\"testdata/generated/\" + name + \"_edge.pdf\")\n    }\n}\n\n// internal/testing/framework/performance_tests.go\ntype PerformanceTestSuite struct {\n    benchmarks map[string]PerformanceBenchmark\n}\n\nfunc (pts *PerformanceTestSuite) RunPerformanceTests(tool PDFTool) PerformanceReport {\n    report := PerformanceReport{}\n    \n    for name, benchmark := range pts.benchmarks {\n        result := benchmark.Run(tool)\n        report.AddResult(name, result)\n        \n        // Check against performance thresholds\n        if result.Duration > benchmark.MaxDuration {\n            report.AddWarning(name, \"Exceeded time threshold\")\n        }\n        \n        if result.MemoryUsed > benchmark.MaxMemory {\n            report.AddWarning(name, \"Exceeded memory threshold\")\n        }\n    }\n    \n    return report\n}\n\n// cmd/test-framework/main.go\nfunc main() {\n    framework := framework.NewPDFTestFramework(\n        framework.WithCorpus(\"testdata/corpus\"),\n        framework.WithValidators(DefaultValidators()),\n        framework.WithReporter(NewJSONReporter()),\n    )\n    \n    // Register all tools to test\n    runner := framework.NewTestRunner()\n    runner.RegisterTool(\"text_extractor\", NewTextExtractor())\n    runner.RegisterTool(\"form_extractor\", NewFormExtractor())\n    runner.RegisterTool(\"image_extractor\", NewImageExtractor())\n    runner.RegisterTool(\"table_extractor\", NewTableExtractor())\n    runner.RegisterTool(\"metadata_extractor\", NewMetadataExtractor())\n    \n    // Run comprehensive test suite\n    report, err := runner.RunComprehensiveTests()\n    if err != nil {\n        log.Fatal(err)\n    }\n    \n    // Generate test report\n    if err := report.SaveHTML(\"test-results.html\"); err != nil {\n        log.Fatal(err)\n    }\n}\n```\n\nImplement automated test generation and validation:\n\n```yaml\n# .github/workflows/pdf-tool-tests.yml\nname: PDF Tool Testing\non:\n  push:\n  pull_request:\n  schedule:\n    - cron: '0 0 * * *' # Daily regression tests\n\njobs:\n  test-framework:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Setup Test Environment\n        run: |\n          # Install PDF generators\n          sudo apt-get update\n          sudo apt-get install -y wkhtmltopdf poppler-utils\n          pip install reportlab pypdf2\n          \n      - name: Generate Test PDFs\n        run: |\n          go run cmd/test-generator/main.go --output testdata/generated\n          \n      - name: Run Comprehensive Tests\n        run: |\n          go test -v ./internal/testing/framework/... -timeout 30m\n          \n      - name: Run Tool Integration Tests\n        run: |\n          go run cmd/test-framework/main.go \\\n            --corpus testdata/corpus \\\n            --output test-results\n            \n      - name: Upload Test Results\n        uses: actions/upload-artifact@v3\n        with:\n          name: test-results\n          path: test-results/\n```\n\nCreate diverse test document corpus:\n\n```go\n// testdata/corpus/manifest.json\n{\n  \"documents\": [\n    {\n      \"id\": \"adobe_simple_text\",\n      \"path\": \"adobe/simple_text.pdf\",\n      \"generator\": \"Adobe Acrobat DC 2021\",\n      \"version\": \"1.7\",\n      \"features\": [\"text\", \"fonts\"],\n      \"validation\": {\n        \"text\": \"testdata/expected/adobe_simple_text.txt\",\n        \"structure\": \"testdata/expected/adobe_simple_text.json\"\n      }\n    },\n    {\n      \"id\": \"pdfcpu_complex_form\",\n      \"path\": \"pdfcpu/complex_form.pdf\",\n      \"generator\": \"pdfcpu v0.5.0\",\n      \"version\": \"1.7\",\n      \"features\": [\"forms\", \"javascript\", \"calculations\"],\n      \"validation\": {\n        \"forms\": \"testdata/expected/pdfcpu_complex_form.json\"\n      }\n    },\n    {\n      \"id\": \"chrome_print_webpage\",\n      \"path\": \"chrome/webpage_print.pdf\",\n      \"generator\": \"Chrome 120.0\",\n      \"version\": \"1.4\",\n      \"features\": [\"text\", \"images\", \"links\", \"css_layout\"],\n      \"validation\": {\n        \"content\": \"testdata/expected/chrome_webpage_print.json\"\n      }\n    }\n  ]\n}\n```",
        "testStrategy": "Comprehensive testing strategy for the PDF tool testing framework:\n\n1. **Framework Component Tests**:\n   - Test corpus builder with various document sources\n   - Verify validator implementations for each content type\n   - Test report generation in multiple formats (JSON, HTML, JUnit)\n   - Validate test runner orchestration and parallel execution\n\n2. **Document Diversity Tests**:\n   - Test with PDFs from major generators (Adobe, pdfcpu, Chrome, MS Office)\n   - Include documents with different PDF versions (1.4, 1.5, 1.7, 2.0)\n   - Test various content types: text-only, forms, images, tables, mixed\n   - Include edge cases: empty PDFs, corrupted files, huge documents\n\n3. **Tool Validation Tests**:\n   - Run each extraction tool against the full test corpus\n   - Compare results against expected baselines\n   - Test error handling for malformed documents\n   - Verify consistent results across multiple runs\n\n4. **Regression Testing**:\n   - Establish baseline results for all tools\n   - Detect any changes in extraction behavior\n   - Generate detailed diff reports for any variations\n   - Test with production PDFs that previously caused issues\n\n5. **Performance Benchmarks**:\n   - Measure extraction time for documents of various sizes\n   - Monitor memory usage during processing\n   - Test with documents up to 1000+ pages\n   - Verify streaming tools maintain constant memory usage\n\n6. **Integration Tests**:\n   - Test framework integration with CI/CD pipeline\n   - Verify automated test execution on schedule\n   - Test result artifact generation and storage\n   - Validate notification system for test failures\n\n7. **Generator-Specific Tests**:\n   - Adobe: Test with forms, portfolios, signed documents\n   - pdfcpu: Test with programmatically generated content\n   - Chrome/browsers: Test with printed web pages, CSS layouts\n   - Office: Test with converted documents, embedded objects\n\n8. **Validation Accuracy**:\n   - Test fuzzy matching for text comparison\n   - Verify tolerance settings for coordinate-based content\n   - Test Unicode and encoding handling across generators\n   - Validate form field type detection accuracy",
        "status": "pending",
        "dependencies": [
          2,
          3,
          6,
          9,
          12,
          14,
          22,
          23
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Core Testing Framework Architecture",
            "description": "Create the foundational architecture for the PDF testing framework including core interfaces, data structures, and the main framework components",
            "dependencies": [],
            "details": "Implement PDFTestFramework struct with TestCorpus, Validator interfaces, TestReporter, and TestConfig. Define TestDocument structure with metadata fields for generator, version, features, and expected data. Create base interfaces for PDFTool and Validator to ensure extensibility.",
            "status": "pending",
            "testStrategy": "Unit test the framework initialization, configuration loading, and basic component registration"
          },
          {
            "id": 2,
            "title": "Build Test Corpus Management System",
            "description": "Implement the test corpus builder and management system that organizes test documents by categories and handles document metadata",
            "dependencies": [
              1
            ],
            "details": "Create CorpusBuilder to load and categorize test documents from manifest.json. Implement document categorization (simple_text, complex_forms, mixed_content, large_documents, encrypted, malformed, edge_cases). Build corpus loading from filesystem with validation of expected data files.",
            "status": "pending",
            "testStrategy": "Test corpus loading with sample manifest, verify category assignment, and validate metadata parsing"
          },
          {
            "id": 3,
            "title": "Implement Content Validators",
            "description": "Create comprehensive validators for different PDF content types including text, forms, images, tables, and metadata",
            "dependencies": [
              1
            ],
            "details": "Implement ContentValidator with methods for ValidateTextExtraction (handling encoding differences and formatting variations), ValidateFormData (field types, values, properties), ValidateImageExtraction, ValidateTableStructure, and ValidateMetadata. Include configurable tolerances for fuzzy matching.",
            "status": "pending",
            "testStrategy": "Unit test each validator with known input/output pairs, test tolerance configurations"
          },
          {
            "id": 4,
            "title": "Create Test PDF Generators",
            "description": "Build test PDF generation system that creates PDFs using different generators (pdfcpu, reportlab, wkhtmltopdf, Chrome) with various features",
            "dependencies": [
              2
            ],
            "details": "Implement TestPDFGenerator with adapters for each PDF generation tool. Create methods to generate simple text documents, complex forms with all field types, mixed content documents, and edge case documents. Ensure each generator creates consistent test cases with known expected outputs.",
            "status": "pending",
            "testStrategy": "Verify each generator produces valid PDFs, validate generated PDFs contain expected features"
          },
          {
            "id": 5,
            "title": "Build Test Runner and Execution Engine",
            "description": "Implement the test runner that executes tests across all tools and document categories, collecting results and handling errors",
            "dependencies": [
              2,
              3
            ],
            "details": "Create TestRunner with tool registration, implement RunComprehensiveTests method that iterates through all tools and document categories. Add parallel test execution support, timeout handling, and error recovery. Implement result aggregation and test status tracking.",
            "status": "pending",
            "testStrategy": "Integration test with mock tools and documents, verify parallel execution and error handling"
          },
          {
            "id": 6,
            "title": "Implement Regression Testing Suite",
            "description": "Create regression testing capabilities that compare current results against baseline data and detect changes",
            "dependencies": [
              3,
              5
            ],
            "details": "Build RegressionTestSuite with baseline storage and management. Implement ContentDiffer for intelligent comparison of extraction results. Create baseline generation and update mechanisms. Add configurable thresholds for acceptable differences and change detection algorithms.",
            "status": "pending",
            "testStrategy": "Test with known baseline data, verify change detection accuracy, test baseline update workflows"
          },
          {
            "id": 7,
            "title": "Add Performance Testing and Benchmarking",
            "description": "Implement performance testing suite that measures execution time, memory usage, and resource consumption for each tool",
            "dependencies": [
              5
            ],
            "details": "Create PerformanceTestSuite with configurable benchmarks for different document sizes and complexities. Implement performance metrics collection (duration, memory, CPU usage). Add threshold checking and performance regression detection. Create performance trend tracking over time.",
            "status": "pending",
            "testStrategy": "Benchmark with documents of varying sizes, verify metric collection accuracy"
          },
          {
            "id": 8,
            "title": "Create Test Reporting and CI/CD Integration",
            "description": "Build comprehensive test reporting system and integrate with GitHub Actions for automated testing",
            "dependencies": [
              5,
              6,
              7
            ],
            "details": "Implement TestReporter with HTML, JSON, and JUnit XML output formats. Create detailed reports showing test results, regressions, performance metrics, and coverage by document type. Implement GitHub Actions workflow for automated testing on push, PR, and scheduled runs. Add artifact uploading and test result visualization.",
            "status": "pending",
            "testStrategy": "Verify report generation in all formats, test CI/CD pipeline with sample workflows"
          }
        ]
      },
      {
        "id": 32,
        "title": "Clean Up Development Cruft and Technical Debt",
        "description": "Remove accumulated technical debt from PDF tool development including unused code, dead imports, temporary files, debug code, commented-out sections, and consolidate duplicate functionality to improve code organization and maintainability.",
        "details": "Perform a comprehensive cleanup of the PDF extraction codebase to remove technical debt and improve code quality:\n\n```go\n// tools/cleanup/analyzer.go\npackage cleanup\n\nimport (\n    \"go/ast\"\n    \"go/parser\"\n    \"go/token\"\n    \"path/filepath\"\n    \"strings\"\n)\n\ntype CodeAnalyzer struct {\n    deadCode      map[string][]Finding\n    unusedImports map[string][]string\n    debugCode     map[string][]CodeBlock\n    duplicates    map[string][]DuplicateFunc\n}\n\ntype Finding struct {\n    File     string\n    Line     int\n    Type     string\n    Message  string\n}\n\nfunc (ca *CodeAnalyzer) AnalyzeProject(rootPath string) (*CleanupReport, error) {\n    // 1. Scan for unused imports\n    ca.findUnusedImports(rootPath)\n    \n    // 2. Detect dead code\n    ca.findDeadCode(rootPath)\n    \n    // 3. Find debug/temporary code\n    ca.findDebugCode(rootPath)\n    \n    // 4. Identify duplicate functionality\n    ca.findDuplicates(rootPath)\n    \n    return ca.generateReport()\n}\n\n// scripts/cleanup.sh\n#!/bin/bash\n# Automated cleanup script\n\necho \"Starting PDF tools cleanup...\"\n\n# Remove temporary files\nfind . -name \"*.tmp\" -o -name \"*.log\" -o -name \"debug_*\" | xargs rm -f\n\n# Clean up test artifacts\nfind . -path \"*/testdata/output/*\" -delete\n\n# Remove commented code blocks\ngrep -r \"^[[:space:]]*//.*TODO.*remove\" . --include=\"*.go\" | cut -d: -f1 | sort -u\n\n// internal/cleanup/refactor.go\ntype Refactorer struct {\n    ast       *ast.Package\n    fileSet   *token.FileSet\n    changes   []Change\n}\n\nfunc (r *Refactorer) ConsolidateDuplicates() error {\n    // Identify common patterns across extractors\n    patterns := []string{\n        \"error handling boilerplate\",\n        \"PDF object parsing\",\n        \"coordinate transformation\",\n        \"text normalization\",\n    }\n    \n    for _, pattern := range patterns {\n        duplicates := r.findPattern(pattern)\n        if len(duplicates) > 1 {\n            r.extractToShared(pattern, duplicates)\n        }\n    }\n    \n    return r.applyChanges()\n}\n\n// Cleanup targets by priority:\n// 1. Remove experimental code that didn't make it to production\n//    - internal/experimental/* (if exists)\n//    - Features marked with \"EXPERIMENTAL\" comments\n//    - Unused extraction strategies\n\n// 2. Clean up debug code\n//    - fmt.Printf/Println statements (replace with proper logging)\n//    - Debug flags and conditional compilation\n//    - Temporary debugging utilities\n\n// 3. Remove dead imports\n//    - Run goimports on all files\n//    - Remove vendor dependencies no longer used\n//    - Clean up replace directives in go.mod\n\n// 4. Consolidate duplicate functionality\n//    - Merge similar error handling patterns\n//    - Unify coordinate transformation logic\n//    - Consolidate PDF object parsing utilities\n\n// 5. Remove commented-out code\n//    - Old implementations kept \"just in case\"\n//    - Alternative approaches that were rejected\n//    - TODO comments older than 3 months\n\n// 6. Organize code structure\n//    - Move misplaced files to correct packages\n//    - Split large files (>500 lines) into logical units\n//    - Ensure consistent package naming\n\n// Example consolidation - Error handling:\n// Before (duplicated across multiple files):\n/*\nif err != nil {\n    log.Printf(\"Failed to parse PDF: %v\", err)\n    return nil, fmt.Errorf(\"pdf parsing failed: %w\", err)\n}\n*/\n\n// After (consolidated):\n// internal/common/errors.go\nfunc HandleParseError(operation string, err error) error {\n    if err == nil {\n        return nil\n    }\n    log.Printf(\"Failed to %s: %v\", operation, err)\n    return fmt.Errorf(\"%s failed: %w\", operation, err)\n}\n\n// Cleanup checklist:\n// [ ] Run go mod tidy to clean dependencies\n// [ ] Execute gofmt -s -w . for consistent formatting\n// [ ] Run staticcheck to find unused code\n// [ ] Use deadcode tool to identify unreachable functions\n// [ ] Review and remove old feature flags\n// [ ] Consolidate test utilities into shared package\n// [ ] Remove or properly document experimental APIs\n// [ ] Clean up outdated documentation\n// [ ] Remove development scripts no longer needed\n// [ ] Archive old benchmark results",
        "testStrategy": "Comprehensive testing strategy to ensure cleanup doesn't break functionality:\n\n1. **Pre-Cleanup Baseline**:\n   - Run full test suite and record coverage percentage\n   - Generate benchmark results for all tools\n   - Create API compatibility report\n   - Document current binary sizes\n\n2. **Static Analysis Verification**:\n   - Run `go vet` to ensure no new issues introduced\n   - Execute `golint` and verify no regressions\n   - Use `ineffassign` to check for unused assignments\n   - Run `gosec` for security issues\n\n3. **Dead Code Detection**:\n   - Use `deadcode` tool to identify unused functions\n   - Run `unused` to find unused types, constants, variables\n   - Verify no public APIs were accidentally removed\n   - Check for orphaned test files\n\n4. **Import Cleanup Validation**:\n   - Run `goimports -l .` and verify no errors\n   - Check `go mod tidy` completes successfully\n   - Ensure no circular dependencies introduced\n   - Verify vendor directory is properly updated\n\n5. **Functionality Preservation**:\n   - Run complete test suite after each major cleanup phase\n   - Compare test coverage (should not decrease)\n   - Execute integration tests with real PDFs\n   - Verify all command-line tools still function\n\n6. **Performance Validation**:\n   - Re-run benchmarks and compare with baseline\n   - Ensure no performance regressions\n   - Check memory usage hasn't increased\n   - Verify binary sizes are reduced or stable\n\n7. **Code Quality Metrics**:\n   - Measure cyclomatic complexity reduction\n   - Check for improved code duplication metrics\n   - Verify consistent code style with `gofmt -d .`\n   - Ensure all files pass `go fmt` checks\n\n8. **Manual Review Checklist**:\n   - Verify no production code accidentally removed\n   - Check that all TODO/FIXME comments are addressed or documented\n   - Ensure experimental features are properly removed or promoted\n   - Validate that debug code is completely eliminated\n\n9. **Regression Testing**:\n   - Test with problematic PDFs from previous bug reports\n   - Verify form extraction still works (Task 27 fixes)\n   - Confirm MediaBox parsing remains functional (Task 28 fixes)\n   - Test all extraction modes for consistency\n\n10. **Documentation Updates**:\n    - Update README with any removed features\n    - Clean up outdated examples\n    - Verify all code comments are still accurate\n    - Update API documentation if interfaces changed",
        "status": "pending",
        "dependencies": [
          27,
          28,
          29,
          30,
          31
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Code Analysis Tool for Dead Code Detection",
            "description": "Create an automated code analyzer that scans the entire PDF tool codebase to identify unused functions, variables, types, and unreachable code blocks using AST parsing and static analysis",
            "dependencies": [],
            "details": "Build the CodeAnalyzer struct with methods to parse Go files using go/ast and go/parser packages. Implement findDeadCode() to traverse the AST and identify unused declarations, findUnusedImports() to detect imports not referenced in code, and findDebugCode() to locate fmt.Printf/Println statements and debug flags. Store findings in structured format for reporting.",
            "status": "pending",
            "testStrategy": "Create test files with known dead code patterns and verify the analyzer correctly identifies them. Test edge cases like interface implementations and reflection usage."
          },
          {
            "id": 2,
            "title": "Remove Debug Code and Temporary Files",
            "description": "Clean up all debugging artifacts including print statements, temporary files, debug flags, experimental code paths, and convert necessary debug output to proper structured logging",
            "dependencies": [
              1
            ],
            "details": "Use the analyzer output to locate and remove fmt.Printf/Println statements, replacing critical ones with proper log statements. Delete files matching patterns like *.tmp, *.log, debug_*, and clean test output directories. Remove code blocks marked with EXPERIMENTAL comments and unused feature flags. Implement the cleanup.sh script to automate file removal.",
            "status": "pending",
            "testStrategy": "Verify no fmt.Print statements remain except in test files. Ensure all logging uses the structured logger. Confirm temporary file patterns are removed."
          },
          {
            "id": 3,
            "title": "Clean Up Imports and Dependencies",
            "description": "Remove all unused imports from Go files, clean up go.mod dependencies, remove vendor files no longer needed, and ensure all imports are properly organized",
            "dependencies": [
              1
            ],
            "details": "Run goimports on all Go files to remove unused imports and organize import blocks. Execute 'go mod tidy' to clean up module dependencies. Review and remove any replace directives in go.mod that are no longer needed. Use the analyzer's unusedImports map to verify all imports are actually used.",
            "status": "pending",
            "testStrategy": "Run 'go mod verify' to ensure dependencies are correct. Use staticcheck to verify no unused imports remain. Build the project to confirm no missing dependencies."
          },
          {
            "id": 4,
            "title": "Consolidate Duplicate Functionality",
            "description": "Identify and merge duplicate code patterns across the codebase, creating shared utilities for common operations like error handling, coordinate transformations, and PDF object parsing",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Implement the Refactorer to identify duplicate patterns using AST analysis. Create internal/common package with shared utilities: errors.go for unified error handling (HandleParseError), transforms.go for coordinate operations, and parsing.go for PDF object utilities. Update all occurrences to use the consolidated functions. Focus on patterns identified in the analyzer's duplicates map.",
            "status": "pending",
            "testStrategy": "Write unit tests for all new shared utilities. Ensure existing tests pass after refactoring. Use code coverage to verify consolidated functions are used throughout."
          },
          {
            "id": 5,
            "title": "Reorganize Code Structure and Documentation",
            "description": "Restructure the codebase for better organization by splitting large files, moving misplaced code to appropriate packages, removing outdated comments and documentation, and ensuring consistent naming conventions",
            "dependencies": [
              4
            ],
            "details": "Split any files over 500 lines into logical units. Move files to correct packages based on functionality. Remove commented-out code blocks and TODO comments older than 3 months. Update package documentation to reflect current implementation. Ensure consistent naming across packages. Archive old benchmark results and remove development scripts no longer needed.",
            "status": "pending",
            "testStrategy": "Verify all imports still resolve after reorganization. Run full test suite to ensure no functionality is broken. Use go vet to check for any issues introduced during restructuring."
          }
        ]
      },
      {
        "id": 33,
        "title": "Implement Complete Stream Filter Support for PDF 1.4 Compliance",
        "description": "Add support for all missing PDF 1.4 stream filters including CCITTFaxDecode, JBIG2Decode, RunLengthDecode, complete LZWDecode implementation, and proper DecodeParms handling to enable reading of PDFs that use these compression methods.",
        "details": "Implement comprehensive stream filter support for PDF 1.4 compliance, building on the existing parser infrastructure:\n\n```go\n// internal/pdf/filters/filter_factory.go\npackage filters\n\nimport (\n    \"io\"\n    \"github.com/yourusername/pdfextract/internal/pdf\"\n)\n\ntype FilterFactory struct {\n    decoders map[string]StreamDecoder\n}\n\ntype StreamDecoder interface {\n    Decode(reader io.Reader, params map[string]interface{}) (io.Reader, error)\n}\n\nfunc NewFilterFactory() *FilterFactory {\n    f := &FilterFactory{\n        decoders: make(map[string]StreamDecoder),\n    }\n    \n    // Register all PDF 1.4 filters\n    f.RegisterDecoder(\"FlateDecode\", &FlateDecoder{})\n    f.RegisterDecoder(\"LZWDecode\", &LZWDecoder{})\n    f.RegisterDecoder(\"RunLengthDecode\", &RunLengthDecoder{})\n    f.RegisterDecoder(\"CCITTFaxDecode\", &CCITTFaxDecoder{})\n    f.RegisterDecoder(\"JBIG2Decode\", &JBIG2Decoder{})\n    f.RegisterDecoder(\"DCTDecode\", &DCTDecoder{})\n    f.RegisterDecoder(\"JPXDecode\", &JPXDecoder{})\n    \n    return f\n}\n\n// internal/pdf/filters/lzw_decoder.go\ntype LZWDecoder struct{}\n\nfunc (d *LZWDecoder) Decode(reader io.Reader, params map[string]interface{}) (io.Reader, error) {\n    // Extract DecodeParms\n    earlyChange := 1 // Default per PDF spec\n    if params != nil {\n        if ec, ok := params[\"EarlyChange\"].(int); ok {\n            earlyChange = ec\n        }\n    }\n    \n    // Implement LZW decompression with proper handling of:\n    // - Variable code length (9-12 bits)\n    // - Clear and EOD codes\n    // - EarlyChange parameter\n    return lzw.NewReader(reader, earlyChange), nil\n}\n\n// internal/pdf/filters/ccitt_decoder.go\ntype CCITTFaxDecoder struct{}\n\nfunc (d *CCITTFaxDecoder) Decode(reader io.Reader, params map[string]interface{}) (io.Reader, error) {\n    // Extract CCITT parameters\n    k := 0 // Default: Group 3, 1-D\n    columns := 1728 // Default width\n    rows := 0 // Default: unspecified\n    blackIs1 := false\n    encodedByteAlign := false\n    \n    if params != nil {\n        if val, ok := params[\"K\"].(int); ok {\n            k = val\n        }\n        if val, ok := params[\"Columns\"].(int); ok {\n            columns = val\n        }\n        if val, ok := params[\"Rows\"].(int); ok {\n            rows = val\n        }\n        if val, ok := params[\"BlackIs1\"].(bool); ok {\n            blackIs1 = val\n        }\n        if val, ok := params[\"EncodedByteAlign\"].(bool); ok {\n            encodedByteAlign = val\n        }\n    }\n    \n    // Implement CCITT Fax decoding:\n    // - Group 3 (K=0) or Group 4 (K<0) or Mixed (K>0)\n    // - Handle run-length encoding\n    // - Process EOL markers\n    decoder := &ccittDecoder{\n        k:                k,\n        columns:          columns,\n        rows:             rows,\n        blackIs1:         blackIs1,\n        encodedByteAlign: encodedByteAlign,\n    }\n    \n    return decoder.decode(reader)\n}\n\n// internal/pdf/filters/jbig2_decoder.go\ntype JBIG2Decoder struct{}\n\nfunc (d *JBIG2Decoder) Decode(reader io.Reader, params map[string]interface{}) (io.Reader, error) {\n    // Extract JBIG2 parameters\n    var globals io.Reader\n    if params != nil {\n        if globalStream, ok := params[\"JBIG2Globals\"].(io.Reader); ok {\n            globals = globalStream\n        }\n    }\n    \n    // Implement JBIG2 decoding:\n    // - Parse segment headers\n    // - Handle symbol dictionaries\n    // - Process text/halftone/generic regions\n    // - Apply arithmetic decoding\n    decoder := &jbig2Decoder{\n        globals: globals,\n    }\n    \n    return decoder.decode(reader)\n}\n\n// internal/pdf/filters/runlength_decoder.go\ntype RunLengthDecoder struct{}\n\nfunc (d *RunLengthDecoder) Decode(reader io.Reader, params map[string]interface{}) (io.Reader, error) {\n    // Implement RunLength decoding per PDF spec:\n    // - Read length byte n\n    // - If 0 <= n <= 127: copy next n+1 bytes literally\n    // - If n = 128: EOD marker\n    // - If 129 <= n <= 255: repeat next byte 257-n times\n    \n    output := &bytes.Buffer{}\n    input := bufio.NewReader(reader)\n    \n    for {\n        length, err := input.ReadByte()\n        if err == io.EOF {\n            break\n        }\n        if err != nil {\n            return nil, err\n        }\n        \n        if length == 128 {\n            // EOD marker\n            break\n        } else if length <= 127 {\n            // Copy n+1 bytes literally\n            for i := 0; i <= int(length); i++ {\n                b, err := input.ReadByte()\n                if err != nil {\n                    return nil, err\n                }\n                output.WriteByte(b)\n            }\n        } else {\n            // Repeat next byte 257-n times\n            b, err := input.ReadByte()\n            if err != nil {\n                return nil, err\n            }\n            count := 257 - int(length)\n            for i := 0; i < count; i++ {\n                output.WriteByte(b)\n            }\n        }\n    }\n    \n    return bytes.NewReader(output.Bytes()), nil\n}\n\n// internal/pdf/filters/decode_parms.go\ntype DecodeParmsParser struct{}\n\nfunc (p *DecodeParmsParser) Parse(obj pdf.Object) (map[string]interface{}, error) {\n    // Handle various DecodeParms formats:\n    // - Single dictionary\n    // - Array of dictionaries (for multiple filters)\n    // - Null (use defaults)\n    \n    if obj == nil {\n        return nil, nil\n    }\n    \n    switch v := obj.(type) {\n    case pdf.Dictionary:\n        return p.parseDictionary(v)\n    case pdf.Array:\n        // For filter chains, return first set of params\n        if len(v) > 0 {\n            if dict, ok := v[0].(pdf.Dictionary); ok {\n                return p.parseDictionary(dict)\n            }\n        }\n    }\n    \n    return nil, nil\n}\n\n// internal/pdf/stream_decoder.go\ntype StreamDecoder struct {\n    filterFactory *FilterFactory\n    parmsParser   *DecodeParmsParser\n}\n\nfunc (d *StreamDecoder) DecodeStream(stream pdf.Stream) ([]byte, error) {\n    // Get filter chain\n    filters := d.getFilters(stream.Dictionary)\n    decodeParms := d.getDecodeParms(stream.Dictionary)\n    \n    // Apply filters in sequence\n    reader := bytes.NewReader(stream.Data)\n    var currentReader io.Reader = reader\n    \n    for i, filter := range filters {\n        decoder, ok := d.filterFactory.decoders[filter]\n        if !ok {\n            return nil, fmt.Errorf(\"unsupported filter: %s\", filter)\n        }\n        \n        // Get parameters for this filter\n        var params map[string]interface{}\n        if i < len(decodeParms) {\n            params = decodeParms[i]\n        }\n        \n        // Apply filter\n        decoded, err := decoder.Decode(currentReader, params)\n        if err != nil {\n            return nil, fmt.Errorf(\"filter %s failed: %w\", filter, err)\n        }\n        \n        currentReader = decoded\n    }\n    \n    // Read final decoded data\n    return io.ReadAll(currentReader)\n}\n```\n\nKey implementation considerations:\n\n1. **Filter Chain Support**: Handle multiple filters applied in sequence\n2. **DecodeParms Handling**: Properly parse and apply filter-specific parameters\n3. **Memory Efficiency**: Use streaming decoders where possible\n4. **Error Recovery**: Graceful handling of corrupted compressed data\n5. **Specification Compliance**: Follow PDF 1.4 specification exactly for each filter\n\nIntegration with existing components:\n- Extend content stream parser to use new filters\n- Update image extractor to handle additional compression formats\n- Ensure streaming parser can decode filters progressively",
        "testStrategy": "Comprehensive testing strategy for stream filter support:\n\n1. **Individual Filter Tests**:\n   - Test each filter with known input/output pairs from PDF specification examples\n   - Verify LZWDecode with different EarlyChange values (0 and 1)\n   - Test CCITTFaxDecode with Group 3 (1D/2D) and Group 4 encoding samples\n   - Validate JBIG2Decode with and without global segments\n   - Test RunLengthDecode with all three cases: literal copy, repeat, and EOD\n   - Verify proper handling of filter-specific DecodeParms\n\n2. **Filter Chain Tests**:\n   - Test multiple filters applied in sequence (e.g., FlateDecode after LZWDecode)\n   - Verify correct parameter association with each filter in chain\n   - Test with missing or null DecodeParms (should use defaults)\n\n3. **Real PDF Tests**:\n   - Collect sample PDFs using each compression method from various sources\n   - Test with scanned documents (typically use CCITTFaxDecode)\n   - Test with PDFs from different generators (Adobe, LibreOffice, etc.)\n   - Verify extracted content matches expected output\n\n4. **Error Handling Tests**:\n   - Test with corrupted compressed data\n   - Verify graceful handling of unsupported filter names\n   - Test with invalid DecodeParms\n   - Ensure proper error messages for debugging\n\n5. **Performance Tests**:\n   - Benchmark decompression speed for large streams\n   - Monitor memory usage during decompression\n   - Test with extremely large compressed streams\n   - Verify streaming decoders don't load entire stream into memory\n\n6. **Integration Tests**:\n   - Test with content stream parser for compressed text streams\n   - Verify image extraction works with all new filters\n   - Test with streaming parser for progressive decoding\n   - Ensure form field values are correctly extracted from compressed streams",
        "status": "done",
        "dependencies": [
          2,
          3,
          6
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 34,
        "title": "Implement Cross-Reference Table Parser for Random Access",
        "description": "Build proper xref table parsing to support random access to PDF objects, handle incremental updates with Prev chain, and implement proper object resolution. This is fundamental to PDF compliance and currently missing from our basic sequential parsing approach.",
        "details": "Implement a comprehensive cross-reference table parser that supports all PDF xref formats and enables efficient random access to objects:\n\n```go\n// internal/pdf/xref/parser.go\npackage xref\n\nimport (\n    \"bufio\"\n    \"fmt\"\n    \"io\"\n    \"strconv\"\n    \"strings\"\n)\n\ntype XRefParser struct {\n    reader      io.ReadSeeker\n    entries     map[int]map[int]*XRefEntry // objNum -> generation -> entry\n    trailers    []*TrailerDict              // Chain of trailers for incremental updates\n    startxref   int64                       // Offset to main xref\n    prevChain   []int64                     // Chain of previous xref offsets\n}\n\ntype XRefEntry struct {\n    Type       EntryType // Free, InUse, Compressed\n    Offset     int64     // Byte offset for InUse, or object stream number for Compressed\n    Generation int       // Generation number (0 for compressed)\n    StreamIndex int      // Index within object stream (for compressed objects)\n}\n\ntype EntryType int\nconst (\n    EntryFree EntryType = iota\n    EntryInUse\n    EntryCompressed\n)\n\ntype TrailerDict struct {\n    Size   int              // Total number of entries\n    Prev   *int64           // Offset to previous xref (for incremental updates)\n    Root   *IndirectRef     // Catalog dictionary\n    Encrypt *IndirectRef    // Encryption dictionary\n    Info   *IndirectRef     // Info dictionary\n    ID     [][]byte         // File identifiers\n}\n\n// ParseXRef parses all cross-reference sections starting from startxref\nfunc (p *XRefParser) ParseXRef(startxref int64) error {\n    p.startxref = startxref\n    p.entries = make(map[int]map[int]*XRefEntry)\n    \n    // Follow the Prev chain to parse all xref sections\n    offset := startxref\n    for offset > 0 {\n        p.prevChain = append(p.prevChain, offset)\n        \n        if _, err := p.reader.Seek(offset, io.SeekStart); err != nil {\n            return fmt.Errorf(\"seek to xref at %d: %w\", offset, err)\n        }\n        \n        // Check if it's a cross-reference stream or table\n        buf := make([]byte, 20)\n        n, _ := p.reader.Read(buf)\n        p.reader.Seek(offset, io.SeekStart) // Reset position\n        \n        var trailer *TrailerDict\n        var err error\n        \n        if strings.Contains(string(buf[:n]), \"xref\") {\n            trailer, err = p.parseXRefTable()\n        } else {\n            trailer, err = p.parseXRefStream()\n        }\n        \n        if err != nil {\n            return fmt.Errorf(\"parse xref at %d: %w\", offset, err)\n        }\n        \n        p.trailers = append(p.trailers, trailer)\n        \n        // Continue with previous xref if exists\n        if trailer.Prev != nil {\n            offset = *trailer.Prev\n        } else {\n            offset = 0\n        }\n    }\n    \n    return nil\n}\n\n// parseXRefTable handles traditional cross-reference tables\nfunc (p *XRefParser) parseXRefTable() (*TrailerDict, error) {\n    scanner := bufio.NewScanner(p.reader)\n    \n    // Read \"xref\" keyword\n    if !scanner.Scan() || scanner.Text() != \"xref\" {\n        return nil, fmt.Errorf(\"expected 'xref' keyword\")\n    }\n    \n    // Parse subsections\n    for scanner.Scan() {\n        line := scanner.Text()\n        if line == \"trailer\" {\n            break\n        }\n        \n        // Parse subsection header: \"start count\"\n        parts := strings.Fields(line)\n        if len(parts) != 2 {\n            return nil, fmt.Errorf(\"invalid xref subsection header: %s\", line)\n        }\n        \n        start, _ := strconv.Atoi(parts[0])\n        count, _ := strconv.Atoi(parts[1])\n        \n        // Read entries\n        for i := 0; i < count; i++ {\n            if !scanner.Scan() {\n                return nil, fmt.Errorf(\"unexpected EOF in xref table\")\n            }\n            \n            entry, err := p.parseXRefEntry(scanner.Text())\n            if err != nil {\n                return nil, err\n            }\n            \n            objNum := start + i\n            if p.entries[objNum] == nil {\n                p.entries[objNum] = make(map[int]*XRefEntry)\n            }\n            p.entries[objNum][entry.Generation] = entry\n        }\n    }\n    \n    // Parse trailer dictionary\n    return p.parseTrailer()\n}\n\n// parseXRefStream handles cross-reference streams (PDF 1.5+)\nfunc (p *XRefParser) parseXRefStream() (*TrailerDict, error) {\n    // Parse the stream object\n    obj, err := p.parseIndirectObject()\n    if err != nil {\n        return nil, fmt.Errorf(\"parse xref stream object: %w\", err)\n    }\n    \n    stream, ok := obj.(*StreamObject)\n    if !ok {\n        return nil, fmt.Errorf(\"expected stream object for xref stream\")\n    }\n    \n    // Decode stream data\n    data, err := stream.Decode()\n    if err != nil {\n        return nil, fmt.Errorf(\"decode xref stream: %w\", err)\n    }\n    \n    // Parse W array for field widths\n    w, err := stream.Dict.GetIntArray(\"W\")\n    if err != nil || len(w) != 3 {\n        return nil, fmt.Errorf(\"invalid W array in xref stream\")\n    }\n    \n    // Parse Index array for subsections\n    index := stream.Dict.GetIntArray(\"Index\")\n    if len(index) == 0 {\n        // Default: single subsection starting at 0\n        size := stream.Dict.GetInt(\"Size\")\n        index = []int{0, size}\n    }\n    \n    // Parse entries from decoded stream\n    entrySize := w[0] + w[1] + w[2]\n    offset := 0\n    \n    for i := 0; i < len(index); i += 2 {\n        start := index[i]\n        count := index[i+1]\n        \n        for j := 0; j < count; j++ {\n            if offset+entrySize > len(data) {\n                return nil, fmt.Errorf(\"xref stream data underflow\")\n            }\n            \n            entry := p.parseXRefStreamEntry(data[offset:offset+entrySize], w)\n            objNum := start + j\n            \n            if p.entries[objNum] == nil {\n                p.entries[objNum] = make(map[int]*XRefEntry)\n            }\n            p.entries[objNum][entry.Generation] = entry\n            \n            offset += entrySize\n        }\n    }\n    \n    // Extract trailer info from stream dictionary\n    return p.extractTrailerFromStream(stream.Dict), nil\n}\n\n// ResolveObject finds and returns the offset/location of an object\nfunc (p *XRefParser) ResolveObject(objNum, generation int) (*XRefEntry, error) {\n    objEntries, exists := p.entries[objNum]\n    if !exists {\n        return nil, fmt.Errorf(\"object %d not found in xref\", objNum)\n    }\n    \n    // For compressed objects, generation is always 0\n    if generation == 0 {\n        // Check for compressed entry first\n        for _, entry := range objEntries {\n            if entry.Type == EntryCompressed {\n                return entry, nil\n            }\n        }\n    }\n    \n    entry, exists := objEntries[generation]\n    if !exists {\n        return nil, fmt.Errorf(\"object %d generation %d not found\", objNum, generation)\n    }\n    \n    if entry.Type == EntryFree {\n        return nil, fmt.Errorf(\"object %d generation %d is free\", objNum, generation)\n    }\n    \n    return entry, nil\n}\n\n// GetLatestGeneration returns the latest generation number for an object\nfunc (p *XRefParser) GetLatestGeneration(objNum int) (int, error) {\n    objEntries, exists := p.entries[objNum]\n    if !exists {\n        return 0, fmt.Errorf(\"object %d not found\", objNum)\n    }\n    \n    maxGen := -1\n    for gen, entry := range objEntries {\n        if entry.Type != EntryFree && gen > maxGen {\n            maxGen = gen\n        }\n    }\n    \n    if maxGen == -1 {\n        return 0, fmt.Errorf(\"no valid generation for object %d\", objNum)\n    }\n    \n    return maxGen, nil\n}\n```\n\nImplement object resolution with support for compressed object streams:\n\n```go\n// internal/pdf/xref/resolver.go\npackage xref\n\ntype ObjectResolver struct {\n    parser       *XRefParser\n    reader       io.ReadSeeker\n    objectCache  map[string]PDFObject      // \"objNum:gen\" -> object\n    streamCache  map[int]*ObjectStream     // Object stream cache\n}\n\ntype ObjectStream struct {\n    First   int                    // First object offset\n    N       int                    // Number of objects\n    Objects map[int]PDFObject      // Parsed objects by number\n}\n\n// ResolveObject retrieves an object by reference\nfunc (r *ObjectResolver) ResolveObject(ref IndirectRef) (PDFObject, error) {\n    // Check cache first\n    key := fmt.Sprintf(\"%d:%d\", ref.ObjNum, ref.Generation)\n    if obj, exists := r.objectCache[key]; exists {\n        return obj, nil\n    }\n    \n    // Get xref entry\n    entry, err := r.parser.ResolveObject(ref.ObjNum, ref.Generation)\n    if err != nil {\n        return nil, err\n    }\n    \n    var obj PDFObject\n    \n    switch entry.Type {\n    case EntryInUse:\n        // Seek to object offset\n        if _, err := r.reader.Seek(entry.Offset, io.SeekStart); err != nil {\n            return nil, fmt.Errorf(\"seek to object: %w\", err)\n        }\n        \n        // Parse indirect object\n        obj, err = r.parseIndirectObject(ref.ObjNum, ref.Generation)\n        if err != nil {\n            return nil, err\n        }\n        \n    case EntryCompressed:\n        // Load from object stream\n        obj, err = r.loadFromObjectStream(entry.Offset, entry.StreamIndex, ref.ObjNum)\n        if err != nil {\n            return nil, err\n        }\n    }\n    \n    // Cache the object\n    r.objectCache[key] = obj\n    return obj, nil\n}\n\n// loadFromObjectStream extracts an object from a compressed object stream\nfunc (r *ObjectResolver) loadFromObjectStream(streamObjNum int64, index int, targetObjNum int) (PDFObject, error) {\n    // Check stream cache\n    stream, exists := r.streamCache[int(streamObjNum)]\n    if !exists {\n        // Load and parse the object stream\n        streamRef := IndirectRef{ObjNum: int(streamObjNum), Generation: 0}\n        streamObj, err := r.ResolveObject(streamRef)\n        if err != nil {\n            return nil, fmt.Errorf(\"load object stream %d: %w\", streamObjNum, err)\n        }\n        \n        stream, err = r.parseObjectStream(streamObj.(*StreamObject))\n        if err != nil {\n            return nil, err\n        }\n        \n        r.streamCache[int(streamObjNum)] = stream\n    }\n    \n    obj, exists := stream.Objects[targetObjNum]\n    if !exists {\n        return nil, fmt.Errorf(\"object %d not found in stream %d\", targetObjNum, streamObjNum)\n    }\n    \n    return obj, nil\n}\n```\n\nImplement hybrid parsing strategy for optimal performance:\n\n```go\n// internal/pdf/xref/hybrid_parser.go\npackage xref\n\ntype HybridParser struct {\n    xrefParser   *XRefParser\n    resolver     *ObjectResolver\n    linearized   bool\n    hint         *HintTable    // For linearized PDFs\n}\n\n// Initialize performs initial PDF analysis\nfunc (h *HybridParser) Initialize(reader io.ReadSeeker) error {\n    // Check for linearization\n    h.linearized = h.checkLinearization(reader)\n    \n    // Find and parse xref\n    startxref, err := h.findStartXRef(reader)\n    if err != nil {\n        return err\n    }\n    \n    h.xrefParser = &XRefParser{reader: reader}\n    if err := h.xrefParser.ParseXRef(startxref); err != nil {\n        return err\n    }\n    \n    h.resolver = &ObjectResolver{\n        parser: h.xrefParser,\n        reader: reader,\n        objectCache: make(map[string]PDFObject),\n        streamCache: make(map[int]*ObjectStream),\n    }\n    \n    // Parse hint table for linearized PDFs\n    if h.linearized {\n        h.hint, _ = h.parseHintTable()\n    }\n    \n    return nil\n}\n\n// findStartXRef locates the startxref offset from EOF\nfunc (h *HybridParser) findStartXRef(reader io.ReadSeeker) (int64, error) {\n    // Seek to end of file\n    size, err := reader.Seek(0, io.SeekEnd)\n    if err != nil {\n        return 0, err\n    }\n    \n    // Read last 1KB (should contain startxref)\n    bufSize := int64(1024)\n    if size < bufSize {\n        bufSize = size\n    }\n    \n    reader.Seek(-bufSize, io.SeekEnd)\n    buf := make([]byte, bufSize)\n    reader.Read(buf)\n    \n    // Find startxref keyword\n    idx := bytes.LastIndex(buf, []byte(\"startxref\"))\n    if idx == -1 {\n        return 0, fmt.Errorf(\"startxref not found\")\n    }\n    \n    // Parse the offset\n    scanner := bufio.NewScanner(bytes.NewReader(buf[idx+9:]))\n    scanner.Scan() // Skip \"startxref\"\n    scanner.Scan() // Get offset\n    \n    offset, err := strconv.ParseInt(scanner.Text(), 10, 64)\n    if err != nil {\n        return 0, fmt.Errorf(\"parse startxref offset: %w\", err)\n    }\n    \n    return offset, nil\n}\n```",
        "testStrategy": "Comprehensive testing strategy for cross-reference table parser:\n\n1. **Basic xref table parsing tests**:\n   - Test with simple PDF containing single xref table\n   - Verify correct parsing of xref entries (offset, generation, type)\n   - Test with multiple subsections in single xref table\n   - Validate trailer dictionary parsing\n\n2. **Cross-reference stream tests (PDF 1.5+)**:\n   - Test parsing of compressed xref streams\n   - Verify correct decoding with different W array configurations\n   - Test Index array handling for non-contiguous object numbers\n   - Validate stream dictionary trailer extraction\n\n3. **Incremental update tests**:\n   - Create PDFs with multiple xref sections linked by Prev\n   - Verify correct parsing of entire Prev chain\n   - Test that newer entries override older ones\n   - Validate proper trailer chain handling\n\n4. **Object resolution tests**:\n   - Test resolution of regular objects via byte offset\n   - Test resolution of compressed objects in object streams\n   - Verify generation number handling\n   - Test cache effectiveness with repeated lookups\n\n5. **Error handling tests**:\n   - Test with corrupted xref tables\n   - Test with missing startxref\n   - Test with invalid xref entries\n   - Verify graceful handling of circular Prev references\n\n6. **Performance benchmarks**:\n   - Measure xref parsing time for large PDFs (1000+ objects)\n   - Test memory usage with extensive object caching\n   - Benchmark random access vs sequential access patterns\n   - Compare performance with and without object stream caching\n\n7. **Compatibility tests**:\n   - Test with PDF 1.4 (traditional xref only)\n   - Test with PDF 1.5+ (xref streams)\n   - Test with linearized PDFs\n   - Test with encrypted PDFs (xref remains unencrypted)\n\n8. **Integration tests**:\n   - Test with real-world PDFs from various sources\n   - Verify compatibility with existing parser components\n   - Test object resolution through multiple layers of references\n   - Validate proper cleanup of resources",
        "status": "done",
        "dependencies": [
          2,
          3
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 35,
        "title": "Implement PDF Encryption and Security Handler",
        "description": "Add support for the Standard Security Handler with password validation, RC4/AES decryption, and permission checking. This is required for reading encrypted PDF documents and PDF 1.4 compliance.",
        "details": "Implement a comprehensive PDF encryption and security handling system that supports the Standard Security Handler (PDF 1.4/1.7 section 7.6):\n\n```go\n// internal/pdf/security/handler.go\npackage security\n\nimport (\n    \"crypto/aes\"\n    \"crypto/cipher\"\n    \"crypto/md5\"\n    \"crypto/rc4\"\n    \"crypto/sha256\"\n    \"errors\"\n)\n\ntype SecurityHandler interface {\n    Authenticate(password []byte) error\n    DecryptObject(objNum, genNum int, data []byte) ([]byte, error)\n    GetPermissions() Permissions\n    IsEncrypted() bool\n}\n\ntype StandardSecurityHandler struct {\n    encryptDict    *EncryptionDictionary\n    encryptionKey  []byte\n    authenticated  bool\n    permissions    uint32\n    revision       int\n    keyLength      int\n}\n\ntype EncryptionDictionary struct {\n    Filter       string  // Standard\n    SubFilter    string  // Optional\n    V            int     // Version (1-5)\n    Length       int     // Key length in bits\n    R            int     // Revision (2-6)\n    O            []byte  // Owner password hash\n    U            []byte  // User password hash\n    OE           []byte  // Owner encryption key (R=6)\n    UE           []byte  // User encryption key (R=6)\n    P            int32   // Permissions\n    EncryptMetadata bool // Whether to encrypt metadata\n    StmF         string  // Stream filter\n    StrF         string  // String filter\n    CF           map[string]CryptFilter // Crypt filters\n}\n\n// internal/pdf/security/permissions.go\ntype Permissions struct {\n    Print            bool // Bit 3\n    Modify           bool // Bit 4\n    Copy             bool // Bit 5\n    Annotate         bool // Bit 6\n    FillForms        bool // Bit 9\n    Extract          bool // Bit 10\n    Assemble         bool // Bit 11\n    PrintHighQuality bool // Bit 12\n}\n\nfunc (p Permissions) FromInt32(perms int32) Permissions {\n    return Permissions{\n        Print:            (perms & 0x04) != 0,\n        Modify:           (perms & 0x08) != 0,\n        Copy:             (perms & 0x10) != 0,\n        Annotate:         (perms & 0x20) != 0,\n        FillForms:        (perms & 0x200) != 0,\n        Extract:          (perms & 0x400) != 0,\n        Assemble:         (perms & 0x800) != 0,\n        PrintHighQuality: (perms & 0x1000) != 0,\n    }\n}\n\n// internal/pdf/security/algorithms.go\nfunc (h *StandardSecurityHandler) computeEncryptionKey(password []byte) []byte {\n    switch h.revision {\n    case 2, 3, 4:\n        return h.computeRC4Key(password)\n    case 5, 6:\n        return h.computeAESKey(password)\n    default:\n        return nil\n    }\n}\n\nfunc (h *StandardSecurityHandler) computeRC4Key(password []byte) []byte {\n    // Algorithm 2 from PDF spec\n    padded := h.padPassword(password)\n    hash := md5.New()\n    hash.Write(padded)\n    hash.Write(h.encryptDict.O)\n    hash.Write(intToBytes(h.encryptDict.P))\n    hash.Write(h.fileID)\n    \n    if h.revision >= 4 && !h.encryptDict.EncryptMetadata {\n        hash.Write([]byte{0xff, 0xff, 0xff, 0xff})\n    }\n    \n    digest := hash.Sum(nil)\n    \n    // For revision 3 or greater, do additional processing\n    if h.revision >= 3 {\n        for i := 0; i < 50; i++ {\n            hash.Reset()\n            hash.Write(digest[:h.keyLength/8])\n            digest = hash.Sum(nil)\n        }\n    }\n    \n    return digest[:h.keyLength/8]\n}\n\n// internal/pdf/security/decryption.go\nfunc (h *StandardSecurityHandler) DecryptObject(objNum, genNum int, data []byte) ([]byte, error) {\n    if !h.authenticated {\n        return nil, errors.New(\"not authenticated\")\n    }\n    \n    // Compute object-specific key\n    objKey := h.computeObjectKey(objNum, genNum)\n    \n    switch h.encryptDict.V {\n    case 1, 2: // RC4\n        return h.decryptRC4(objKey, data)\n    case 4, 5: // AES\n        return h.decryptAES(objKey, data)\n    default:\n        return nil, fmt.Errorf(\"unsupported encryption version: %d\", h.encryptDict.V)\n    }\n}\n\nfunc (h *StandardSecurityHandler) decryptRC4(key, data []byte) ([]byte, error) {\n    cipher, err := rc4.NewCipher(key)\n    if err != nil {\n        return nil, err\n    }\n    \n    decrypted := make([]byte, len(data))\n    cipher.XORKeyStream(decrypted, data)\n    return decrypted, nil\n}\n\nfunc (h *StandardSecurityHandler) decryptAES(key, data []byte) ([]byte, error) {\n    if len(data) < aes.BlockSize {\n        return nil, errors.New(\"ciphertext too short\")\n    }\n    \n    block, err := aes.NewCipher(key)\n    if err != nil {\n        return nil, err\n    }\n    \n    // Extract IV (first 16 bytes)\n    iv := data[:aes.BlockSize]\n    ciphertext := data[aes.BlockSize:]\n    \n    mode := cipher.NewCBCDecrypter(block, iv)\n    decrypted := make([]byte, len(ciphertext))\n    mode.CryptBlocks(decrypted, ciphertext)\n    \n    // Remove PKCS7 padding\n    return removePKCS7Padding(decrypted)\n}\n\n// internal/pdf/security/parser.go\ntype SecurityParser struct {\n    resolver ObjectResolver\n}\n\nfunc (p *SecurityParser) ParseEncryptDict(dict map[string]Object) (*EncryptionDictionary, error) {\n    enc := &EncryptionDictionary{\n        EncryptMetadata: true, // Default\n    }\n    \n    // Parse required fields\n    if filter, ok := dict[\"Filter\"].(Name); ok {\n        enc.Filter = string(filter)\n    } else {\n        return nil, errors.New(\"missing Filter in encryption dictionary\")\n    }\n    \n    if v, ok := dict[\"V\"].(Integer); ok {\n        enc.V = int(v)\n    } else {\n        return nil, errors.New(\"missing V in encryption dictionary\")\n    }\n    \n    // Parse algorithm-specific fields\n    if enc.V >= 2 {\n        if length, ok := dict[\"Length\"].(Integer); ok {\n            enc.Length = int(length)\n        } else {\n            enc.Length = 40 // Default for V=1\n        }\n    }\n    \n    // Parse password hashes\n    if o, ok := dict[\"O\"].(String); ok {\n        enc.O = []byte(o)\n    }\n    if u, ok := dict[\"U\"].(String); ok {\n        enc.U = []byte(u)\n    }\n    \n    // Parse permissions\n    if p, ok := dict[\"P\"].(Integer); ok {\n        enc.P = int32(p)\n    }\n    \n    return enc, nil\n}\n\n// Integration with main parser\n// internal/pdf/parser.go (extension)\ntype PDFParser struct {\n    // ... existing fields ...\n    security SecurityHandler\n}\n\nfunc (p *PDFParser) checkEncryption() error {\n    if encryptRef, ok := p.trailer[\"Encrypt\"]; ok {\n        // Resolve encryption dictionary\n        encryptDict := p.resolveObject(encryptRef)\n        \n        // Parse encryption dictionary\n        parser := &SecurityParser{resolver: p}\n        encDict, err := parser.ParseEncryptDict(encryptDict)\n        if err != nil {\n            return err\n        }\n        \n        // Create appropriate security handler\n        switch encDict.Filter {\n        case \"Standard\":\n            p.security = NewStandardSecurityHandler(encDict, p.fileID)\n        default:\n            return fmt.Errorf(\"unsupported security handler: %s\", encDict.Filter)\n        }\n    }\n    \n    return nil\n}\n\n// Modify object reading to handle decryption\nfunc (p *PDFParser) readObject(ref ObjectRef) (Object, error) {\n    // ... existing object reading code ...\n    \n    if p.security != nil && p.security.IsEncrypted() {\n        // Decrypt strings and streams\n        obj = p.decryptObject(ref.ObjNum, ref.GenNum, obj)\n    }\n    \n    return obj, nil\n}",
        "testStrategy": "Comprehensive testing strategy for PDF encryption and security handler:\n\n1. **Password validation tests**:\n   - Test with correct user and owner passwords\n   - Test with incorrect passwords (should fail authentication)\n   - Test with empty password (for PDFs with no user password)\n   - Test password padding algorithm for passwords < 32 bytes\n   - Test with Unicode passwords (UTF-8 encoding)\n\n2. **Encryption algorithm tests**:\n   - Test RC4 decryption (40-bit and 128-bit keys) for V=1,2\n   - Test AES-128 decryption for V=4\n   - Test AES-256 decryption for V=5\n   - Verify correct IV extraction for AES\n   - Test PKCS7 padding removal\n   - Test object-specific key generation\n\n3. **Permission checking tests**:\n   - Test all permission bits (print, modify, copy, etc.)\n   - Verify permission enforcement based on P value\n   - Test with various permission combinations\n   - Ensure permissions are correctly parsed from negative int32 values\n\n4. **Integration tests with encrypted PDFs**:\n   - Test with PDF 1.4 encrypted documents (40-bit RC4)\n   - Test with PDF 1.6 encrypted documents (128-bit AES)\n   - Test with PDF 1.7 encrypted documents (256-bit AES)\n   - Verify correct decryption of strings, streams, and metadata\n   - Test with PDFs that don't encrypt metadata\n\n5. **Edge cases and error handling**:\n   - Test with malformed encryption dictionaries\n   - Test with unsupported encryption versions\n   - Test with missing required fields\n   - Test with corrupted encrypted data\n   - Verify proper error messages for each failure case\n\n6. **Performance tests**:\n   - Benchmark decryption of large encrypted PDFs\n   - Measure overhead of decryption vs unencrypted parsing\n   - Test memory usage with streaming decryption\n\n7. **Compatibility tests**:\n   - Test with PDFs encrypted by various tools (Adobe Acrobat, pdfcpu, etc.)\n   - Verify interoperability with different encryption implementations\n   - Test incremental updates on encrypted PDFs",
        "status": "done",
        "dependencies": [
          2,
          3,
          34
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 36,
        "title": "Implement Advanced Color Space Support",
        "description": "Add support for CIE-based color spaces (CalGray, CalRGB, Lab), ICCBased color profiles, and special color spaces (Indexed, Pattern, Separation, DeviceN) for accurate color rendering and PDF 1.4 compliance.",
        "details": "Implement a comprehensive color space handling system that supports all PDF color spaces according to PDF 1.4/1.7 specifications:\n\n```go\n// internal/pdf/color/colorspace.go\npackage color\n\nimport (\n    \"encoding/binary\"\n    \"fmt\"\n    \"io\"\n    \"math\"\n)\n\ntype ColorSpace interface {\n    Name() string\n    NumComponents() int\n    ToRGB(components []float64) (r, g, b float64, err error)\n    DefaultColor() []float64\n}\n\n// Base color spaces\ntype DeviceGray struct{}\ntype DeviceRGB struct{}\ntype DeviceCMYK struct{}\n\n// CIE-based color spaces\ntype CalGray struct {\n    WhitePoint [3]float64 // Required: X, Y, Z of white point\n    BlackPoint [3]float64 // Optional: defaults to [0, 0, 0]\n    Gamma      float64    // Optional: defaults to 1.0\n}\n\ntype CalRGB struct {\n    WhitePoint [3]float64   // Required\n    BlackPoint [3]float64   // Optional\n    Gamma      [3]float64   // Optional: defaults to [1, 1, 1]\n    Matrix     [9]float64   // Optional: linear transformation matrix\n}\n\ntype Lab struct {\n    WhitePoint [3]float64 // Required\n    BlackPoint [3]float64 // Optional\n    Range      [4]float64 // Optional: [a_min, a_max, b_min, b_max]\n}\n\n// ICC-based color space\ntype ICCBased struct {\n    Profile    []byte      // ICC profile data\n    Alternate  ColorSpace  // Fallback color space\n    N          int         // Number of components\n    metadata   ICCMetadata // Parsed ICC header\n}\n\n// Special color spaces\ntype Indexed struct {\n    Base     ColorSpace\n    HiVal    int      // Maximum index value\n    Lookup   []byte   // Color lookup table\n}\n\ntype Pattern struct {\n    UnderlyingSpace ColorSpace // Optional underlying color space\n}\n\ntype Separation struct {\n    Name      string\n    Alternate ColorSpace\n    TintTransform Function // Maps tint value to alternate space\n}\n\ntype DeviceN struct {\n    Names         []string\n    Alternate     ColorSpace\n    TintTransform Function\n    Attributes    map[string]interface{} // Optional attributes\n}\n\n// internal/pdf/color/parser.go\ntype ColorSpaceParser struct {\n    resolver ObjectResolver\n    cache    map[string]ColorSpace\n}\n\nfunc (p *ColorSpaceParser) ParseColorSpace(obj Object) (ColorSpace, error) {\n    switch v := obj.(type) {\n    case Name:\n        return p.parseNamedColorSpace(string(v))\n    case Array:\n        return p.parseArrayColorSpace(v)\n    default:\n        return nil, fmt.Errorf(\"invalid color space object type: %T\", obj)\n    }\n}\n\nfunc (p *ColorSpaceParser) parseArrayColorSpace(arr Array) (ColorSpace, error) {\n    if len(arr) == 0 {\n        return nil, errors.New(\"empty color space array\")\n    }\n    \n    name, ok := arr[0].(Name)\n    if !ok {\n        return nil, errors.New(\"color space array must start with name\")\n    }\n    \n    switch string(name) {\n    case \"CalGray\":\n        return p.parseCalGray(arr)\n    case \"CalRGB\":\n        return p.parseCalRGB(arr)\n    case \"Lab\":\n        return p.parseLab(arr)\n    case \"ICCBased\":\n        return p.parseICCBased(arr)\n    case \"Indexed\":\n        return p.parseIndexed(arr)\n    case \"Pattern\":\n        return p.parsePattern(arr)\n    case \"Separation\":\n        return p.parseSeparation(arr)\n    case \"DeviceN\":\n        return p.parseDeviceN(arr)\n    default:\n        return nil, fmt.Errorf(\"unknown color space: %s\", name)\n    }\n}\n\n// internal/pdf/color/icc.go\ntype ICCProfileParser struct {\n    data []byte\n}\n\ntype ICCMetadata struct {\n    ProfileSize        uint32\n    PreferredCMM       string\n    Version            string\n    ProfileClass       string\n    ColorSpace         string\n    PCS                string // Profile Connection Space\n    CreationDate       time.Time\n    ProfileSignature   string\n    PrimaryPlatform    string\n    Flags              uint32\n    DeviceManufacturer string\n    DeviceModel        string\n    RenderingIntent    uint32\n}\n\nfunc (p *ICCProfileParser) Parse() (*ICCMetadata, error) {\n    if len(p.data) < 128 {\n        return nil, errors.New(\"ICC profile too small\")\n    }\n    \n    // Parse ICC profile header (first 128 bytes)\n    meta := &ICCMetadata{}\n    meta.ProfileSize = binary.BigEndian.Uint32(p.data[0:4])\n    meta.PreferredCMM = string(p.data[4:8])\n    // ... parse remaining header fields\n    \n    return meta, nil\n}\n\n// internal/pdf/color/conversion.go\nfunc (cs *CalGray) ToRGB(components []float64) (r, g, b float64, err error) {\n    if len(components) != 1 {\n        return 0, 0, 0, fmt.Errorf(\"CalGray requires 1 component, got %d\", len(components))\n    }\n    \n    gray := components[0]\n    \n    // Apply gamma correction\n    if cs.Gamma != 0 && cs.Gamma != 1.0 {\n        gray = math.Pow(gray, cs.Gamma)\n    }\n    \n    // Convert to XYZ using white point\n    X := gray * cs.WhitePoint[0]\n    Y := gray * cs.WhitePoint[1]\n    Z := gray * cs.WhitePoint[2]\n    \n    // Convert XYZ to RGB (sRGB matrix)\n    r = 3.2406*X - 1.5372*Y - 0.4986*Z\n    g = -0.9689*X + 1.8758*Y + 0.0415*Z\n    b = 0.0557*X - 0.2040*Y + 1.0570*Z\n    \n    // Clamp to [0, 1]\n    r = math.Max(0, math.Min(1, r))\n    g = math.Max(0, math.Min(1, g))\n    b = math.Max(0, math.Min(1, b))\n    \n    return r, g, b, nil\n}\n\nfunc (cs *Lab) ToRGB(components []float64) (r, g, b float64, err error) {\n    if len(components) != 3 {\n        return 0, 0, 0, fmt.Errorf(\"Lab requires 3 components, got %d\", len(components))\n    }\n    \n    L := components[0]\n    a := components[1]\n    b_star := components[2]\n    \n    // Apply range if specified\n    if cs.Range[0] != 0 || cs.Range[1] != 0 {\n        a = cs.Range[0] + a*(cs.Range[1]-cs.Range[0])/100.0\n    }\n    if cs.Range[2] != 0 || cs.Range[3] != 0 {\n        b_star = cs.Range[2] + b_star*(cs.Range[3]-cs.Range[2])/100.0\n    }\n    \n    // Convert Lab to XYZ\n    fy := (L + 16) / 116\n    fx := a/500 + fy\n    fz := fy - b_star/200\n    \n    // Helper function for Lab to XYZ conversion\n    f := func(t float64) float64 {\n        if t > 0.206893 {\n            return t * t * t\n        }\n        return (t - 16.0/116.0) / 7.787\n    }\n    \n    X := cs.WhitePoint[0] * f(fx)\n    Y := cs.WhitePoint[1] * f(fy)\n    Z := cs.WhitePoint[2] * f(fz)\n    \n    // Convert XYZ to RGB\n    return xyzToRGB(X, Y, Z)\n}\n\n// internal/pdf/color/function.go\ntype Function interface {\n    Evaluate(inputs []float64) ([]float64, error)\n    InputDimension() int\n    OutputDimension() int\n}\n\ntype SampledFunction struct {\n    Domain     []float64\n    Range      []float64\n    Size       []int\n    BitsPerSample int\n    Samples    []byte\n    Encode     []float64\n    Decode     []float64\n}\n\ntype ExponentialFunction struct {\n    Domain []float64\n    Range  []float64\n    C0     []float64\n    C1     []float64\n    N      float64\n}\n\n// internal/pdf/color/renderer.go\ntype ColorRenderer struct {\n    colorSpaces map[string]ColorSpace\n    currentSpace ColorSpace\n}\n\nfunc (r *ColorRenderer) SetColorSpace(name string, cs ColorSpace) {\n    r.colorSpaces[name] = cs\n    r.currentSpace = cs\n}\n\nfunc (r *ColorRenderer) RenderColor(components []float64) (Color, error) {\n    if r.currentSpace == nil {\n        return Color{}, errors.New(\"no color space set\")\n    }\n    \n    rgb, err := r.currentSpace.ToRGB(components)\n    if err != nil {\n        return Color{}, err\n    }\n    \n    return Color{\n        R: uint8(rgb[0] * 255),\n        G: uint8(rgb[1] * 255),\n        B: uint8(rgb[2] * 255),\n        A: 255,\n    }, nil\n}\n```\n\nKey implementation considerations:\n\n1. **Color Space Hierarchy**:\n   - Implement base interface for all color spaces\n   - Support device-dependent spaces (Gray, RGB, CMYK)\n   - Implement CIE-based spaces with proper white point handling\n   - Support ICC profiles with fallback mechanisms\n\n2. **ICC Profile Support**:\n   - Parse ICC profile headers to extract metadata\n   - Implement profile caching to avoid re-parsing\n   - Provide fallback to alternate color space when ICC fails\n   - Support both v2 and v4 ICC profiles\n\n3. **Special Color Spaces**:\n   - Indexed: Implement efficient lookup table access\n   - Pattern: Handle both colored and uncolored patterns\n   - Separation: Support spot colors with tint transforms\n   - DeviceN: Handle multiple spot colors simultaneously\n\n4. **Color Conversion**:\n   - Implement accurate XYZ to RGB conversion\n   - Support different illuminants (D50, D65)\n   - Handle gamma correction properly\n   - Implement clamping and rounding correctly\n\n5. **Function Support**:\n   - Implement Type 0 (sampled) functions for tint transforms\n   - Support Type 2 (exponential) functions\n   - Handle multi-dimensional interpolation for sampled functions\n\n6. **Performance Optimization**:\n   - Cache parsed color spaces to avoid re-parsing\n   - Pre-compute conversion matrices where possible\n   - Use lookup tables for indexed color spaces\n   - Implement lazy loading for ICC profiles",
        "testStrategy": "Comprehensive testing strategy for advanced color space support:\n\n1. **Basic Color Space Tests**:\n   - Test DeviceGray, DeviceRGB, DeviceCMYK with known values\n   - Verify correct number of components for each space\n   - Test default color values for each space\n   - Validate color clamping to [0, 1] range\n\n2. **CIE-Based Color Space Tests**:\n   - Test CalGray with different gamma values (1.0, 1.8, 2.2)\n   - Verify CalRGB with identity and non-identity matrices\n   - Test Lab color space with standard D50 white point\n   - Validate Lab with custom ranges [-100, 100] for a* and b*\n   - Compare conversions against reference implementations\n\n3. **ICC Profile Tests**:\n   - Test parsing of valid ICC v2 and v4 profiles\n   - Verify header extraction (size, version, color space, etc.)\n   - Test with corrupted ICC data (should fall back to alternate)\n   - Validate profile caching works correctly\n   - Test with standard sRGB and Adobe RGB profiles\n\n4. **Indexed Color Space Tests**:\n   - Test with 256-color palette (8-bit indexed)\n   - Verify correct lookup for edge cases (index 0, max index)\n   - Test with different base color spaces (RGB, CMYK)\n   - Validate error handling for out-of-range indices\n\n5. **Separation Color Space Tests**:\n   - Test standard spot colors (PANTONE examples)\n   - Verify tint transform function evaluation\n   - Test with linear and non-linear tint functions\n   - Validate conversion to alternate color space\n\n6. **DeviceN Color Space Tests**:\n   - Test with multiple spot colors (2, 3, 4 components)\n   - Verify correct mapping to alternate space\n   - Test with \"All\" and \"None\" special colorant names\n   - Validate attributes dictionary parsing\n\n7. **Pattern Color Space Tests**:\n   - Test uncolored patterns (no underlying space)\n   - Test colored patterns with RGB underlying space\n   - Verify pattern color space validation\n\n8. **Function Tests**:\n   - Test Type 0 (sampled) functions with 1D and 2D inputs\n   - Verify interpolation accuracy for sampled functions\n   - Test Type 2 (exponential) functions with various N values\n   - Validate domain and range clamping\n\n9. **Integration Tests**:\n   - Test color space parsing from actual PDF objects\n   - Verify color rendering in content streams\n   - Test with PDF 1.4 transparency groups\n   - Validate against PDF test suite color examples\n\n10. **Performance Tests**:\n    - Benchmark color conversion performance\n    - Test caching effectiveness with repeated conversions\n    - Measure memory usage for large ICC profiles\n    - Verify no memory leaks in color space lifecycle",
        "status": "pending",
        "dependencies": [
          2,
          3,
          34
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 37,
        "title": "Implement PDF 1.4 Transparency Model",
        "description": "Add complete transparency support including transparency groups, blend modes (Normal, Multiply, Screen, Overlay, etc.), opacity/alpha channels, soft masks, and compositing model. This is the major new feature in PDF 1.4 and critical for full compliance.",
        "details": "Implement a comprehensive PDF 1.4 transparency model that supports all transparency features according to PDF 1.4/1.7 specifications (Chapter 11):\n\n```go\n// internal/pdf/transparency/model.go\npackage transparency\n\nimport (\n    \"fmt\"\n    \"math\"\n    \"github.com/yourusername/pdfextract/internal/pdf/color\"\n)\n\ntype TransparencyModel struct {\n    colorSpace    color.ColorSpace\n    blendMode     BlendMode\n    groupStack    []*TransparencyGroup\n    softMaskStack []*SoftMask\n}\n\ntype BlendMode int\n\nconst (\n    BlendNormal BlendMode = iota\n    BlendMultiply\n    BlendScreen\n    BlendOverlay\n    BlendDarken\n    BlendLighten\n    BlendColorDodge\n    BlendColorBurn\n    BlendHardLight\n    BlendSoftLight\n    BlendDifference\n    BlendExclusion\n    BlendHue\n    BlendSaturation\n    BlendColor\n    BlendLuminosity\n)\n\n// Transparency group for isolated/knockout rendering\ntype TransparencyGroup struct {\n    ColorSpace    color.ColorSpace\n    Isolated      bool\n    Knockout      bool\n    BlendMode     BlendMode\n    Alpha         float64\n    SoftMask      *SoftMask\n    BoundingBox   *Rectangle\n    BackdropColor []float64\n}\n\n// Soft mask for alpha/luminosity masking\ntype SoftMask struct {\n    Type          SoftMaskType // Alpha or Luminosity\n    Group         *TransparencyGroup\n    BackdropColor []float64\n    TransferFunc  TransferFunction\n}\n\ntype SoftMaskType int\n\nconst (\n    SoftMaskAlpha SoftMaskType = iota\n    SoftMaskLuminosity\n)\n\n// internal/pdf/transparency/blending.go\npackage transparency\n\n// Implement all PDF blend modes according to PDF 1.4 spec\nfunc (tm *TransparencyModel) Blend(backdrop, source []float64, alpha float64, mode BlendMode) []float64 {\n    switch mode {\n    case BlendNormal:\n        return tm.blendNormal(backdrop, source, alpha)\n    case BlendMultiply:\n        return tm.blendMultiply(backdrop, source, alpha)\n    case BlendScreen:\n        return tm.blendScreen(backdrop, source, alpha)\n    case BlendOverlay:\n        return tm.blendOverlay(backdrop, source, alpha)\n    // ... implement all 16 blend modes\n    }\n}\n\nfunc (tm *TransparencyModel) blendNormal(backdrop, source []float64, alpha float64) []float64 {\n    result := make([]float64, len(backdrop))\n    for i := range backdrop {\n        result[i] = (1-alpha)*backdrop[i] + alpha*source[i]\n    }\n    return result\n}\n\nfunc (tm *TransparencyModel) blendMultiply(backdrop, source []float64, alpha float64) []float64 {\n    result := make([]float64, len(backdrop))\n    for i := range backdrop {\n        result[i] = (1-alpha)*backdrop[i] + alpha*(backdrop[i]*source[i])\n    }\n    return result\n}\n\n// internal/pdf/transparency/compositor.go\npackage transparency\n\ntype Compositor struct {\n    model         *TransparencyModel\n    deviceBuffer  *DeviceBuffer\n    groupStack    []*GroupBuffer\n    alphaConstant float64\n    shapeAlpha    float64\n}\n\n// Composite a transparency group onto the backdrop\nfunc (c *Compositor) CompositeGroup(group *TransparencyGroup, content func()) error {\n    // 1. Create group buffer\n    groupBuffer := c.createGroupBuffer(group)\n    \n    // 2. Push group onto stack\n    c.groupStack = append(c.groupStack, groupBuffer)\n    \n    // 3. Initialize group with backdrop if not isolated\n    if !group.Isolated {\n        c.initializeBackdrop(groupBuffer)\n    }\n    \n    // 4. Render content into group\n    content()\n    \n    // 5. Apply soft mask if present\n    if group.SoftMask != nil {\n        c.applySoftMask(groupBuffer, group.SoftMask)\n    }\n    \n    // 6. Composite group onto parent with blend mode\n    c.compositeToParent(groupBuffer, group.BlendMode, group.Alpha)\n    \n    // 7. Pop group from stack\n    c.groupStack = c.groupStack[:len(c.groupStack)-1]\n    \n    return nil\n}\n\n// internal/pdf/transparency/parser.go\npackage transparency\n\nimport (\n    \"github.com/yourusername/pdfextract/internal/pdf\"\n)\n\ntype TransparencyParser struct {\n    resolver pdf.ObjectResolver\n}\n\n// Parse ExtGState dictionary for transparency parameters\nfunc (tp *TransparencyParser) ParseExtGState(dict map[string]pdf.Object) (*GraphicsState, error) {\n    gs := &GraphicsState{}\n    \n    // Parse blend mode\n    if bm, ok := dict[\"BM\"]; ok {\n        gs.BlendMode = tp.parseBlendMode(bm)\n    }\n    \n    // Parse constant alpha\n    if ca, ok := dict[\"ca\"]; ok {\n        gs.StrokeAlpha = pdf.GetFloat(ca)\n    }\n    if CA, ok := dict[\"CA\"]; ok {\n        gs.FillAlpha = pdf.GetFloat(CA)\n    }\n    \n    // Parse soft mask\n    if smask, ok := dict[\"SMask\"]; ok {\n        gs.SoftMask = tp.parseSoftMask(smask)\n    }\n    \n    // Parse alpha source flag\n    if ais, ok := dict[\"AIS\"]; ok {\n        gs.AlphaIsShape = pdf.GetBool(ais)\n    }\n    \n    return gs, nil\n}\n\n// Parse transparency group from Page or Form XObject\nfunc (tp *TransparencyParser) ParseTransparencyGroup(dict map[string]pdf.Object) (*TransparencyGroup, error) {\n    group, ok := dict[\"Group\"].(map[string]pdf.Object)\n    if !ok {\n        return nil, nil\n    }\n    \n    tg := &TransparencyGroup{}\n    \n    // Check it's a transparency group\n    if s, ok := group[\"S\"]; ok && pdf.GetName(s) != \"Transparency\" {\n        return nil, nil\n    }\n    \n    // Parse color space\n    if cs, ok := group[\"CS\"]; ok {\n        tg.ColorSpace = tp.parseColorSpace(cs)\n    }\n    \n    // Parse isolation flag\n    if i, ok := group[\"I\"]; ok {\n        tg.Isolated = pdf.GetBool(i)\n    }\n    \n    // Parse knockout flag\n    if k, ok := group[\"K\"]; ok {\n        tg.Knockout = pdf.GetBool(k)\n    }\n    \n    return tg, nil\n}\n\n// internal/pdf/transparency/state.go\npackage transparency\n\n// Graphics state with transparency parameters\ntype GraphicsState struct {\n    BlendMode    BlendMode\n    FillAlpha    float64\n    StrokeAlpha  float64\n    AlphaIsShape bool\n    SoftMask     *SoftMask\n}\n\n// State machine for tracking transparency state during rendering\ntype TransparencyStateMachine struct {\n    stateStack []*GraphicsState\n    current    *GraphicsState\n}\n\nfunc (tsm *TransparencyStateMachine) Save() {\n    state := *tsm.current\n    tsm.stateStack = append(tsm.stateStack, &state)\n}\n\nfunc (tsm *TransparencyStateMachine) Restore() error {\n    if len(tsm.stateStack) == 0 {\n        return fmt.Errorf(\"state stack underflow\")\n    }\n    tsm.current = tsm.stateStack[len(tsm.stateStack)-1]\n    tsm.stateStack = tsm.stateStack[:len(tsm.stateStack)-1]\n    return nil\n}\n\n// internal/pdf/transparency/renderer.go\npackage transparency\n\n// Integration with content stream rendering\ntype TransparencyRenderer struct {\n    compositor *Compositor\n    state      *TransparencyStateMachine\n    parser     *TransparencyParser\n}\n\n// Handle transparency-related operators in content stream\nfunc (tr *TransparencyRenderer) HandleOperator(op string, operands []pdf.Object) error {\n    switch op {\n    case \"gs\": // Set graphics state with ExtGState\n        name := pdf.GetName(operands[0])\n        extGState := tr.getExtGState(name)\n        return tr.applyExtGState(extGState)\n        \n    case \"BM\": // Set blend mode (inline)\n        mode := tr.parser.parseBlendMode(operands[0])\n        tr.state.current.BlendMode = mode\n        \n    case \"ca\": // Set stroke alpha\n        tr.state.current.StrokeAlpha = pdf.GetFloat(operands[0])\n        \n    case \"CA\": // Set fill alpha\n        tr.state.current.FillAlpha = pdf.GetFloat(operands[0])\n    }\n    return nil\n}\n\n// Render a Form XObject with potential transparency group\nfunc (tr *TransparencyRenderer) RenderFormXObject(xobj *FormXObject) error {\n    // Check if it has a transparency group\n    group := tr.parser.ParseTransparencyGroup(xobj.Dict)\n    \n    if group != nil {\n        // Render as transparency group\n        return tr.compositor.CompositeGroup(group, func() {\n            tr.renderContentStream(xobj.Stream)\n        })\n    } else {\n        // Render normally\n        return tr.renderContentStream(xobj.Stream)\n    }\n}",
        "testStrategy": "Comprehensive testing strategy for PDF 1.4 transparency model:\n\n1. **Blend Mode Tests**:\n   - Test all 16 blend modes with known input/output values from PDF specification\n   - Verify Normal mode: result = (1-)backdrop + source\n   - Verify Multiply mode: result = backdrop  source\n   - Test Screen mode: result = backdrop + source - backdropsource\n   - Test Overlay mode combines Multiply and Screen based on backdrop\n   - Verify separable vs non-separable blend modes\n   - Test with grayscale, RGB, and CMYK color spaces\n\n2. **Alpha Channel Tests**:\n   - Test constant alpha (ca/CA) application to strokes and fills\n   - Verify shape vs opacity alpha distinction\n   - Test alpha compositing formula: r = b + s - bs\n   - Test with various alpha values (0.0, 0.5, 1.0)\n   - Verify proper alpha premultiplication\n\n3. **Transparency Group Tests**:\n   - Test isolated groups (no backdrop contribution)\n   - Test non-isolated groups (backdrop shows through)\n   - Test knockout groups (objects knock out backdrop)\n   - Test nested transparency groups\n   - Verify group color space conversion\n   - Test groups with soft masks applied\n\n4. **Soft Mask Tests**:\n   - Test alpha soft masks (use alpha channel)\n   - Test luminosity soft masks (convert to grayscale)\n   - Test soft mask with transfer function\n   - Test backdrop color handling\n   - Verify mask clipping to group bounds\n\n5. **Integration Tests**:\n   - Test with PDF 1.4 specification examples (Annex G)\n   - Test transparency groups in Form XObjects\n   - Test transparency on different page backgrounds\n   - Test interaction with different color spaces\n   - Verify state save/restore preserves transparency settings\n\n6. **Performance Tests**:\n   - Benchmark compositing operations\n   - Test memory usage with complex transparency stacks\n   - Verify efficient buffer management\n   - Test with documents containing many transparent objects\n\n7. **Parser Tests**:\n   - Test ExtGState dictionary parsing\n   - Test Group dictionary parsing from Page/XObject\n   - Test handling of missing/invalid transparency parameters\n   - Verify backward compatibility with non-transparent PDFs\n\n8. **Edge Cases**:\n   - Test with invalid blend mode names\n   - Test alpha values outside [0,1] range\n   - Test empty transparency groups\n   - Test circular soft mask references\n   - Test state stack underflow/overflow",
        "status": "pending",
        "dependencies": [
          2,
          3,
          4,
          36
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 38,
        "title": "Implement Composite Font Support (Type 0 Fonts)",
        "description": "Add support for composite fonts including Type 0 fonts, CIDFont dictionaries, CMap character mappings, and proper ToUnicode mapping for accurate text extraction from CJK and complex scripts. This is essential for international document support.",
        "details": "Implement comprehensive composite font support for handling CJK and complex script documents according to PDF 1.4/1.7 specifications (Chapter 5.6):\n\n```go\n// internal/pdf/fonts/composite.go\npackage fonts\n\nimport (\n    \"bytes\"\n    \"fmt\"\n    \"io\"\n    \"unicode/utf16\"\n)\n\ntype Type0Font struct {\n    BaseFont     string\n    Encoding     Encoding\n    DescendantFont CIDFont\n    ToUnicode    *CMap\n}\n\ntype CIDFont struct {\n    Type         string // CIDFontType0 or CIDFontType2\n    BaseFont     string\n    CIDSystemInfo CIDSystemInfo\n    FontDescriptor *FontDescriptor\n    DW           float64 // Default width\n    W            []CIDWidth // Width array\n    DW2          [2]float64 // Default metrics for vertical writing\n    W2           []CIDMetrics // Vertical metrics array\n}\n\ntype CIDSystemInfo struct {\n    Registry   string\n    Ordering   string\n    Supplement int\n}\n\n// CMap parsing and character mapping\ntype CMap struct {\n    name         string\n    codeSpaceRanges []CodeSpaceRange\n    cidMappings  map[uint32]uint16 // Character code to CID\n    unicodeMappings map[uint32][]rune // Character code to Unicode\n    bfChars      map[uint32][]rune // Direct character mappings\n    bfRanges     []BFRange // Range mappings\n}\n\ntype CodeSpaceRange struct {\n    Low  []byte\n    High []byte\n}\n\ntype BFRange struct {\n    SrcLow  uint32\n    SrcHigh uint32\n    Dest    interface{} // Can be uint32 or []rune array\n}\n\n// Parse Type 0 font dictionary\nfunc ParseType0Font(dict map[string]interface{}, resolver ObjectResolver) (*Type0Font, error) {\n    font := &Type0Font{}\n    \n    // Extract BaseFont\n    if baseFont, ok := dict[\"BaseFont\"].(string); ok {\n        font.BaseFont = baseFont\n    }\n    \n    // Parse Encoding (CMap name or stream)\n    if encoding := dict[\"Encoding\"]; encoding != nil {\n        switch enc := encoding.(type) {\n        case string:\n            font.Encoding = &CMAPEncoding{Name: enc}\n        case *PDFStream:\n            cmap, err := ParseCMapStream(enc)\n            if err != nil {\n                return nil, fmt.Errorf(\"failed to parse CMap stream: %w\", err)\n            }\n            font.Encoding = &CMAPEncoding{CMap: cmap}\n        }\n    }\n    \n    // Parse DescendantFonts array\n    if descFonts, ok := dict[\"DescendantFonts\"].([]interface{}); ok && len(descFonts) > 0 {\n        if descDict, err := resolver.ResolveObject(descFonts[0]); err == nil {\n            cidFont, err := ParseCIDFont(descDict.(map[string]interface{}), resolver)\n            if err != nil {\n                return nil, fmt.Errorf(\"failed to parse CID font: %w\", err)\n            }\n            font.DescendantFont = cidFont\n        }\n    }\n    \n    // Parse ToUnicode CMap if present\n    if toUnicode := dict[\"ToUnicode\"]; toUnicode != nil {\n        if stream, err := resolver.ResolveStream(toUnicode); err == nil {\n            font.ToUnicode, _ = ParseCMapStream(stream)\n        }\n    }\n    \n    return font, nil\n}\n\n// CMap parser implementation\nfunc ParseCMapStream(stream *PDFStream) (*CMap, error) {\n    cmap := &CMap{\n        cidMappings: make(map[uint32]uint16),\n        unicodeMappings: make(map[uint32][]rune),\n        bfChars: make(map[uint32][]rune),\n    }\n    \n    data := stream.DecodedData()\n    parser := NewCMapParser(bytes.NewReader(data))\n    \n    for {\n        token, err := parser.NextToken()\n        if err == io.EOF {\n            break\n        }\n        if err != nil {\n            return nil, err\n        }\n        \n        switch token {\n        case \"begincodespacerange\":\n            count := parser.ReadInt()\n            for i := 0; i < count; i++ {\n                low := parser.ReadHexString()\n                high := parser.ReadHexString()\n                cmap.codeSpaceRanges = append(cmap.codeSpaceRanges, CodeSpaceRange{\n                    Low: low, High: high,\n                })\n            }\n            \n        case \"beginbfchar\":\n            count := parser.ReadInt()\n            for i := 0; i < count; i++ {\n                src := parser.ReadHexUint32()\n                dest := parser.ReadUnicodeString()\n                cmap.bfChars[src] = dest\n            }\n            \n        case \"beginbfrange\":\n            count := parser.ReadInt()\n            for i := 0; i < count; i++ {\n                srcLow := parser.ReadHexUint32()\n                srcHigh := parser.ReadHexUint32()\n                dest := parser.ReadDestination() // Can be hex string or array\n                \n                cmap.bfRanges = append(cmap.bfRanges, BFRange{\n                    SrcLow: srcLow,\n                    SrcHigh: srcHigh,\n                    Dest: dest,\n                })\n            }\n            \n        case \"begincidchar\":\n            count := parser.ReadInt()\n            for i := 0; i < count; i++ {\n                code := parser.ReadHexUint32()\n                cid := parser.ReadUint16()\n                cmap.cidMappings[code] = cid\n            }\n        }\n    }\n    \n    return cmap, nil\n}\n\n// Character decoding with CMap\nfunc (cmap *CMap) DecodeCharacter(code []byte) ([]rune, bool) {\n    codeValue := bytesToUint32(code)\n    \n    // Check direct mappings first\n    if unicode, ok := cmap.bfChars[codeValue]; ok {\n        return unicode, true\n    }\n    \n    // Check range mappings\n    for _, r := range cmap.bfRanges {\n        if codeValue >= r.SrcLow && codeValue <= r.SrcHigh {\n            offset := codeValue - r.SrcLow\n            \n            switch dest := r.Dest.(type) {\n            case uint32:\n                // Single value destination\n                return []rune{rune(dest + offset)}, true\n            case []rune:\n                // Array destination\n                if int(offset) < len(dest) {\n                    return []rune{dest[offset]}, true\n                }\n            }\n        }\n    }\n    \n    return nil, false\n}\n\n// CID to GID mapping for TrueType CIDFonts\ntype CIDToGIDMap interface {\n    MapCIDToGID(cid uint16) uint16\n}\n\ntype IdentityCIDToGIDMap struct{}\n\nfunc (m *IdentityCIDToGIDMap) MapCIDToGID(cid uint16) uint16 {\n    return cid\n}\n\ntype StreamCIDToGIDMap struct {\n    data []byte\n}\n\nfunc (m *StreamCIDToGIDMap) MapCIDToGID(cid uint16) uint16 {\n    if int(cid)*2+1 < len(m.data) {\n        return uint16(m.data[cid*2])<<8 | uint16(m.data[cid*2+1])\n    }\n    return 0\n}\n\n// Width calculation for CID fonts\nfunc (f *CIDFont) GetWidth(cid uint16) float64 {\n    // Check W array for specific widths\n    for i := 0; i < len(f.W); i++ {\n        w := f.W[i]\n        if w.Type == CIDWidthRange && cid >= w.First && cid <= w.Last {\n            return w.Width\n        } else if w.Type == CIDWidthArray && cid >= w.First && int(cid-w.First) < len(w.Widths) {\n            return w.Widths[cid-w.First]\n        }\n    }\n    \n    // Return default width\n    return f.DW\n}\n\n// Integration with text extraction\ntype CompositeTextExtractor struct {\n    font *Type0Font\n    scale float64\n}\n\nfunc (e *CompositeTextExtractor) ExtractText(data []byte) (string, []PositionedGlyph, error) {\n    var result strings.Builder\n    var glyphs []PositionedGlyph\n    \n    reader := bytes.NewReader(data)\n    \n    for reader.Len() > 0 {\n        // Read character code based on CMap code space\n        code := e.readCharacterCode(reader)\n        if code == nil {\n            break\n        }\n        \n        // Convert to Unicode using ToUnicode CMap\n        var text []rune\n        if e.font.ToUnicode != nil {\n            text, _ = e.font.ToUnicode.DecodeCharacter(code)\n        }\n        \n        // Fallback to predefined CMaps for CJK\n        if len(text) == 0 {\n            text = e.fallbackDecode(code)\n        }\n        \n        // Get CID for width calculation\n        cid := e.getCID(code)\n        width := e.font.DescendantFont.GetWidth(cid) * e.scale / 1000.0\n        \n        if len(text) > 0 {\n            result.WriteString(string(text))\n            glyphs = append(glyphs, PositionedGlyph{\n                Text: string(text),\n                Width: width,\n                CID: cid,\n            })\n        }\n    }\n    \n    return result.String(), glyphs, nil\n}\n\n// Predefined CMap support for standard CJK encodings\nvar predefinedCMaps = map[string]func() *CMap{\n    \"UniGB-UCS2-H\": loadUniGBUCS2H,\n    \"UniCNS-UCS2-H\": loadUniCNSUCS2H,\n    \"UniJIS-UCS2-H\": loadUniJISUCS2H,\n    \"UniKS-UCS2-H\": loadUniKSUCS2H,\n    \"Identity-H\": loadIdentityH,\n    \"Identity-V\": loadIdentityV,\n}\n\nfunc loadPredefinedCMap(name string) (*CMap, error) {\n    if loader, ok := predefinedCMaps[name]; ok {\n        return loader(), nil\n    }\n    return nil, fmt.Errorf(\"unknown predefined CMap: %s\", name)\n}",
        "testStrategy": "Comprehensive testing strategy for composite font support:\n\n1. **Type 0 Font Parsing Tests**:\n   - Test parsing Type 0 font dictionaries with CIDFontType0 and CIDFontType2 descendants\n   - Verify correct extraction of BaseFont, Encoding, and DescendantFonts\n   - Test with embedded and external CMap references\n   - Validate CIDSystemInfo parsing (Registry-Ordering-Supplement)\n\n2. **CMap Parsing Tests**:\n   - Test parsing of ToUnicode CMaps with beginbfchar and beginbfrange mappings\n   - Verify correct parsing of codespacerange definitions\n   - Test with multi-byte character codes (2-byte, 3-byte)\n   - Validate handling of array destinations in bfrange mappings\n   - Test CID mappings (begincidchar, begincidrange)\n\n3. **Character Decoding Tests**:\n   - Test decoding of CJK characters using standard CMaps (UniGB-UCS2-H, UniJIS-UCS2-H, etc.)\n   - Verify correct Unicode mapping for common Chinese, Japanese, and Korean characters\n   - Test Identity-H and Identity-V encodings\n   - Validate fallback mechanisms when ToUnicode is missing\n\n4. **Width Calculation Tests**:\n   - Test CID width lookups with W array containing ranges and individual values\n   - Verify default width (DW) is used when CID not in W array\n   - Test vertical writing metrics (DW2, W2) for vertical text\n\n5. **Integration Tests**:\n   - Test with real CJK PDFs (Chinese GB18030, Japanese Shift-JIS, Korean KSC5601)\n   - Extract text from documents with mixed Latin and CJK content\n   - Verify correct positioning and spacing of CJK characters\n   - Test with PDFs from different sources (Adobe, MS Office, LibreOffice)\n\n6. **Edge Cases**:\n   - Test with malformed CMap data\n   - Handle missing DescendantFonts array\n   - Test with custom/embedded CMaps\n   - Verify handling of surrogate pairs for Unicode beyond BMP\n\n7. **Performance Tests**:\n   - Benchmark CMap parsing for large ToUnicode mappings (10,000+ entries)\n   - Test memory usage with complex CJK documents\n   - Verify efficient character code reading for variable-length codes",
        "status": "pending",
        "dependencies": [
          3,
          4
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 39,
        "title": "Implement Comprehensive Annotation Support",
        "description": "Add support for all standard PDF annotation types including Text, Link, FreeText, Line, Square, Circle, Polygon, Highlight, Underline, Squiggly, StrikeOut, Stamp, Caret, Ink, Popup, FileAttachment, Sound, Movie, Widget, and 3D annotations with proper rendering and interaction.",
        "details": "Implement a comprehensive annotation system that supports all PDF annotation types according to PDF 1.4/1.7 specifications (Chapter 12.5):\n\n```go\n// internal/pdf/annotations/types.go\npackage annotations\n\nimport (\n    \"time\"\n    \"github.com/yourusername/pdfextract/internal/pdf/appearance\"\n    \"github.com/yourusername/pdfextract/internal/pdf/actions\"\n)\n\ntype AnnotationType string\n\nconst (\n    TypeText           AnnotationType = \"Text\"\n    TypeLink           AnnotationType = \"Link\"\n    TypeFreeText       AnnotationType = \"FreeText\"\n    TypeLine           AnnotationType = \"Line\"\n    TypeSquare         AnnotationType = \"Square\"\n    TypeCircle         AnnotationType = \"Circle\"\n    TypePolygon        AnnotationType = \"Polygon\"\n    TypePolyLine       AnnotationType = \"PolyLine\"\n    TypeHighlight      AnnotationType = \"Highlight\"\n    TypeUnderline      AnnotationType = \"Underline\"\n    TypeSquiggly       AnnotationType = \"Squiggly\"\n    TypeStrikeOut      AnnotationType = \"StrikeOut\"\n    TypeStamp          AnnotationType = \"Stamp\"\n    TypeCaret          AnnotationType = \"Caret\"\n    TypeInk            AnnotationType = \"Ink\"\n    TypePopup          AnnotationType = \"Popup\"\n    TypeFileAttachment AnnotationType = \"FileAttachment\"\n    TypeSound          AnnotationType = \"Sound\"\n    TypeMovie          AnnotationType = \"Movie\"\n    TypeWidget         AnnotationType = \"Widget\"\n    Type3D             AnnotationType = \"3D\"\n)\n\n// Base annotation interface\ntype Annotation interface {\n    Type() AnnotationType\n    GetRect() Rectangle\n    GetAppearance() *appearance.AppearanceStream\n    GetFlags() AnnotationFlags\n    GetModificationDate() time.Time\n    Render(context *RenderContext) error\n}\n\n// Common annotation properties\ntype BaseAnnotation struct {\n    Subtype   AnnotationType        `json:\"subtype\"`\n    Rect      Rectangle            `json:\"rect\"`\n    Contents  string               `json:\"contents,omitempty\"`\n    P         *PageReference       `json:\"page,omitempty\"`\n    NM        string               `json:\"name,omitempty\"`\n    M         time.Time            `json:\"modDate,omitempty\"`\n    F         AnnotationFlags      `json:\"flags\"`\n    AP        *AppearanceDictionary `json:\"appearance,omitempty\"`\n    AS        string               `json:\"appearanceState,omitempty\"`\n    Border    []float64            `json:\"border,omitempty\"`\n    C         []float64            `json:\"color,omitempty\"`\n    StructParent int               `json:\"structParent,omitempty\"`\n}\n\n// Specific annotation types\ntype TextAnnotation struct {\n    BaseAnnotation\n    Open     bool   `json:\"open\"`\n    Name     string `json:\"iconName\"` // Comment, Key, Note, Help, etc.\n    State    string `json:\"state,omitempty\"`\n    StateModel string `json:\"stateModel,omitempty\"`\n}\n\ntype LinkAnnotation struct {\n    BaseAnnotation\n    A           *actions.Action     `json:\"action,omitempty\"`\n    Dest        Destination         `json:\"destination,omitempty\"`\n    H           HighlightMode       `json:\"highlightMode,omitempty\"`\n    PA          *actions.URIAction  `json:\"previousAction,omitempty\"`\n    QuadPoints  []float64          `json:\"quadPoints,omitempty\"`\n}\n\ntype FreeTextAnnotation struct {\n    BaseAnnotation\n    DA    string              `json:\"defaultAppearance\"`\n    Q     int                 `json:\"quadding\"`\n    RC    string              `json:\"richText,omitempty\"`\n    DS    string              `json:\"defaultStyle,omitempty\"`\n    CL    []float64          `json:\"calloutLine,omitempty\"`\n    IT    FreeTextIntent     `json:\"intent,omitempty\"`\n    BE    *BorderEffect      `json:\"borderEffect,omitempty\"`\n    RD    []float64          `json:\"rectDifferences,omitempty\"`\n    BS    *BorderStyle       `json:\"borderStyle,omitempty\"`\n    LE    LineEnding         `json:\"lineEnding,omitempty\"`\n}\n\n// Markup annotations (Highlight, Underline, etc.)\ntype MarkupAnnotation struct {\n    BaseAnnotation\n    QuadPoints []float64 `json:\"quadPoints\"`\n    MarkupType AnnotationType `json:\"markupType\"`\n}\n\n// Geometric annotations\ntype LineAnnotation struct {\n    BaseAnnotation\n    L       []float64      `json:\"line\"` // [x1, y1, x2, y2]\n    BS      *BorderStyle   `json:\"borderStyle,omitempty\"`\n    LE      []LineEnding   `json:\"lineEndings,omitempty\"`\n    IC      []float64      `json:\"interiorColor,omitempty\"`\n    LL      float64        `json:\"leaderLine,omitempty\"`\n    LLE     float64        `json:\"leaderLineExtension,omitempty\"`\n    Cap     bool           `json:\"caption,omitempty\"`\n    IT      LineIntent     `json:\"intent,omitempty\"`\n    LLO     float64        `json:\"leaderLineOffset,omitempty\"`\n    CP      CaptionPosition `json:\"captionPosition,omitempty\"`\n    Measure *MeasureDict   `json:\"measure,omitempty\"`\n    CO      []float64      `json:\"captionOffset,omitempty\"`\n}\n\n// internal/pdf/annotations/parser.go\ntype AnnotationParser struct {\n    resolver ObjectResolver\n    pageRef  *PageReference\n}\n\nfunc (p *AnnotationParser) ParseAnnotation(dict map[string]Object) (Annotation, error) {\n    subtype, ok := dict[\"Subtype\"].(Name)\n    if !ok {\n        return nil, fmt.Errorf(\"missing annotation subtype\")\n    }\n    \n    base := p.parseBaseAnnotation(dict)\n    \n    switch AnnotationType(subtype) {\n    case TypeText:\n        return p.parseTextAnnotation(base, dict)\n    case TypeLink:\n        return p.parseLinkAnnotation(base, dict)\n    case TypeFreeText:\n        return p.parseFreeTextAnnotation(base, dict)\n    case TypeLine:\n        return p.parseLineAnnotation(base, dict)\n    case TypeSquare, TypeCircle:\n        return p.parseShapeAnnotation(base, dict, AnnotationType(subtype))\n    case TypePolygon, TypePolyLine:\n        return p.parsePolyAnnotation(base, dict, AnnotationType(subtype))\n    case TypeHighlight, TypeUnderline, TypeSquiggly, TypeStrikeOut:\n        return p.parseMarkupAnnotation(base, dict, AnnotationType(subtype))\n    case TypeInk:\n        return p.parseInkAnnotation(base, dict)\n    case TypeStamp:\n        return p.parseStampAnnotation(base, dict)\n    case TypeWidget:\n        return p.parseWidgetAnnotation(base, dict)\n    case TypeFileAttachment:\n        return p.parseFileAttachmentAnnotation(base, dict)\n    case TypeSound:\n        return p.parseSoundAnnotation(base, dict)\n    case TypeMovie:\n        return p.parseMovieAnnotation(base, dict)\n    case Type3D:\n        return p.parse3DAnnotation(base, dict)\n    default:\n        return nil, fmt.Errorf(\"unsupported annotation type: %s\", subtype)\n    }\n}\n\n// internal/pdf/annotations/renderer.go\ntype AnnotationRenderer struct {\n    colorSpace   color.ColorSpace\n    fontManager  *FontManager\n    mediaHandler *MediaHandler\n}\n\nfunc (r *AnnotationRenderer) RenderAnnotation(annot Annotation, ctx *RenderContext) error {\n    // Check if annotation should be visible\n    if !r.shouldRender(annot, ctx) {\n        return nil\n    }\n    \n    // Use appearance stream if available\n    if ap := annot.GetAppearance(); ap != nil {\n        return r.renderAppearanceStream(ap, ctx)\n    }\n    \n    // Otherwise render based on annotation type\n    switch a := annot.(type) {\n    case *TextAnnotation:\n        return r.renderTextAnnotation(a, ctx)\n    case *LinkAnnotation:\n        return r.renderLinkAnnotation(a, ctx)\n    case *MarkupAnnotation:\n        return r.renderMarkupAnnotation(a, ctx)\n    case *LineAnnotation:\n        return r.renderLineAnnotation(a, ctx)\n    case *InkAnnotation:\n        return r.renderInkAnnotation(a, ctx)\n    // ... handle all annotation types\n    }\n}\n\n// internal/pdf/annotations/interaction.go\ntype InteractionHandler struct {\n    annotations []Annotation\n    actions     *actions.ActionHandler\n}\n\nfunc (h *InteractionHandler) HandleClick(x, y float64) error {\n    // Find annotation at position\n    annot := h.findAnnotationAt(x, y)\n    if annot == nil {\n        return nil\n    }\n    \n    switch a := annot.(type) {\n    case *LinkAnnotation:\n        if a.A != nil {\n            return h.actions.ExecuteAction(a.A)\n        } else if a.Dest != nil {\n            return h.navigateToDestination(a.Dest)\n        }\n    case *TextAnnotation:\n        // Toggle open/closed state\n        a.Open = !a.Open\n        return h.updateAnnotation(a)\n    case *FileAttachmentAnnotation:\n        return h.extractAttachment(a)\n    case *SoundAnnotation:\n        return h.playSound(a)\n    case *MovieAnnotation:\n        return h.playMovie(a)\n    }\n    return nil\n}\n\n// internal/pdf/annotations/appearance.go\ntype AppearanceGenerator struct {\n    fontManager *FontManager\n    graphics    *GraphicsRenderer\n}\n\nfunc (g *AppearanceGenerator) GenerateAppearance(annot Annotation) (*AppearanceStream, error) {\n    switch a := annot.(type) {\n    case *FreeTextAnnotation:\n        return g.generateFreeTextAppearance(a)\n    case *StampAnnotation:\n        return g.generateStampAppearance(a)\n    case *InkAnnotation:\n        return g.generateInkAppearance(a)\n    default:\n        return g.generateDefaultAppearance(annot)\n    }\n}\n```\n\nKey implementation considerations:\n1. **Annotation Parsing**: Parse all annotation dictionaries from page Annots array\n2. **Appearance Streams**: Handle Normal, Rollover, and Down appearance states\n3. **Interactive Features**: Implement click handling, hover effects, and state changes\n4. **Rendering Order**: Respect annotation Z-order and page rotation\n5. **Coordinate Systems**: Transform between annotation, page, and device coordinates\n6. **Form Integration**: Special handling for Widget annotations linked to form fields\n7. **Media Handling**: Support for embedded sound, movie, and 3D content\n8. **Action Execution**: Handle annotation actions (GoTo, URI, JavaScript, etc.)\n9. **Popup Management**: Associate popup annotations with their parent annotations\n10. **Appearance Generation**: Create default appearances when AP dict is missing",
        "testStrategy": "Comprehensive testing strategy for annotation support:\n\n1. **Basic Annotation Parsing Tests**:\n   - Test parsing of all 22 annotation types from sample PDFs\n   - Verify correct extraction of common properties (Rect, Contents, Flags)\n   - Test with missing optional properties\n   - Validate annotation flag interpretation (Hidden, Print, NoView, etc.)\n\n2. **Appearance Stream Tests**:\n   - Test annotations with Normal, Rollover, and Down appearances\n   - Verify correct appearance state selection\n   - Test appearance generation for annotations without AP dict\n   - Validate form XObject rendering within appearances\n\n3. **Markup Annotation Tests**:\n   - Test Highlight, Underline, Squiggly, StrikeOut with QuadPoints\n   - Verify correct text coverage calculation\n   - Test with rotated pages and transformed text\n   - Validate color and opacity handling\n\n4. **Interactive Annotation Tests**:\n   - Test Link annotations with various action types\n   - Verify destination navigation (page, named, XYZ)\n   - Test Text annotation open/close toggling\n   - Validate FileAttachment extraction\n   - Test Sound and Movie playback triggers\n\n5. **Geometric Annotation Tests**:\n   - Test Line, Square, Circle, Polygon rendering\n   - Verify border styles (solid, dashed, beveled, inset, underline)\n   - Test line endings (Square, Circle, Diamond, OpenArrow, etc.)\n   - Validate interior color filling\n\n6. **FreeText Annotation Tests**:\n   - Test text rendering with various fonts and sizes\n   - Verify text alignment (left, center, right, justified)\n   - Test callout lines for different intents\n   - Validate rich text formatting\n\n7. **Widget Annotation Tests**:\n   - Test integration with form fields (text, button, choice, signature)\n   - Verify appearance generation for field values\n   - Test interactive form filling\n   - Validate field validation and formatting\n\n8. **Coordinate Transform Tests**:\n   - Test annotation rendering with page rotation (0, 90, 180, 270)\n   - Verify correct positioning after page cropping\n   - Test with various page boxes (MediaBox, CropBox, etc.)\n   - Validate mouse hit testing accuracy\n\n9. **Performance Tests**:\n   - Benchmark rendering 1000+ annotations per page\n   - Test memory usage with complex appearance streams\n   - Verify efficient hit testing for interaction\n   - Profile appearance generation performance\n\n10. **Edge Case Tests**:\n    - Test circular references in popup associations\n    - Handle malformed annotation dictionaries\n    - Test with encrypted PDFs\n    - Verify handling of unknown annotation types",
        "status": "pending",
        "dependencies": [
          2,
          3,
          4,
          12,
          34,
          36
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 40,
        "title": "Implement PDF Actions Support",
        "description": "Add support for all standard PDF actions including GoTo, GoToR (remote), Launch, URI, Named, SubmitForm, ResetForm, ImportData, JavaScript, SetOCGState, Rendition, Trans, and GoTo3DView actions. Include proper destination handling and security validation.",
        "details": "Implement a comprehensive PDF actions system that supports all standard action types according to PDF 1.4/1.7 specifications (Chapter 12.6):\n\n```go\n// internal/pdf/actions/types.go\npackage actions\n\nimport (\n    \"net/url\"\n    \"github.com/yourusername/pdfextract/internal/pdf/destinations\"\n    \"github.com/yourusername/pdfextract/internal/pdf/security\"\n)\n\ntype ActionType string\n\nconst (\n    TypeGoTo       ActionType = \"GoTo\"       // Go to destination in current document\n    TypeGoToR      ActionType = \"GoToR\"      // Go to destination in another document\n    TypeLaunch     ActionType = \"Launch\"     // Launch application\n    TypeURI        ActionType = \"URI\"        // Resolve URI\n    TypeNamed      ActionType = \"Named\"      // Execute named action\n    TypeSubmitForm ActionType = \"SubmitForm\" // Submit form data\n    TypeResetForm  ActionType = \"ResetForm\"  // Reset form fields\n    TypeImportData ActionType = \"ImportData\" // Import form data\n    TypeJavaScript ActionType = \"JavaScript\" // Execute JavaScript\n    TypeSetOCGState ActionType = \"SetOCGState\" // Set optional content state\n    TypeRendition  ActionType = \"Rendition\"  // Control multimedia content\n    TypeTrans      ActionType = \"Trans\"      // Set page transition\n    TypeGoTo3DView ActionType = \"GoTo3DView\" // Set 3D view\n)\n\n// Base action interface\ntype Action interface {\n    Type() ActionType\n    Validate() error\n    Execute(context *ActionContext) error\n    GetNext() []Action // Action chains\n}\n\n// Action context for execution\ntype ActionContext struct {\n    Document     *PDFDocument\n    CurrentPage  int\n    Security     *security.SecurityHandler\n    FormData     map[string]interface{}\n    AllowExternal bool // Security flag for external actions\n}\n\n// internal/pdf/actions/goto.go\ntype GoToAction struct {\n    Destination destinations.Destination\n    Next        []Action\n}\n\nfunc (a *GoToAction) Type() ActionType {\n    return TypeGoTo\n}\n\nfunc (a *GoToAction) Validate() error {\n    if a.Destination == nil {\n        return fmt.Errorf(\"GoTo action requires destination\")\n    }\n    return a.Destination.Validate()\n}\n\nfunc (a *GoToAction) Execute(ctx *ActionContext) error {\n    // Navigate to destination within current document\n    pageNum, view := a.Destination.GetTarget()\n    if pageNum < 0 || pageNum >= ctx.Document.PageCount() {\n        return fmt.Errorf(\"invalid page number: %d\", pageNum)\n    }\n    // Implementation would trigger navigation\n    return nil\n}\n\n// internal/pdf/actions/gotor.go\ntype GoToRAction struct {\n    File        string                   // File specification\n    Destination destinations.Destination // Destination in remote file\n    NewWindow   bool                     // Open in new window\n    Next        []Action\n}\n\nfunc (a *GoToRAction) Validate() error {\n    if !a.ctx.AllowExternal {\n        return fmt.Errorf(\"external file access not allowed\")\n    }\n    // Validate file specification\n    if a.File == \"\" {\n        return fmt.Errorf(\"GoToR requires file specification\")\n    }\n    // Security check for file path\n    if err := validateFilePath(a.File); err != nil {\n        return fmt.Errorf(\"invalid file path: %w\", err)\n    }\n    return nil\n}\n\n// internal/pdf/actions/uri.go\ntype URIAction struct {\n    URI      string\n    IsMap    bool // Server-side image map\n    Next     []Action\n}\n\nfunc (a *URIAction) Validate() error {\n    parsedURL, err := url.Parse(a.URI)\n    if err != nil {\n        return fmt.Errorf(\"invalid URI: %w\", err)\n    }\n    // Security validation\n    if !isAllowedScheme(parsedURL.Scheme) {\n        return fmt.Errorf(\"disallowed URI scheme: %s\", parsedURL.Scheme)\n    }\n    return nil\n}\n\n// internal/pdf/actions/form.go\ntype SubmitFormAction struct {\n    URL          string\n    Fields       []string // Field names to submit\n    Flags        SubmitFlags\n    CharSet      string\n    Next         []Action\n}\n\ntype SubmitFlags uint32\n\nconst (\n    SubmitIncludeNoValue SubmitFlags = 1 << 0\n    SubmitExportFormat   SubmitFlags = 1 << 1\n    SubmitGetMethod      SubmitFlags = 1 << 2\n    SubmitCoordinates    SubmitFlags = 1 << 3\n    // ... other flags\n)\n\ntype ResetFormAction struct {\n    Fields []string // Field names to reset (empty = all)\n    Flags  uint32\n    Next   []Action\n}\n\n// internal/pdf/actions/javascript.go\ntype JavaScriptAction struct {\n    Script string\n    Next   []Action\n}\n\nfunc (a *JavaScriptAction) Validate() error {\n    // Basic JavaScript validation\n    if containsDangerousPatterns(a.Script) {\n        return fmt.Errorf(\"potentially dangerous JavaScript detected\")\n    }\n    return nil\n}\n\nfunc (a *JavaScriptAction) Execute(ctx *ActionContext) error {\n    if !ctx.AllowJavaScript {\n        return fmt.Errorf(\"JavaScript execution not allowed\")\n    }\n    // Would integrate with JavaScript engine\n    return fmt.Errorf(\"JavaScript execution not implemented\")\n}\n\n// internal/pdf/actions/parser.go\ntype ActionParser struct {\n    resolver ObjectResolver\n}\n\nfunc (p *ActionParser) ParseAction(dict map[string]interface{}) (Action, error) {\n    typeStr, ok := dict[\"Type\"].(string)\n    if !ok || typeStr != \"Action\" {\n        return nil, fmt.Errorf(\"invalid action dictionary\")\n    }\n    \n    subtype, ok := dict[\"S\"].(string)\n    if !ok {\n        return nil, fmt.Errorf(\"action missing subtype\")\n    }\n    \n    var action Action\n    var err error\n    \n    switch ActionType(subtype) {\n    case TypeGoTo:\n        action, err = p.parseGoToAction(dict)\n    case TypeGoToR:\n        action, err = p.parseGoToRAction(dict)\n    case TypeLaunch:\n        action, err = p.parseLaunchAction(dict)\n    case TypeURI:\n        action, err = p.parseURIAction(dict)\n    case TypeNamed:\n        action, err = p.parseNamedAction(dict)\n    case TypeSubmitForm:\n        action, err = p.parseSubmitFormAction(dict)\n    case TypeResetForm:\n        action, err = p.parseResetFormAction(dict)\n    case TypeImportData:\n        action, err = p.parseImportDataAction(dict)\n    case TypeJavaScript:\n        action, err = p.parseJavaScriptAction(dict)\n    case TypeSetOCGState:\n        action, err = p.parseSetOCGStateAction(dict)\n    case TypeRendition:\n        action, err = p.parseRenditionAction(dict)\n    case TypeTrans:\n        action, err = p.parseTransAction(dict)\n    case TypeGoTo3DView:\n        action, err = p.parseGoTo3DViewAction(dict)\n    default:\n        return nil, fmt.Errorf(\"unsupported action type: %s\", subtype)\n    }\n    \n    if err != nil {\n        return nil, err\n    }\n    \n    // Parse Next actions (action chains)\n    if next, ok := dict[\"Next\"]; ok {\n        nextActions, err := p.parseNextActions(next)\n        if err != nil {\n            return nil, fmt.Errorf(\"failed to parse Next actions: %w\", err)\n        }\n        action.SetNext(nextActions)\n    }\n    \n    return action, nil\n}\n\n// internal/pdf/actions/destinations.go\npackage destinations\n\ntype Destination interface {\n    GetTarget() (pageNum int, view View)\n    Validate() error\n}\n\ntype View interface {\n    Type() string\n    Parameters() []float64\n}\n\ntype XYZView struct {\n    Left, Top, Zoom float64\n}\n\ntype FitView struct{}\n\ntype FitHView struct {\n    Top float64\n}\n\n// internal/pdf/actions/security.go\nfunc validateFilePath(path string) error {\n    // Prevent directory traversal\n    if strings.Contains(path, \"..\") {\n        return fmt.Errorf(\"directory traversal detected\")\n    }\n    // Check for absolute paths\n    if filepath.IsAbs(path) {\n        return fmt.Errorf(\"absolute paths not allowed\")\n    }\n    return nil\n}\n\nfunc isAllowedScheme(scheme string) bool {\n    allowed := []string{\"http\", \"https\", \"mailto\", \"ftp\"}\n    for _, s := range allowed {\n        if strings.EqualFold(scheme, s) {\n            return true\n        }\n    }\n    return false\n}\n\nfunc containsDangerousPatterns(script string) bool {\n    dangerous := []string{\n        \"eval(\",\n        \"Function(\",\n        \"setTimeout(\",\n        \"setInterval(\",\n        \".constructor(\",\n    }\n    scriptLower := strings.ToLower(script)\n    for _, pattern := range dangerous {\n        if strings.Contains(scriptLower, strings.ToLower(pattern)) {\n            return true\n        }\n    }\n    return false\n}\n\n// internal/pdf/actions/executor.go\ntype ActionExecutor struct {\n    securityHandler *security.SecurityHandler\n    allowExternal   bool\n    allowJavaScript bool\n}\n\nfunc (e *ActionExecutor) ExecuteAction(action Action, doc *PDFDocument, currentPage int) error {\n    ctx := &ActionContext{\n        Document:      doc,\n        CurrentPage:   currentPage,\n        Security:      e.securityHandler,\n        AllowExternal: e.allowExternal,\n    }\n    \n    // Validate action before execution\n    if err := action.Validate(); err != nil {\n        return fmt.Errorf(\"action validation failed: %w\", err)\n    }\n    \n    // Execute action\n    if err := action.Execute(ctx); err != nil {\n        return fmt.Errorf(\"action execution failed: %w\", err)\n    }\n    \n    // Execute chained actions\n    for _, next := range action.GetNext() {\n        if err := e.ExecuteAction(next, doc, currentPage); err != nil {\n            return fmt.Errorf(\"next action failed: %w\", err)\n        }\n    }\n    \n    return nil\n}\n```\n\nKey implementation considerations:\n\n1. **Security validation**: Implement strict validation for all external actions (GoToR, Launch, URI)\n2. **Destination handling**: Proper parsing and validation of all destination types\n3. **Action chains**: Support Next field for sequential action execution\n4. **Form actions**: Complete support for form submission and reset with proper field handling\n5. **JavaScript sandboxing**: Basic validation to prevent dangerous operations\n6. **Optional content**: Support for SetOCGState to control layer visibility\n7. **3D content**: Basic support for 3D view actions\n8. **Error handling**: Comprehensive error reporting for invalid actions",
        "testStrategy": "Comprehensive testing strategy for PDF actions support:\n\n1. **Basic Action Parsing Tests**:\n   - Test parsing of all 13 action types from sample PDFs\n   - Verify correct extraction of action parameters\n   - Test with missing required fields (should fail)\n   - Test with invalid action dictionaries\n   - Validate action type detection\n\n2. **GoTo Action Tests**:\n   - Test with various destination types (XYZ, Fit, FitH, etc.)\n   - Test with named destinations\n   - Test with explicit destinations\n   - Verify page number validation\n   - Test with out-of-range page numbers\n\n3. **GoToR (Remote) Action Tests**:\n   - Test with relative file paths\n   - Test with absolute paths (should fail with security check)\n   - Test directory traversal attempts (should fail)\n   - Test with various destination types in remote file\n   - Verify NewWindow flag handling\n\n4. **URI Action Tests**:\n   - Test with HTTP/HTTPS URLs\n   - Test with mailto: links\n   - Test with dangerous schemes (javascript:, file:) - should fail\n   - Test URL encoding and special characters\n   - Test IsMap flag for image maps\n\n5. **Form Action Tests**:\n   - Test SubmitForm with various flag combinations\n   - Test field inclusion/exclusion lists\n   - Test GET vs POST method flags\n   - Test ResetForm with specific fields\n   - Test ImportData action parsing\n\n6. **JavaScript Action Tests**:\n   - Test basic JavaScript parsing\n   - Test dangerous pattern detection (eval, Function constructor)\n   - Test with obfuscated dangerous patterns\n   - Verify script extraction accuracy\n   - Test execution context security\n\n7. **Action Chain Tests**:\n   - Test single Next action\n   - Test array of Next actions\n   - Test deeply nested action chains\n   - Test circular references (should handle gracefully)\n   - Verify execution order\n\n8. **Security Validation Tests**:\n   - Test all external actions with security disabled\n   - Test file path validation for GoToR and Launch\n   - Test URI scheme whitelisting\n   - Test JavaScript sandboxing\n   - Verify permission checking\n\n9. **Integration Tests**:\n   - Test actions within annotations (Link annotations)\n   - Test form field actions (button actions)\n   - Test page open/close actions\n   - Test document-level actions\n   - Test with encrypted PDFs\n\n10. **Error Handling Tests**:\n    - Test with malformed action dictionaries\n    - Test with circular action references\n    - Test with missing required fields\n    - Test with invalid destination references\n    - Verify graceful error recovery\n\n11. **Performance Tests**:\n    - Test parsing of documents with many actions\n    - Measure action validation overhead\n    - Test with complex action chains\n    - Verify memory usage with large action sets\n\n12. **Compliance Tests**:\n    - Test against PDF 1.4 specification examples\n    - Test against PDF 1.7 specification examples\n    - Verify all action types from ISO 32000-1\n    - Test with real-world PDFs containing actions",
        "status": "pending",
        "dependencies": [
          2,
          3,
          34,
          35,
          39
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 41,
        "title": "Implement Document Outline/Bookmarks Support",
        "description": "Add support for parsing and navigating PDF document outlines (bookmarks) with proper hierarchy, destinations, and actions. Include support for both explicit and named destinations.",
        "details": "Implement a comprehensive document outline/bookmarks system that supports the full PDF outline hierarchy according to PDF 1.4/1.7 specifications (Chapter 12.3):\n\n```go\n// internal/pdf/outlines/parser.go\npackage outlines\n\nimport (\n    \"fmt\"\n    \"github.com/yourusername/pdfextract/internal/pdf/destinations\"\n    \"github.com/yourusername/pdfextract/internal/pdf/actions\"\n)\n\ntype OutlineParser struct {\n    resolver     ObjectResolver\n    destResolver destinations.DestinationResolver\n    actionParser actions.ActionParser\n}\n\ntype OutlineItem struct {\n    Title       string           `json:\"title\"`\n    Dest        *Destination     `json:\"dest,omitempty\"`\n    Action      *actions.Action  `json:\"action,omitempty\"`\n    Parent      *OutlineItem     `json:\"-\"`\n    First       *OutlineItem     `json:\"first,omitempty\"`\n    Last        *OutlineItem     `json:\"last,omitempty\"`\n    Next        *OutlineItem     `json:\"next,omitempty\"`\n    Prev        *OutlineItem     `json:\"-\"`\n    Count       int              `json:\"count\"`\n    SE          *StructElement   `json:\"se,omitempty\"`\n    C           []float64        `json:\"color,omitempty\"`\n    F           int              `json:\"flags,omitempty\"`\n    Children    []*OutlineItem   `json:\"children,omitempty\"`\n}\n\ntype Destination struct {\n    Type        DestinationType  `json:\"type\"`\n    Page        int              `json:\"page\"`\n    View        string           `json:\"view\"` // Fit, FitH, FitV, FitR, FitB, FitBH, FitBV, XYZ\n    Parameters  []float64        `json:\"parameters,omitempty\"`\n    NamedDest   string           `json:\"namedDest,omitempty\"`\n}\n\ntype DestinationType int\n\nconst (\n    ExplicitDest DestinationType = iota\n    NamedDest\n    RemoteDest\n)\n\nfunc (p *OutlineParser) ParseOutlines(catalog map[string]interface{}) (*OutlineItem, error) {\n    outlinesRef, ok := catalog[\"Outlines\"]\n    if !ok {\n        return nil, nil // No outlines in document\n    }\n    \n    outlinesDict, err := p.resolver.ResolveObject(outlinesRef)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to resolve outlines: %w\", err)\n    }\n    \n    // Parse the outline tree starting from First\n    firstRef, ok := outlinesDict[\"First\"]\n    if !ok {\n        return nil, nil // Empty outline\n    }\n    \n    root := &OutlineItem{Title: \"Document Outline\"}\n    err = p.parseOutlineTree(firstRef, root, 0)\n    if err != nil {\n        return nil, err\n    }\n    \n    return root, nil\n}\n\nfunc (p *OutlineParser) parseOutlineTree(itemRef interface{}, parent *OutlineItem, depth int) error {\n    if depth > 100 { // Prevent infinite recursion\n        return fmt.Errorf(\"outline depth exceeded maximum\")\n    }\n    \n    item, err := p.parseOutlineItem(itemRef)\n    if err != nil {\n        return err\n    }\n    \n    item.Parent = parent\n    if parent.Children == nil {\n        parent.Children = make([]*OutlineItem, 0)\n    }\n    parent.Children = append(parent.Children, item)\n    \n    // Parse children if present\n    if item.First != nil {\n        err = p.parseOutlineTree(item.First, item, depth+1)\n        if err != nil {\n            return err\n        }\n    }\n    \n    // Parse siblings\n    if item.Next != nil {\n        err = p.parseOutlineTree(item.Next, parent, depth)\n        if err != nil {\n            return err\n        }\n    }\n    \n    return nil\n}\n\nfunc (p *OutlineParser) parseOutlineItem(itemRef interface{}) (*OutlineItem, error) {\n    itemDict, err := p.resolver.ResolveObject(itemRef)\n    if err != nil {\n        return nil, err\n    }\n    \n    item := &OutlineItem{}\n    \n    // Extract title (required)\n    title, ok := itemDict[\"Title\"].(string)\n    if !ok {\n        return nil, fmt.Errorf(\"outline item missing required Title\")\n    }\n    item.Title = title\n    \n    // Extract destination or action\n    if dest, ok := itemDict[\"Dest\"]; ok {\n        item.Dest, err = p.parseDestination(dest)\n        if err != nil {\n            return nil, fmt.Errorf(\"failed to parse destination: %w\", err)\n        }\n    } else if action, ok := itemDict[\"A\"]; ok {\n        item.Action, err = p.actionParser.ParseAction(action)\n        if err != nil {\n            return nil, fmt.Errorf(\"failed to parse action: %w\", err)\n        }\n    }\n    \n    // Extract optional properties\n    if count, ok := itemDict[\"Count\"].(int); ok {\n        item.Count = count\n    }\n    \n    if color, ok := itemDict[\"C\"].([]interface{}); ok && len(color) == 3 {\n        item.C = make([]float64, 3)\n        for i, c := range color {\n            if f, ok := c.(float64); ok {\n                item.C[i] = f\n            }\n        }\n    }\n    \n    if flags, ok := itemDict[\"F\"].(int); ok {\n        item.F = flags\n    }\n    \n    // Store references for tree building\n    item.First = itemDict[\"First\"]\n    item.Last = itemDict[\"Last\"]\n    item.Next = itemDict[\"Next\"]\n    item.Prev = itemDict[\"Prev\"]\n    \n    return item, nil\n}\n\nfunc (p *OutlineParser) parseDestination(dest interface{}) (*Destination, error) {\n    // Handle different destination formats\n    switch d := dest.(type) {\n    case string:\n        // Named destination\n        return &Destination{\n            Type:      NamedDest,\n            NamedDest: d,\n        }, nil\n        \n    case []interface{}:\n        // Explicit destination array\n        if len(d) < 2 {\n            return nil, fmt.Errorf(\"invalid destination array\")\n        }\n        \n        pageRef := d[0]\n        pageNum, err := p.resolver.GetPageNumber(pageRef)\n        if err != nil {\n            return nil, err\n        }\n        \n        viewType, ok := d[1].(string)\n        if !ok {\n            return nil, fmt.Errorf(\"invalid view type in destination\")\n        }\n        \n        dest := &Destination{\n            Type: ExplicitDest,\n            Page: pageNum,\n            View: viewType,\n        }\n        \n        // Extract view parameters based on type\n        switch viewType {\n        case \"XYZ\":\n            if len(d) >= 5 {\n                dest.Parameters = []float64{\n                    toFloat64(d[2]), // left\n                    toFloat64(d[3]), // top\n                    toFloat64(d[4]), // zoom\n                }\n            }\n        case \"Fit\", \"FitB\":\n            // No parameters\n        case \"FitH\", \"FitBH\":\n            if len(d) >= 3 {\n                dest.Parameters = []float64{toFloat64(d[2])} // top\n            }\n        case \"FitV\", \"FitBV\":\n            if len(d) >= 3 {\n                dest.Parameters = []float64{toFloat64(d[2])} // left\n            }\n        case \"FitR\":\n            if len(d) >= 6 {\n                dest.Parameters = []float64{\n                    toFloat64(d[2]), // left\n                    toFloat64(d[3]), // bottom\n                    toFloat64(d[4]), // right\n                    toFloat64(d[5]), // top\n                }\n            }\n        }\n        \n        return dest, nil\n        \n    case map[string]interface{}:\n        // Remote destination dictionary\n        return p.parseRemoteDestination(d)\n        \n    default:\n        return nil, fmt.Errorf(\"unsupported destination type: %T\", dest)\n    }\n}\n\n// internal/pdf/outlines/named_destinations.go\ntype NamedDestinationResolver struct {\n    catalog  map[string]interface{}\n    resolver ObjectResolver\n    cache    map[string]*Destination\n}\n\nfunc (r *NamedDestinationResolver) ResolveNamedDestination(name string) (*Destination, error) {\n    if cached, ok := r.cache[name]; ok {\n        return cached, nil\n    }\n    \n    // Check Dests dictionary in catalog\n    if dests, ok := r.catalog[\"Dests\"]; ok {\n        if destDict, err := r.resolver.ResolveObject(dests); err == nil {\n            if dest, ok := destDict[name]; ok {\n                resolved, err := r.parseDestination(dest)\n                if err == nil {\n                    r.cache[name] = resolved\n                    return resolved, nil\n                }\n            }\n        }\n    }\n    \n    // Check Names tree\n    if names, ok := r.catalog[\"Names\"]; ok {\n        if namesDict, err := r.resolver.ResolveObject(names); err == nil {\n            if destsTree, ok := namesDict[\"Dests\"]; ok {\n                dest, err := r.searchNameTree(destsTree, name)\n                if err == nil && dest != nil {\n                    r.cache[name] = dest\n                    return dest, nil\n                }\n            }\n        }\n    }\n    \n    return nil, fmt.Errorf(\"named destination not found: %s\", name)\n}\n\n// internal/pdf/outlines/api.go\ntype OutlineAPI struct {\n    parser   *OutlineParser\n    resolver *NamedDestinationResolver\n}\n\nfunc (api *OutlineAPI) GetOutlineTree(catalog map[string]interface{}) (*OutlineItem, error) {\n    return api.parser.ParseOutlines(catalog)\n}\n\nfunc (api *OutlineAPI) FlattenOutline(root *OutlineItem) []*FlatOutlineEntry {\n    entries := make([]*FlatOutlineEntry, 0)\n    api.flattenRecursive(root, 0, &entries)\n    return entries\n}\n\ntype FlatOutlineEntry struct {\n    Title       string       `json:\"title\"`\n    Level       int          `json:\"level\"`\n    Page        int          `json:\"page,omitempty\"`\n    Destination *Destination `json:\"destination,omitempty\"`\n    Action      interface{}  `json:\"action,omitempty\"`\n    Color       []float64    `json:\"color,omitempty\"`\n    Style       string       `json:\"style,omitempty\"`\n}\n\nfunc (api *OutlineAPI) flattenRecursive(item *OutlineItem, level int, entries *[]*FlatOutlineEntry) {\n    if item == nil || item.Title == \"\" {\n        return\n    }\n    \n    entry := &FlatOutlineEntry{\n        Title: item.Title,\n        Level: level,\n        Color: item.C,\n    }\n    \n    // Determine style from flags\n    if item.F&1 != 0 {\n        entry.Style = \"italic\"\n    }\n    if item.F&2 != 0 {\n        if entry.Style != \"\" {\n            entry.Style += \",bold\"\n        } else {\n            entry.Style = \"bold\"\n        }\n    }\n    \n    // Resolve destination\n    if item.Dest != nil {\n        entry.Destination = item.Dest\n        if item.Dest.Type == NamedDest {\n            resolved, err := api.resolver.ResolveNamedDestination(item.Dest.NamedDest)\n            if err == nil {\n                entry.Destination = resolved\n                entry.Page = resolved.Page\n            }\n        } else {\n            entry.Page = item.Dest.Page\n        }\n    } else if item.Action != nil {\n        entry.Action = item.Action\n        // Extract page from GoTo action if present\n        if goTo, ok := item.Action.(*actions.GoToAction); ok && goTo.Dest != nil {\n            entry.Page = goTo.Dest.Page\n        }\n    }\n    \n    *entries = append(*entries, entry)\n    \n    // Process children\n    for _, child := range item.Children {\n        api.flattenRecursive(child, level+1, entries)\n    }\n}\n\n// Helper function\nfunc toFloat64(v interface{}) float64 {\n    switch val := v.(type) {\n    case float64:\n        return val\n    case int:\n        return float64(val)\n    case nil:\n        return 0\n    default:\n        return 0\n    }\n}\n```",
        "testStrategy": "Comprehensive testing strategy for document outline/bookmarks support:\n\n1. **Basic Outline Parsing Tests**:\n   - Test with PDFs containing simple linear outline structures\n   - Verify correct parsing of outline items with Title, Dest, and Action\n   - Test with nested outline hierarchies (multiple levels)\n   - Validate parent-child relationships in outline tree\n   - Test with empty outlines dictionary\n\n2. **Destination Handling Tests**:\n   - Test explicit destinations with all view types (XYZ, Fit, FitH, FitV, FitR, FitB, FitBH, FitBV)\n   - Verify correct parameter extraction for each view type\n   - Test named destinations with Dests dictionary lookup\n   - Test named destinations with Names tree lookup\n   - Test remote destinations (GoToR actions)\n   - Verify page number resolution for all destination types\n\n3. **Complex Outline Tests**:\n   - Test with PDF specifications that have deep outline hierarchies\n   - Verify Count field handling (positive for open, negative for closed)\n   - Test color (C) array parsing for colored bookmarks\n   - Test flags (F) for italic (1) and bold (2) styling\n   - Test outline items with both Dest and A (action should take precedence)\n\n4. **Tree Navigation Tests**:\n   - Test First/Last/Next/Prev reference following\n   - Verify circular reference detection\n   - Test maximum depth protection (prevent stack overflow)\n   - Validate sibling relationships in outline tree\n\n5. **Integration Tests**:\n   - Test outline extraction with real PDF documents\n   - Verify flattened outline generation with correct levels\n   - Test outline navigation to actual page destinations\n   - Validate action execution from outline items\n   - Test with PDFs from different generators (Adobe, LibreOffice, etc.)\n\n6. **Error Handling Tests**:\n   - Test with malformed outline dictionaries\n   - Test with invalid destination arrays\n   - Test with missing required fields (Title)\n   - Test with broken object references\n   - Verify graceful handling of corrupted outline structures\n\n7. **Performance Tests**:\n   - Benchmark outline parsing for documents with 1000+ bookmarks\n   - Test memory usage with deeply nested outlines\n   - Verify efficient caching of named destination lookups",
        "status": "pending",
        "dependencies": [
          2,
          3,
          34,
          40
        ],
        "priority": "low",
        "subtasks": []
      },
      {
        "id": 42,
        "title": "Fix PDF Parsing Robustness and Error Recovery",
        "description": "Improve error handling and recovery mechanisms throughout the PDF parsing pipeline to handle malformed PDFs gracefully. Add comprehensive panic recovery, better error reporting, and fallback strategies when encountering invalid stream objects, corrupted data, or spec violations.",
        "details": "Implement a comprehensive error handling and recovery system that makes the PDF parser resilient to malformed documents and spec violations:\n\n```go\n// internal/pdf/errors/types.go\npackage errors\n\nimport (\n    \"fmt\"\n    \"runtime/debug\"\n)\n\ntype PDFError struct {\n    Type        ErrorType\n    Message     string\n    Context     string\n    Offset      int64\n    ObjectNum   int\n    GenNum      int\n    Recoverable bool\n    StackTrace  string\n}\n\ntype ErrorType int\n\nconst (\n    ErrorTypeInvalidHeader ErrorType = iota\n    ErrorTypeCorruptedXRef\n    ErrorTypeMalformedObject\n    ErrorTypeInvalidStream\n    ErrorTypeMissingObject\n    ErrorTypeCircularReference\n    ErrorTypeInvalidEncoding\n    ErrorTypeCorruptedData\n)\n\n// internal/pdf/parser/recovery.go\npackage parser\n\ntype RecoveryStrategy interface {\n    Recover(err *PDFError, context ParseContext) (interface{}, error)\n    CanRecover(err *PDFError) bool\n}\n\ntype ParseContext struct {\n    Reader      io.ReadSeeker\n    XRef        *XRefTable\n    ObjectCache map[string]interface{}\n    Options     *ParseOptions\n}\n\ntype ParseOptions struct {\n    StrictMode          bool\n    MaxRecoveryAttempts int\n    EnableFallbacks     bool\n    SkipCorruptedPages  bool\n    RepairXRef          bool\n}\n\n// internal/pdf/parser/robust_parser.go\ntype RobustParser struct {\n    parser      *PDFParser\n    recovery    map[ErrorType]RecoveryStrategy\n    errorLog    []PDFError\n    options     ParseOptions\n}\n\nfunc (rp *RobustParser) ParseWithRecovery() (*Document, error) {\n    defer func() {\n        if r := recover(); r != nil {\n            err := &PDFError{\n                Type:       ErrorTypeCorruptedData,\n                Message:    fmt.Sprintf(\"Parser panic: %v\", r),\n                StackTrace: string(debug.Stack()),\n                Recoverable: false,\n            }\n            rp.errorLog = append(rp.errorLog, *err)\n        }\n    }()\n    \n    doc, err := rp.parser.Parse()\n    if err != nil {\n        return rp.attemptRecovery(err)\n    }\n    return doc, nil\n}\n\n// internal/pdf/parser/stream_recovery.go\ntype StreamRecovery struct {\n    fallbackFilters []string\n    maxRetries      int\n}\n\nfunc (sr *StreamRecovery) RecoverCorruptedStream(obj *StreamObject, err error) ([]byte, error) {\n    // Try alternative decompression methods\n    for _, filter := range sr.fallbackFilters {\n        if data, err := sr.tryFilter(obj.RawData, filter); err == nil {\n            return data, nil\n        }\n    }\n    \n    // Try to extract readable portions\n    if partial := sr.extractReadableData(obj.RawData); len(partial) > 0 {\n        return partial, fmt.Errorf(\"partial stream recovery: %w\", err)\n    }\n    \n    return nil, fmt.Errorf(\"stream unrecoverable: %w\", err)\n}\n\n// internal/pdf/parser/xref_recovery.go\ntype XRefRecovery struct {\n    scanner *bufio.Scanner\n}\n\nfunc (xr *XRefRecovery) RebuildXRef(reader io.ReadSeeker) (*XRefTable, error) {\n    // Scan entire file for obj/endobj pairs\n    objects := xr.scanForObjects(reader)\n    \n    // Build new xref table\n    xref := NewXRefTable()\n    for _, obj := range objects {\n        xref.AddEntry(obj.Number, obj.Generation, obj.Offset)\n    }\n    \n    // Try to find trailer\n    trailer := xr.findTrailer(reader)\n    if trailer == nil {\n        trailer = xr.reconstructTrailer(objects)\n    }\n    \n    return xref, nil\n}\n\n// internal/pdf/parser/object_recovery.go\ntype ObjectRecovery struct {\n    validator ObjectValidator\n}\n\nfunc (or *ObjectRecovery) RecoverMalformedObject(data []byte, expectedType string) (interface{}, error) {\n    // Try lenient parsing\n    if obj, err := or.parseLenient(data); err == nil {\n        if or.validator.IsValidType(obj, expectedType) {\n            return obj, nil\n        }\n    }\n    \n    // Try type coercion\n    if coerced := or.tryCoercion(data, expectedType); coerced != nil {\n        return coerced, nil\n    }\n    \n    // Return default value for type\n    return or.getDefaultValue(expectedType), fmt.Errorf(\"object recovery: using default\")\n}\n\n// internal/pdf/parser/error_reporter.go\ntype ErrorReporter struct {\n    errors   []PDFError\n    warnings []PDFError\n    logger   *log.Logger\n}\n\nfunc (er *ErrorReporter) Report(err PDFError) {\n    if err.Recoverable {\n        er.warnings = append(er.warnings, err)\n        er.logger.Warn(\"PDF warning\", \n            \"type\", err.Type,\n            \"message\", err.Message,\n            \"context\", err.Context,\n            \"offset\", err.Offset)\n    } else {\n        er.errors = append(er.errors, err)\n        er.logger.Error(\"PDF error\",\n            \"type\", err.Type,\n            \"message\", err.Message,\n            \"context\", err.Context,\n            \"offset\", err.Offset,\n            \"stack\", err.StackTrace)\n    }\n}\n\nfunc (er *ErrorReporter) GenerateReport() *ParseReport {\n    return &ParseReport{\n        Errors:      er.errors,\n        Warnings:    er.warnings,\n        ErrorCount:  len(er.errors),\n        WarningCount: len(er.warnings),\n        Recoverable: er.calculateRecoverability(),\n    }\n}\n\n// Example usage in tools\nfunc (t *PDFExtractTool) ExtractWithRecovery(path string) (*ExtractResult, error) {\n    options := ParseOptions{\n        StrictMode:          false,\n        MaxRecoveryAttempts: 3,\n        EnableFallbacks:     true,\n        SkipCorruptedPages:  true,\n        RepairXRef:          true,\n    }\n    \n    parser := NewRobustParser(path, options)\n    doc, err := parser.ParseWithRecovery()\n    \n    if err != nil && !parser.IsPartialSuccess() {\n        return nil, fmt.Errorf(\"unrecoverable PDF errors: %w\", err)\n    }\n    \n    result := &ExtractResult{\n        Content:     doc.ExtractableContent(),\n        ParseReport: parser.GetReport(),\n        Partial:     parser.IsPartialSuccess(),\n    }\n    \n    return result, nil\n}\n```",
        "testStrategy": "Comprehensive testing strategy for PDF parsing robustness and error recovery:\n\n1. **Panic Recovery Tests**:\n   - Test with PDFs that trigger panics in various parser components\n   - Verify panic recovery doesn't lose parser state\n   - Test nested panic scenarios\n   - Validate stack trace capture and reporting\n\n2. **Malformed PDF Tests**:\n   - Test with PDFs missing required headers (%PDF-)\n   - Test with truncated files at various points\n   - Test with corrupted cross-reference tables\n   - Test with invalid object definitions\n   - Test with circular object references\n   - Test with missing endobj/endstream markers\n\n3. **Stream Recovery Tests**:\n   - Test with corrupted compressed streams\n   - Test with incorrect stream length declarations\n   - Test with missing stream filters\n   - Test with partially readable stream data\n   - Verify fallback decompression strategies\n\n4. **XRef Recovery Tests**:\n   - Test XRef table reconstruction from object scanning\n   - Test with missing or corrupted trailer dictionaries\n   - Test with broken Prev chains in incremental updates\n   - Verify object offset discovery through file scanning\n\n5. **Object Recovery Tests**:\n   - Test recovery of malformed dictionaries (missing >>)\n   - Test recovery of corrupted arrays (missing ])\n   - Test type coercion for common mistakes (string vs name)\n   - Test default value generation for missing required fields\n\n6. **Error Reporting Tests**:\n   - Verify comprehensive error logging with context\n   - Test error categorization (recoverable vs fatal)\n   - Test report generation with statistics\n   - Verify error messages are actionable\n\n7. **Integration Tests**:\n   - Test with real-world corrupted PDFs from various sources\n   - Test with PDFs from legacy generators with known issues\n   - Test partial extraction success scenarios\n   - Verify tools continue processing after recoverable errors\n\n8. **Performance Tests**:\n   - Ensure recovery mechanisms don't significantly impact parsing speed\n   - Test memory usage with large corrupted files\n   - Verify recovery attempt limits work correctly",
        "status": "done",
        "dependencies": [
          2,
          3,
          4,
          6,
          33,
          34
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 43,
        "title": "Fix Critical Server Crashes in PDF Extraction Commands",
        "description": "Add comprehensive panic recovery, timeout handling, and memory management to the pdf_extract_complete and pdf_extract_semantic commands to prevent server crashes and timeouts that make the server unstable and unusable.",
        "details": "Implement critical stability fixes for the PDF extraction commands that are causing server crashes:\n\n```go\n// internal/tools/pdf_extract_complete.go\npackage tools\n\nimport (\n    \"context\"\n    \"runtime\"\n    \"runtime/debug\"\n    \"sync\"\n    \"time\"\n    \"github.com/yourusername/pdfextract/internal/pdf/errors\"\n)\n\ntype PDFExtractComplete struct {\n    memoryMonitor *MemoryMonitor\n    panicHandler  *PanicRecoveryHandler\n    timeout       time.Duration\n}\n\n// Wrap extraction with panic recovery\nfunc (p *PDFExtractComplete) ExtractWithRecovery(ctx context.Context, path string) (result interface{}, err error) {\n    // Set memory limit to prevent OOM\n    runtime.MemoryLimit(2 * 1024 * 1024 * 1024) // 2GB limit\n    \n    // Defer panic recovery\n    defer func() {\n        if r := recover(); r != nil {\n            err = &errors.PDFError{\n                Type:       errors.ErrorTypePanic,\n                Message:    fmt.Sprintf(\"Panic during extraction: %v\", r),\n                StackTrace: string(debug.Stack()),\n                Recoverable: false,\n            }\n            // Log panic details\n            p.panicHandler.LogPanic(r, debug.Stack())\n        }\n    }()\n    \n    // Create timeout context\n    timeoutCtx, cancel := context.WithTimeout(ctx, p.timeout)\n    defer cancel()\n    \n    // Monitor memory usage\n    stopMonitor := p.memoryMonitor.Start(timeoutCtx)\n    defer stopMonitor()\n    \n    // Run extraction in goroutine with result channel\n    type extractResult struct {\n        data interface{}\n        err  error\n    }\n    \n    resultChan := make(chan extractResult, 1)\n    \n    go func() {\n        defer func() {\n            if r := recover(); r != nil {\n                resultChan <- extractResult{\n                    err: fmt.Errorf(\"extraction panic: %v\", r),\n                }\n            }\n        }()\n        \n        data, err := p.performExtraction(timeoutCtx, path)\n        resultChan <- extractResult{data: data, err: err}\n    }()\n    \n    // Wait for result or timeout\n    select {\n    case res := <-resultChan:\n        return res.data, res.err\n    case <-timeoutCtx.Done():\n        return nil, fmt.Errorf(\"extraction timeout after %v\", p.timeout)\n    }\n}\n\n// Memory monitoring to prevent OOM\ntype MemoryMonitor struct {\n    threshold   uint64\n    checkPeriod time.Duration\n    mu          sync.RWMutex\n}\n\nfunc (m *MemoryMonitor) Start(ctx context.Context) func() {\n    done := make(chan struct{})\n    \n    go func() {\n        ticker := time.NewTicker(m.checkPeriod)\n        defer ticker.Stop()\n        \n        for {\n            select {\n            case <-ticker.C:\n                var memStats runtime.MemStats\n                runtime.ReadMemStats(&memStats)\n                \n                if memStats.Alloc > m.threshold {\n                    // Force GC and check again\n                    runtime.GC()\n                    runtime.ReadMemStats(&memStats)\n                    \n                    if memStats.Alloc > m.threshold {\n                        panic(fmt.Sprintf(\"memory threshold exceeded: %d > %d\", memStats.Alloc, m.threshold))\n                    }\n                }\n            case <-ctx.Done():\n                return\n            case <-done:\n                return\n            }\n        }\n    }()\n    \n    return func() { close(done) }\n}\n\n// internal/tools/pdf_extract_semantic.go\n// Similar implementation for semantic extraction with additional safeguards\n\ntype PDFExtractSemantic struct {\n    extractor     *SemanticExtractor\n    memoryMonitor *MemoryMonitor\n    panicHandler  *PanicRecoveryHandler\n    timeout       time.Duration\n    maxFileSize   int64\n}\n\nfunc (p *PDFExtractSemantic) ExtractWithRecovery(ctx context.Context, path string) (result interface{}, err error) {\n    // Check file size before processing\n    fileInfo, err := os.Stat(path)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to stat file: %w\", err)\n    }\n    \n    if fileInfo.Size() > p.maxFileSize {\n        return nil, fmt.Errorf(\"file too large: %d bytes (max: %d)\", fileInfo.Size(), p.maxFileSize)\n    }\n    \n    // Use streaming parser for large files\n    if fileInfo.Size() > 100*1024*1024 { // 100MB\n        return p.extractStreamingWithRecovery(ctx, path)\n    }\n    \n    // Standard extraction with recovery (similar to complete extraction)\n    // ... implementation similar to PDFExtractComplete\n}\n\n// Streaming extraction for large files\nfunc (p *PDFExtractSemantic) extractStreamingWithRecovery(ctx context.Context, path string) (interface{}, error) {\n    // Open file with limited buffer\n    file, err := os.Open(path)\n    if err != nil {\n        return nil, err\n    }\n    defer file.Close()\n    \n    // Use buffered reader with size limit\n    reader := bufio.NewReaderSize(file, 64*1024) // 64KB buffer\n    \n    // Process in chunks with periodic memory checks\n    chunkSize := 10 * 1024 * 1024 // 10MB chunks\n    buffer := make([]byte, chunkSize)\n    \n    var results []interface{}\n    \n    for {\n        select {\n        case <-ctx.Done():\n            return nil, ctx.Err()\n        default:\n            // Check memory before processing chunk\n            var memStats runtime.MemStats\n            runtime.ReadMemStats(&memStats)\n            if memStats.Alloc > p.memoryMonitor.threshold {\n                runtime.GC()\n                runtime.ReadMemStats(&memStats)\n                if memStats.Alloc > p.memoryMonitor.threshold {\n                    return nil, fmt.Errorf(\"memory limit exceeded during streaming\")\n                }\n            }\n            \n            n, err := reader.Read(buffer)\n            if err == io.EOF {\n                break\n            }\n            if err != nil {\n                return nil, fmt.Errorf(\"read error: %w\", err)\n            }\n            \n            // Process chunk with panic recovery\n            chunkResult, err := p.processChunkWithRecovery(buffer[:n])\n            if err != nil {\n                // Log error but continue processing\n                p.panicHandler.LogError(err)\n                continue\n            }\n            \n            results = append(results, chunkResult)\n        }\n    }\n    \n    return p.mergeResults(results), nil\n}\n\n// Panic recovery handler with logging\ntype PanicRecoveryHandler struct {\n    logger     Logger\n    maxPanics  int\n    panicCount int\n    mu         sync.Mutex\n}\n\nfunc (h *PanicRecoveryHandler) RecoverWithLogging(operation string) {\n    if r := recover(); r != nil {\n        h.mu.Lock()\n        h.panicCount++\n        count := h.panicCount\n        h.mu.Unlock()\n        \n        stack := debug.Stack()\n        h.logger.Error(\"Panic recovered in %s: %v\\nStack: %s\", operation, r, stack)\n        \n        // If too many panics, stop processing\n        if count > h.maxPanics {\n            panic(fmt.Sprintf(\"too many panics (%d), aborting\", count))\n        }\n    }\n}\n\n// Resource cleanup helper\ntype ResourceManager struct {\n    resources []io.Closer\n    mu        sync.Mutex\n}\n\nfunc (r *ResourceManager) Add(resource io.Closer) {\n    r.mu.Lock()\n    defer r.mu.Unlock()\n    r.resources = append(r.resources, resource)\n}\n\nfunc (r *ResourceManager) CleanupAll() {\n    r.mu.Lock()\n    defer r.mu.Unlock()\n    \n    for _, resource := range r.resources {\n        if resource != nil {\n            resource.Close()\n        }\n    }\n    r.resources = nil\n}\n\n// Update MCP server to use safe extraction\n// internal/server/mcp.go\nfunc (s *MCPServer) CallTool(ctx context.Context, name string, args map[string]interface{}) (interface{}, error) {\n    // Wrap all tool calls with panic recovery\n    defer func() {\n        if r := recover(); r != nil {\n            s.logger.Error(\"Tool %s panicked: %v\", name, r)\n            // Return error instead of crashing server\n            err = fmt.Errorf(\"tool execution failed: %v\", r)\n        }\n    }()\n    \n    switch name {\n    case \"pdf_extract_complete\":\n        return s.pdfExtractComplete.ExtractWithRecovery(ctx, args[\"path\"].(string))\n    case \"pdf_extract_semantic\":\n        return s.pdfExtractSemantic.ExtractWithRecovery(ctx, args[\"path\"].(string))\n    default:\n        return s.callToolNormal(ctx, name, args)\n    }\n}",
        "testStrategy": "Comprehensive testing strategy for server crash fixes:\n\n1. **Panic Recovery Tests**:\n   - Create PDFs that trigger known panics (corrupted streams, invalid objects, circular references)\n   - Verify server continues running after panic recovery\n   - Test nested panic scenarios\n   - Validate error messages contain useful debugging information\n   - Test with concurrent extraction requests that panic\n\n2. **Timeout Handling Tests**:\n   - Test with PDFs that have extremely complex content streams\n   - Create PDFs with infinite loops in content parsing\n   - Verify timeout cancels extraction cleanly\n   - Test different timeout values (1s, 10s, 60s)\n   - Ensure partial results are not returned on timeout\n\n3. **Memory Management Tests**:\n   - Test with very large PDFs (500MB+)\n   - Create PDFs with thousands of embedded images\n   - Monitor memory usage during extraction\n   - Verify GC is triggered when threshold approached\n   - Test streaming mode activation for large files\n   - Ensure memory is released after extraction\n\n4. **Stress Testing**:\n   - Run 100 concurrent extractions with mixed PDF types\n   - Include PDFs known to cause issues\n   - Monitor server stability over extended periods\n   - Test with limited memory (e.g., 512MB container)\n   - Verify no memory leaks after repeated extractions\n\n5. **Resource Cleanup Tests**:\n   - Verify all file handles are closed after extraction\n   - Test cleanup on panic, timeout, and normal completion\n   - Monitor open file descriptors during testing\n   - Test with read-only and locked files\n\n6. **Integration Tests**:\n   - Test through MCP protocol with real client\n   - Verify error responses are properly formatted\n   - Test server recovery after tool failure\n   - Ensure other tools remain functional after crash",
        "status": "done",
        "dependencies": [
          2,
          3,
          4,
          6,
          42
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 44,
        "title": "Fix MediaBox Parsing Errors",
        "description": "Implement proper MediaBox parsing that handles various PDF structures including missing MediaBox entries, inherited MediaBox from parent nodes, and different coordinate systems to fix extraction failures affecting table extraction, form extraction, and all advanced features.",
        "details": "Implement robust MediaBox parsing that handles all PDF specification variations and edge cases:\n\n```go\n// internal/pdf/page/mediabox_parser.go\npackage page\n\nimport (\n    \"fmt\"\n    \"github.com/yourusername/pdfextract/internal/pdf\"\n    \"github.com/yourusername/pdfextract/internal/pdf/errors\"\n)\n\ntype MediaBoxParser struct {\n    resolver   pdf.ObjectResolver\n    logger     *log.Logger\n    cache      map[string]*Rectangle\n}\n\ntype Rectangle struct {\n    LLX float64 // Lower-left X\n    LLY float64 // Lower-left Y\n    URX float64 // Upper-right X\n    URY float64 // Upper-right Y\n}\n\n// ParseMediaBox extracts MediaBox with proper inheritance and validation\nfunc (p *MediaBoxParser) ParseMediaBox(pageDict map[string]interface{}, pageTree *PageTreeNode) (*Rectangle, error) {\n    defer func() {\n        if r := recover(); r != nil {\n            p.logger.Error(\"Panic in MediaBox parsing: %v\", r)\n            // Return default US Letter size as fallback\n            return &Rectangle{0, 0, 612, 792}, fmt.Errorf(\"recovered from panic: %v\", r)\n        }\n    }()\n\n    // Check cache first\n    if pageRef, ok := pageDict[\"__ref__\"].(pdf.ObjectRef); ok {\n        cacheKey := fmt.Sprintf(\"%d_%d\", pageRef.Number, pageRef.Generation)\n        if cached, exists := p.cache[cacheKey]; exists {\n            return cached, nil\n        }\n    }\n\n    // Try direct MediaBox in page dictionary\n    if mediaBox := p.extractMediaBox(pageDict); mediaBox != nil {\n        return mediaBox, nil\n    }\n\n    // Walk up the page tree for inherited MediaBox\n    current := pageTree\n    for current != nil {\n        if parentDict := current.Dictionary; parentDict != nil {\n            if mediaBox := p.extractMediaBox(parentDict); mediaBox != nil {\n                return mediaBox, nil\n            }\n        }\n        current = current.Parent\n    }\n\n    // No MediaBox found - use PDF default\n    p.logger.Warn(\"No MediaBox found, using default US Letter size\")\n    return &Rectangle{0, 0, 612, 792}, nil\n}\n\n// extractMediaBox from a dictionary entry\nfunc (p *MediaBoxParser) extractMediaBox(dict map[string]interface{}) *Rectangle {\n    mediaBoxObj, exists := dict[\"MediaBox\"]\n    if !exists {\n        return nil\n    }\n\n    // Handle indirect reference\n    if ref, ok := mediaBoxObj.(pdf.ObjectRef); ok {\n        resolved, err := p.resolver.ResolveObject(ref)\n        if err != nil {\n            p.logger.Debug(\"Failed to resolve MediaBox reference: %v\", err)\n            return nil\n        }\n        mediaBoxObj = resolved\n    }\n\n    // Parse array format [llx lly urx ury]\n    switch v := mediaBoxObj.(type) {\n    case []interface{}:\n        return p.parseRectangleArray(v)\n    case pdf.Array:\n        return p.parseRectangleArray([]interface{}(v))\n    case *pdf.Array:\n        return p.parseRectangleArray([]interface{}(*v))\n    default:\n        p.logger.Debug(\"Unknown MediaBox type: %T\", v)\n        return nil\n    }\n}\n\n// parseRectangleArray converts array to Rectangle with validation\nfunc (p *MediaBoxParser) parseRectangleArray(arr []interface{}) *Rectangle {\n    if len(arr) != 4 {\n        p.logger.Debug(\"Invalid MediaBox array length: %d\", len(arr))\n        return nil\n    }\n\n    rect := &Rectangle{}\n    values := []float64{0, 0, 0, 0}\n\n    // Convert each element to float64\n    for i, val := range arr {\n        switch v := val.(type) {\n        case float64:\n            values[i] = v\n        case int:\n            values[i] = float64(v)\n        case pdf.Number:\n            values[i] = float64(v)\n        case pdf.Integer:\n            values[i] = float64(v)\n        case pdf.Real:\n            values[i] = float64(v)\n        default:\n            // Try string conversion as last resort\n            if str, ok := val.(string); ok {\n                if f, err := strconv.ParseFloat(str, 64); err == nil {\n                    values[i] = f\n                    continue\n                }\n            }\n            p.logger.Debug(\"Invalid MediaBox value type at index %d: %T\", i, val)\n            return nil\n        }\n    }\n\n    rect.LLX = values[0]\n    rect.LLY = values[1]\n    rect.URX = values[2]\n    rect.URY = values[3]\n\n    // Validate rectangle\n    if rect.URX <= rect.LLX || rect.URY <= rect.LLY {\n        p.logger.Warn(\"Invalid MediaBox dimensions: [%f %f %f %f]\", rect.LLX, rect.LLY, rect.URX, rect.URY)\n        // Try to fix common issues\n        if rect.URX < rect.LLX {\n            rect.LLX, rect.URX = rect.URX, rect.LLX\n        }\n        if rect.URY < rect.LLY {\n            rect.LLY, rect.URY = rect.URY, rect.LLY\n        }\n    }\n\n    return rect\n}\n\n// BuildPageTree constructs the page tree hierarchy for inheritance\nfunc (p *MediaBoxParser) BuildPageTree(catalog map[string]interface{}) (*PageTreeNode, error) {\n    pagesRef, ok := catalog[\"Pages\"].(pdf.ObjectRef)\n    if !ok {\n        return nil, fmt.Errorf(\"invalid Pages reference in catalog\")\n    }\n\n    pagesObj, err := p.resolver.ResolveObject(pagesRef)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to resolve Pages: %w\", err)\n    }\n\n    pagesDict, ok := pagesObj.(map[string]interface{})\n    if !ok {\n        return nil, fmt.Errorf(\"Pages is not a dictionary\")\n    }\n\n    return p.buildPageTreeNode(pagesDict, nil)\n}\n\ntype PageTreeNode struct {\n    Type       string\n    Dictionary map[string]interface{}\n    Parent     *PageTreeNode\n    Kids       []*PageTreeNode\n}\n\nfunc (p *MediaBoxParser) buildPageTreeNode(dict map[string]interface{}, parent *PageTreeNode) (*PageTreeNode, error) {\n    node := &PageTreeNode{\n        Dictionary: dict,\n        Parent:     parent,\n    }\n\n    if typeObj, ok := dict[\"Type\"].(pdf.Name); ok {\n        node.Type = string(typeObj)\n    }\n\n    // Process kids for page tree nodes\n    if node.Type == \"Pages\" {\n        if kidsObj, ok := dict[\"Kids\"]; ok {\n            kids := p.resolveKidsArray(kidsObj)\n            for _, kidRef := range kids {\n                kidObj, err := p.resolver.ResolveObject(kidRef)\n                if err != nil {\n                    p.logger.Debug(\"Failed to resolve kid: %v\", err)\n                    continue\n                }\n\n                if kidDict, ok := kidObj.(map[string]interface{}); ok {\n                    kidNode, err := p.buildPageTreeNode(kidDict, node)\n                    if err == nil {\n                        node.Kids = append(node.Kids, kidNode)\n                    }\n                }\n            }\n        }\n    }\n\n    return node, nil\n}\n\n// Integration with extraction tools\nfunc (p *MediaBoxParser) GetPageDimensions(pageNum int, doc *pdf.Document) (*Rectangle, error) {\n    page, err := doc.GetPage(pageNum)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to get page %d: %w\", pageNum, err)\n    }\n\n    pageTree, err := p.BuildPageTree(doc.Catalog)\n    if err != nil {\n        p.logger.Warn(\"Failed to build page tree: %v\", err)\n        pageTree = nil\n    }\n\n    return p.ParseMediaBox(page.Dictionary, p.findPageNode(page, pageTree))\n}\n```\n\nUpdate extraction tools to use the new MediaBox parser:\n\n```go\n// internal/extractors/table_extractor.go\nfunc (te *TableExtractor) ExtractTables(doc *pdf.Document) ([]Table, error) {\n    mediaBoxParser := page.NewMediaBoxParser(doc.Resolver, te.logger)\n    \n    for pageNum := 1; pageNum <= doc.NumPages; pageNum++ {\n        // Get proper page dimensions\n        mediaBox, err := mediaBoxParser.GetPageDimensions(pageNum, doc)\n        if err != nil {\n            te.logger.Warn(\"Failed to get page dimensions for page %d: %v\", pageNum, err)\n            // Continue with default dimensions\n            mediaBox = &page.Rectangle{0, 0, 612, 792}\n        }\n        \n        // Use mediaBox for coordinate calculations\n        te.processPageWithDimensions(pageNum, mediaBox)\n    }\n}\n\n// internal/extractors/form_extractor.go\nfunc (fe *FormExtractor) ExtractForms(doc *pdf.Document) ([]FormField, error) {\n    mediaBoxParser := page.NewMediaBoxParser(doc.Resolver, fe.logger)\n    \n    // Similar integration for form extraction\n}\n```",
        "testStrategy": "Comprehensive testing strategy for MediaBox parsing fixes:\n\n1. **Basic MediaBox Parsing Tests**:\n   - Test with PDFs containing direct MediaBox arrays: `[0 0 612 792]`\n   - Test with indirect object references to MediaBox\n   - Test with missing MediaBox (should use default US Letter)\n   - Test with various numeric types (int, float, string representations)\n\n2. **Inheritance Tests**:\n   - Create test PDFs with MediaBox only in Pages root (not in individual Page objects)\n   - Test multi-level page tree inheritance (Pages -> Pages -> Page)\n   - Test override scenarios where child Page has different MediaBox than parent\n   - Verify correct inheritance order (closest parent wins)\n\n3. **Edge Case Tests**:\n   - Test with inverted coordinates (URX < LLX, URY < LLY)\n   - Test with zero-size MediaBox\n   - Test with extremely large coordinates\n   - Test with negative coordinates\n   - Test with non-standard array lengths (< 4 or > 4 elements)\n\n4. **Error Recovery Tests**:\n   - Test with corrupted MediaBox data\n   - Test with circular references in page tree\n   - Test with missing Pages reference in catalog\n   - Verify panic recovery doesn't crash server\n   - Ensure fallback to default dimensions works\n\n5. **Integration Tests**:\n   - Test table extraction with various MediaBox configurations\n   - Test form field positioning with inherited MediaBox\n   - Test coordinate transformations with non-standard MediaBox origins\n   - Verify all extraction methods work with fixed MediaBox parsing\n\n6. **Performance Tests**:\n   - Test caching effectiveness with large PDFs\n   - Measure parsing time for complex page trees\n   - Verify no memory leaks in recursive tree walking\n\n7. **Specification Compliance**:\n   - Test against PDF 1.4 and 1.7 specification examples\n   - Verify handling of all coordinate system variations\n   - Test with real-world PDFs from different generators (Adobe, LibreOffice, etc.)",
        "status": "done",
        "dependencies": [
          2,
          3,
          4,
          34
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 45,
        "title": "Redesign pdf_analyze_document API for Simplicity",
        "description": "Redesign the pdf_analyze_document tool API to be intuitive and self-configuring, with automatic content extraction, optional parameters with sensible defaults, and clear error messages that guide users to successful document analysis.",
        "details": "Implement a complete API redesign for pdf_analyze_document that prioritizes ease of use and automatic configuration:\n\n```go\n// cmd/pdf_analyze_document/main.go\npackage main\n\nimport (\n    \"github.com/yourusername/pdfextract/internal/analysis\"\n    \"github.com/yourusername/pdfextract/pkg/models\"\n)\n\n// Simplified API with all optional parameters\ntype AnalyzeRequest struct {\n    // Optional parameters with sensible defaults\n    Pages      []int    `json:\"pages,omitempty\"`      // Default: all pages\n    Features   []string `json:\"features,omitempty\"`   // Default: [\"text\", \"structure\", \"metadata\"]\n    MaxDepth   int      `json:\"max_depth,omitempty\"`  // Default: 3 (for structure analysis)\n    Language   string   `json:\"language,omitempty\"`   // Default: \"en\" \n    OutputMode string   `json:\"output_mode,omitempty\"` // Default: \"full\"\n}\n\n// Redesigned main analysis function\nfunc analyzeDocument(pdfPath string, options *AnalyzeRequest) (*DocumentAnalysis, error) {\n    // Apply defaults if options is nil\n    if options == nil {\n        options = &AnalyzeRequest{}\n    }\n    \n    // Apply sensible defaults\n    if len(options.Features) == 0 {\n        options.Features = []string{\"text\", \"structure\", \"metadata\", \"summary\"}\n    }\n    if options.MaxDepth == 0 {\n        options.MaxDepth = 3\n    }\n    if options.Language == \"\" {\n        options.Language = \"en\"\n    }\n    \n    // Create analyzer with auto-configuration\n    analyzer := analysis.NewAutoAnalyzer()\n    \n    // Automatically extract content as needed\n    result, err := analyzer.AnalyzeWithAutoExtraction(pdfPath, options)\n    if err != nil {\n        return nil, wrapErrorWithHelpfulMessage(err, pdfPath)\n    }\n    \n    return result, nil\n}\n\n// internal/analysis/auto_analyzer.go\npackage analysis\n\ntype AutoAnalyzer struct {\n    extractor    *SmartExtractor\n    analyzer     *DocumentAnalyzer\n    errorHandler *UserFriendlyErrorHandler\n}\n\nfunc (a *AutoAnalyzer) AnalyzeWithAutoExtraction(pdfPath string, opts *AnalyzeRequest) (*DocumentAnalysis, error) {\n    // Step 1: Auto-detect what needs to be extracted\n    needed := a.detectNeededContent(opts.Features)\n    \n    // Step 2: Extract only what's needed, with progress feedback\n    content, err := a.extractor.ExtractContent(pdfPath, needed, func(progress float64) {\n        // Optional progress callback\n    })\n    if err != nil {\n        return nil, a.errorHandler.WrapWithContext(err, \"extraction\", pdfPath)\n    }\n    \n    // Step 3: Perform analysis with extracted content\n    analysis := &DocumentAnalysis{\n        DocumentInfo: a.buildDocumentInfo(content),\n        Content:      a.analyzeContent(content, opts),\n        Structure:    a.analyzeStructure(content, opts.MaxDepth),\n        Summary:      a.generateSummary(content, opts.Language),\n    }\n    \n    return analysis, nil\n}\n\n// Simplified error messages with actionable guidance\nfunc wrapErrorWithHelpfulMessage(err error, pdfPath string) error {\n    switch {\n    case errors.Is(err, ErrFileNotFound):\n        return fmt.Errorf(\"PDF file not found at '%s'. Please check the file path and try again\", pdfPath)\n    \n    case errors.Is(err, ErrCorruptedPDF):\n        return fmt.Errorf(\"The PDF appears to be corrupted. Try opening it in a PDF viewer first. If it opens, please report this issue with the file\")\n    \n    case errors.Is(err, ErrPasswordProtected):\n        return fmt.Errorf(\"This PDF is password protected. The tool currently doesn't support encrypted PDFs\")\n    \n    case errors.Is(err, ErrUnsupportedVersion):\n        return fmt.Errorf(\"This PDF uses features from a newer PDF version that aren't supported yet. The tool supports PDF 1.4 and 1.7\")\n    \n    default:\n        return fmt.Errorf(\"Failed to analyze PDF: %v. If this persists, try with a simpler PDF first\", err)\n    }\n}\n\n// Example usage documentation\n/*\nExamples:\n\n1. Analyze entire document with defaults:\n   pdf_analyze_document document.pdf\n\n2. Analyze specific pages:\n   pdf_analyze_document document.pdf --pages 1,2,3\n\n3. Get only text content:\n   pdf_analyze_document document.pdf --features text\n\n4. Get structure up to headings:\n   pdf_analyze_document document.pdf --features structure --max-depth 2\n*/\n\n// pkg/models/analysis.go\ntype DocumentAnalysis struct {\n    DocumentInfo DocumentInfo           `json:\"document_info\"`\n    Content      map[string]interface{} `json:\"content\"`\n    Structure    *DocumentStructure     `json:\"structure,omitempty\"`\n    Summary      *DocumentSummary       `json:\"summary,omitempty\"`\n    Warnings     []string              `json:\"warnings,omitempty\"`\n}\n\ntype DocumentInfo struct {\n    Title       string    `json:\"title\"`\n    Author      string    `json:\"author,omitempty\"`\n    PageCount   int       `json:\"page_count\"`\n    FileSize    int64     `json:\"file_size\"`\n    CreatedDate time.Time `json:\"created_date,omitempty\"`\n    PDFVersion  string    `json:\"pdf_version\"`\n}\n\n// Smart content extraction that handles common issues automatically\ntype SmartExtractor struct {\n    cache        *ContentCache\n    errorRecovery *ErrorRecovery\n}\n\nfunc (se *SmartExtractor) ExtractContent(pdfPath string, needed []string, progress func(float64)) (*ExtractedContent, error) {\n    // Check cache first\n    if cached := se.cache.Get(pdfPath, needed); cached != nil {\n        return cached, nil\n    }\n    \n    // Open PDF with automatic recovery\n    doc, err := se.openPDFWithRecovery(pdfPath)\n    if err != nil {\n        return nil, err\n    }\n    defer doc.Close()\n    \n    content := &ExtractedContent{}\n    \n    // Extract based on what's needed\n    for i, feature := range needed {\n        progress(float64(i) / float64(len(needed)))\n        \n        switch feature {\n        case \"text\":\n            content.Text = se.extractTextWithFallback(doc)\n        case \"structure\":\n            content.Structure = se.detectStructure(doc)\n        case \"metadata\":\n            content.Metadata = se.extractMetadata(doc)\n        }\n    }\n    \n    // Cache for future use\n    se.cache.Store(pdfPath, needed, content)\n    \n    return content, nil\n}",
        "testStrategy": "Comprehensive testing strategy for the simplified pdf_analyze_document API:\n\n1. **Zero-Configuration Tests**:\n   - Test calling with just a PDF path (no options) on various document types\n   - Verify sensible defaults are applied automatically\n   - Ensure full document analysis completes without errors\n   - Test that all default features (text, structure, metadata, summary) are included\n\n2. **Automatic Content Extraction Tests**:\n   - Test that content is extracted automatically when analyze is called\n   - Verify extraction only happens for requested features (efficiency)\n   - Test caching prevents redundant extraction on repeated calls\n   - Verify progress callbacks work during extraction\n\n3. **Error Message Clarity Tests**:\n   - Test with non-existent file path, verify clear \"file not found\" message\n   - Test with corrupted PDF, verify helpful corruption message with guidance\n   - Test with password-protected PDF, verify clear unsupported message\n   - Test with PDF 2.0 file, verify version incompatibility message\n   - Ensure no technical stack traces in user-facing errors\n\n4. **Optional Parameter Tests**:\n   - Test pages parameter: single page, range, non-contiguous pages\n   - Test features parameter: single feature, multiple features, invalid features\n   - Test max_depth parameter for structure analysis limiting\n   - Test language parameter affects summary generation\n   - Verify all parameters truly optional with good defaults\n\n5. **Robustness Tests**:\n   - Test with PDFs that previously caused \"no content elements\" errors\n   - Test with large PDFs (1000+ pages) - should work without configuration\n   - Test with PDFs missing common structures (no text, no metadata)\n   - Test recovery from transient errors during extraction\n\n6. **API Usability Tests**:\n   - Verify command works with just: `pdf_analyze_document file.pdf`\n   - Test that common use cases require no configuration\n   - Verify output is self-explanatory without documentation\n   - Test that warnings are included for non-critical issues\n\n7. **Performance Tests**:\n   - Verify caching improves performance on repeated analysis\n   - Test that partial feature extraction is faster than full\n   - Ensure memory usage is reasonable for large documents\n   - Test concurrent analysis requests don't interfere",
        "status": "in-progress",
        "dependencies": [
          24,
          10,
          42,
          43
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 46,
        "title": "Fix Failing PageRange Tests - PDF Object Parsing and XRef Table Issues",
        "description": "Debug and fix failing tests in the page-range extraction functionality caused by PDF object parsing errors and cross-reference table lookup issues in the streaming parser, ensuring reliable extraction of specific page ranges from large PDF documents.",
        "details": "Investigate and fix the root causes of failing page-range extraction tests by addressing PDF object parsing and cross-reference table issues in the streaming parser:\n\n```go\n// internal/pdf/streaming/object_parser_fix.go\npackage streaming\n\nimport (\n    \"fmt\"\n    \"io\"\n    \"github.com/yourusername/pdfextract/internal/pdf/errors\"\n)\n\n// Enhanced object parser with better error handling\ntype ObjectParser struct {\n    reader      *bufio.Reader\n    xrefTable   *XRefTable\n    objectCache *LRUCache\n    logger      *log.Logger\n    recovery    *RecoveryHandler\n}\n\n// Fix object parsing to handle edge cases\nfunc (p *ObjectParser) ParseObject(objNum, genNum int) (PDFObject, error) {\n    defer func() {\n        if r := recover(); r != nil {\n            p.logger.Error(\"Panic in object parsing: %v\", r)\n            p.recovery.RecoverFromPanic(r)\n        }\n    }()\n    \n    // Step 1: Validate object reference\n    if objNum < 0 || genNum < 0 {\n        return nil, fmt.Errorf(\"invalid object reference: %d %d R\", objNum, genNum)\n    }\n    \n    // Step 2: Check cache first\n    cacheKey := fmt.Sprintf(\"%d_%d\", objNum, genNum)\n    if cached := p.objectCache.Get(cacheKey); cached != nil {\n        return cached.(PDFObject), nil\n    }\n    \n    // Step 3: Lookup in xref table with validation\n    offset, err := p.xrefTable.GetObjectOffset(objNum, genNum)\n    if err != nil {\n        // Try recovery strategies\n        if recoveredOffset := p.tryRecoverObjectOffset(objNum, genNum); recoveredOffset > 0 {\n            offset = recoveredOffset\n        } else {\n            return nil, fmt.Errorf(\"object %d %d not found in xref: %w\", objNum, genNum, err)\n        }\n    }\n    \n    // Step 4: Seek to object position with bounds checking\n    currentPos, _ := p.reader.Seek(0, io.SeekCurrent)\n    if _, err := p.reader.Seek(offset, io.SeekStart); err != nil {\n        return nil, fmt.Errorf(\"failed to seek to object at offset %d: %w\", offset, err)\n    }\n    defer p.reader.Seek(currentPos, io.SeekStart) // Restore position\n    \n    // Step 5: Parse object with enhanced error handling\n    obj, err := p.parseObjectAtCurrentPosition(objNum, genNum)\n    if err != nil {\n        return nil, err\n    }\n    \n    // Cache successful parse\n    p.objectCache.Put(cacheKey, obj)\n    return obj, nil\n}\n\n// Fix xref table parsing for compressed objects\nfunc (p *ObjectParser) parseCompressedXRef() (*XRefTable, error) {\n    // Handle compressed xref streams (PDF 1.5+)\n    streamObj, err := p.parseStreamObject()\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to parse xref stream: %w\", err)\n    }\n    \n    // Decode predictor if present\n    if predictor := streamObj.Dict.GetInt(\"Predictor\"); predictor > 1 {\n        streamObj.Data = p.decodePrediction(streamObj.Data, predictor)\n    }\n    \n    // Parse W array for field widths\n    wArray := streamObj.Dict.GetArray(\"W\")\n    if len(wArray) != 3 {\n        return nil, fmt.Errorf(\"invalid W array in xref stream\")\n    }\n    \n    fieldWidths := make([]int, 3)\n    for i, w := range wArray {\n        fieldWidths[i] = w.(int)\n    }\n    \n    // Parse entries with proper byte alignment\n    return p.parseXRefEntries(streamObj.Data, fieldWidths)\n}\n\n// internal/pdf/streaming/xref_recovery.go\ntype XRefRecovery struct {\n    reader *bufio.Reader\n    logger *log.Logger\n}\n\n// Implement xref table recovery for corrupted PDFs\nfunc (r *XRefRecovery) RecoverXRefTable() (*XRefTable, error) {\n    r.logger.Info(\"Attempting xref table recovery\")\n    \n    // Strategy 1: Scan for xref keyword\n    xrefOffset := r.scanForXRefKeyword()\n    if xrefOffset > 0 {\n        if table := r.parseXRefAt(xrefOffset); table != nil {\n            return table, nil\n        }\n    }\n    \n    // Strategy 2: Rebuild from object scanning\n    r.logger.Info(\"Rebuilding xref table from object scan\")\n    return r.rebuildXRefFromObjects()\n}\n\nfunc (r *XRefRecovery) rebuildXRefFromObjects() (*XRefTable, error) {\n    table := NewXRefTable()\n    offset := int64(0)\n    \n    // Scan file for object definitions\n    scanner := bufio.NewScanner(r.reader)\n    scanner.Split(scanPDFTokens)\n    \n    for scanner.Scan() {\n        token := scanner.Text()\n        if isObjectDefinition(token) {\n            objNum, genNum := parseObjectDef(token)\n            table.AddEntry(objNum, genNum, offset, 'n')\n        }\n        offset += int64(len(token))\n    }\n    \n    return table, nil\n}\n\n// internal/pdf/pagerange/extractor_fix.go\n// Fix page range extraction to handle xref issues\nfunc (e *PageRangeExtractor) ExtractPages(ranges []PageRange) (*ExtractedContent, error) {\n    // Add recovery mechanism\n    defer func() {\n        if r := recover(); r != nil {\n            e.logger.Error(\"Panic during page extraction: %v\", r)\n            // Attempt graceful recovery\n            e.resetParserState()\n        }\n    }()\n    \n    // Validate xref table before extraction\n    if err := e.validateXRefTable(); err != nil {\n        e.logger.Warn(\"XRef table validation failed, attempting recovery: %v\", err)\n        if recoveredTable, err := e.recoverXRefTable(); err == nil {\n            e.streamParser.SetXRefTable(recoveredTable)\n        } else {\n            return nil, fmt.Errorf(\"xref table recovery failed: %w\", err)\n        }\n    }\n    \n    // Extract with enhanced error handling\n    content := &ExtractedContent{\n        Pages: make([]PageContent, 0),\n    }\n    \n    for _, pageRange := range ranges {\n        for pageNum := pageRange.Start; pageNum <= pageRange.End; pageNum++ {\n            pageContent, err := e.extractSinglePage(pageNum)\n            if err != nil {\n                e.logger.Error(\"Failed to extract page %d: %v\", pageNum, err)\n                // Continue with other pages instead of failing completely\n                continue\n            }\n            content.Pages = append(content.Pages, pageContent)\n        }\n    }\n    \n    if len(content.Pages) == 0 {\n        return nil, fmt.Errorf(\"no pages could be extracted\")\n    }\n    \n    return content, nil\n}\n\n// Fix object resolution for page extraction\nfunc (e *PageRangeExtractor) resolvePageObject(pageRef ObjectRef) (*PageObject, error) {\n    // Enhanced object resolution with multiple strategies\n    obj, err := e.streamParser.GetObject(pageRef.ObjNum, pageRef.GenNum)\n    if err != nil {\n        // Try alternative resolution methods\n        if obj = e.tryAlternativeResolution(pageRef); obj == nil {\n            return nil, fmt.Errorf(\"failed to resolve page object %v: %w\", pageRef, err)\n        }\n    }\n    \n    // Validate page object structure\n    pageDict, ok := obj.(Dictionary)\n    if !ok {\n        return nil, fmt.Errorf(\"page object is not a dictionary\")\n    }\n    \n    // Ensure required page attributes\n    if pageDict[\"Type\"] != \"/Page\" {\n        return nil, fmt.Errorf(\"object is not a page\")\n    }\n    \n    return &PageObject{\n        Dict:   pageDict,\n        Number: pageRef.ObjNum,\n    }, nil\n}",
        "testStrategy": "Comprehensive testing strategy for fixing page-range extraction issues:\n\n1. **XRef Table Parsing Tests**:\n   - Test with PDFs containing standard xref tables\n   - Test with compressed xref streams (PDF 1.5+)\n   - Test with hybrid xref tables (both standard and compressed)\n   - Test with corrupted xref tables requiring recovery\n   - Verify correct offset calculations for all object types\n\n2. **Object Parsing Edge Cases**:\n   - Test parsing objects at file boundaries\n   - Test with objects containing nested references\n   - Test with circular object references (should detect and break)\n   - Test with missing objects referenced in xref\n   - Test with objects using various compression filters\n\n3. **Page Range Extraction Regression Tests**:\n   - Re-run all existing page-range tests that were failing\n   - Test extraction of first, middle, and last pages\n   - Test extraction of non-contiguous page ranges\n   - Test with PDFs where page objects are stored out of order\n   - Test with inherited page properties from parent nodes\n\n4. **Error Recovery Tests**:\n   - Test with PDFs missing xref tables entirely\n   - Test with partially corrupted object streams\n   - Test recovery continues extraction after encountering bad pages\n   - Verify error messages clearly indicate which pages failed\n   - Test memory usage remains stable during recovery operations\n\n5. **Performance Tests**:\n   - Benchmark page extraction before and after fixes\n   - Verify no performance regression for valid PDFs\n   - Test that caching mechanisms work correctly\n   - Measure overhead of recovery mechanisms\n\n6. **Integration Tests**:\n   - Test page-range extraction with real-world PDFs that previously failed\n   - Verify extracted content matches expected output\n   - Test with PDFs from different generators (Adobe, LibreOffice, etc.)\n   - Test with encrypted PDFs after decryption",
        "status": "pending",
        "dependencies": [
          22,
          19,
          42,
          43
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 47,
        "title": "Refactor Direct PDF Library Usage to Use Wrapper Factory Pattern",
        "description": "Systematically replace all direct PDF library calls throughout the codebase with the wrapper factory pattern established in Task 13, ensuring consistent abstraction and enabling seamless library switching across all PDF operations.",
        "details": "Refactor the entire codebase to eliminate direct PDF library usage and consistently use the wrapper factory pattern:\n\n```go\n// internal/pdf/refactor/migration.go\npackage refactor\n\nimport (\n    \"github.com/yourusername/pdfextract/internal/pdf/wrapper\"\n    \"github.com/yourusername/pdfextract/internal/pdf/factory\"\n)\n\n// Migration tracker to ensure complete refactoring\ntype MigrationTracker struct {\n    filesScanned   int\n    directUsages   []DirectUsage\n    refactored     []RefactoredFile\n    pendingFiles   []string\n}\n\ntype DirectUsage struct {\n    File       string\n    Line       int\n    Import     string\n    Usage      string\n    Suggestion string\n}\n\n// Step 1: Scan for direct library usage\nfunc ScanDirectUsage() (*MigrationTracker, error) {\n    tracker := &MigrationTracker{}\n    \n    // Patterns to detect direct usage\n    directImports := []string{\n        \"github.com/ledongthuc/pdf\",\n        \"github.com/pdfcpu/pdfcpu\",\n        \"github.com/unidoc/unipdf\",\n    }\n    \n    // Scan all Go files\n    err := filepath.Walk(\".\", func(path string, info os.FileInfo, err error) error {\n        if strings.HasSuffix(path, \".go\") {\n            content, _ := ioutil.ReadFile(path)\n            ast, _ := parser.ParseFile(token.NewFileSet(), path, content, 0)\n            \n            // Check imports\n            for _, imp := range ast.Imports {\n                impPath := strings.Trim(imp.Path.Value, \"\\\"\")\n                if contains(directImports, impPath) {\n                    tracker.directUsages = append(tracker.directUsages, DirectUsage{\n                        File:   path,\n                        Import: impPath,\n                    })\n                }\n            }\n        }\n        return nil\n    })\n    \n    return tracker, err\n}\n\n// Step 2: Update imports to use wrapper\nfunc UpdateImports(file string) error {\n    // Replace direct imports with wrapper import\n    replacements := map[string]string{\n        `\"github.com/ledongthuc/pdf\"`: `\"github.com/yourusername/pdfextract/internal/pdf/wrapper\"`,\n        `\"github.com/pdfcpu/pdfcpu\"`:  `\"github.com/yourusername/pdfextract/internal/pdf/wrapper\"`,\n    }\n    \n    content, err := ioutil.ReadFile(file)\n    if err != nil {\n        return err\n    }\n    \n    newContent := string(content)\n    for old, new := range replacements {\n        newContent = strings.Replace(newContent, old, new, -1)\n    }\n    \n    return ioutil.WriteFile(file, []byte(newContent), 0644)\n}\n\n// Step 3: Refactor direct API calls to use wrapper\nfunc RefactorAPICalls(file string) error {\n    // Common refactoring patterns\n    patterns := []RefactorPattern{\n        {\n            // Direct PDF opening\n            Old: `pdf.Open(reader)`,\n            New: `factory.GetPDFLibrary().Open(reader)`,\n        },\n        {\n            // Direct page count\n            Old: `doc.GetPage(n).V.(*pdf.Page)`,\n            New: `doc.GetPage(n)`,\n        },\n        {\n            // Direct text extraction\n            Old: `page.GetPlainText(nil)`,\n            New: `doc.ExtractText(pageNum)`,\n        },\n    }\n    \n    // Apply refactoring patterns\n    content, _ := ioutil.ReadFile(file)\n    newContent := string(content)\n    \n    for _, pattern := range patterns {\n        newContent = strings.Replace(newContent, pattern.Old, pattern.New, -1)\n    }\n    \n    return ioutil.WriteFile(file, []byte(newContent), 0644)\n}\n\n// Step 4: Update specific components\n// internal/pdf/streaming/parser.go\nfunc RefactorStreamingParser() error {\n    // Before:\n    // import \"github.com/ledongthuc/pdf\"\n    // reader, _ := pdf.NewReader(file, size)\n    \n    // After:\n    newCode := `\npackage streaming\n\nimport (\n    \"github.com/yourusername/pdfextract/internal/pdf/wrapper\"\n    \"github.com/yourusername/pdfextract/internal/pdf/factory\"\n)\n\ntype Parser struct {\n    pdfLib wrapper.PDFLibrary\n    doc    wrapper.PDFDocument\n}\n\nfunc NewParser(options ...ParserOption) *Parser {\n    p := &Parser{\n        pdfLib: factory.GetPDFLibrary(),\n    }\n    // Apply options\n    for _, opt := range options {\n        opt(p)\n    }\n    return p\n}\n\nfunc (p *Parser) Parse(reader io.Reader) error {\n    doc, err := p.pdfLib.Open(reader)\n    if err != nil {\n        return fmt.Errorf(\"failed to open PDF: %w\", err)\n    }\n    p.doc = doc\n    return nil\n}\n`\n    return ioutil.WriteFile(\"internal/pdf/streaming/parser.go\", []byte(newCode), 0644)\n}\n\n// Step 5: Update extraction tools\n// cmd/pdf_extract_complete/main.go\nfunc RefactorExtractionTools() error {\n    toolFiles := []string{\n        \"cmd/pdf_extract_complete/main.go\",\n        \"cmd/pdf_extract_semantic/main.go\",\n        \"cmd/pdf_analyze_document/main.go\",\n    }\n    \n    for _, file := range toolFiles {\n        // Refactor to use factory pattern\n        err := refactorToolFile(file)\n        if err != nil {\n            return err\n        }\n    }\n    \n    return nil\n}\n\nfunc refactorToolFile(file string) error {\n    template := `\npackage main\n\nimport (\n    \"github.com/yourusername/pdfextract/internal/pdf/factory\"\n    \"github.com/yourusername/pdfextract/internal/pdf/wrapper\"\n)\n\nfunc extractPDF(path string) error {\n    // Get PDF library from factory\n    pdfLib := factory.GetPDFLibrary()\n    \n    // Open document using wrapper\n    doc, err := pdfLib.OpenFile(path)\n    if err != nil {\n        return fmt.Errorf(\"failed to open PDF: %w\", err)\n    }\n    defer doc.Close()\n    \n    // Use wrapper methods for all operations\n    pageCount, err := doc.GetPageCount()\n    if err != nil {\n        return err\n    }\n    \n    for i := 1; i <= pageCount; i++ {\n        text, err := doc.ExtractText(i)\n        if err != nil {\n            log.Printf(\"Failed to extract page %d: %v\", i, err)\n            continue\n        }\n        // Process text...\n    }\n    \n    return nil\n}\n`\n    return ioutil.WriteFile(file, []byte(template), 0644)\n}\n\n// Step 6: Update test files\nfunc RefactorTests() error {\n    // Find all test files using direct PDF libraries\n    testFiles, _ := filepath.Glob(\"**/*_test.go\")\n    \n    for _, file := range testFiles {\n        content, _ := ioutil.ReadFile(file)\n        if strings.Contains(string(content), \"ledongthuc/pdf\") ||\n           strings.Contains(string(content), \"pdfcpu\") {\n            // Refactor test to use wrapper\n            refactorTestFile(file)\n        }\n    }\n    \n    return nil\n}\n\n// Step 7: Verify complete migration\nfunc VerifyMigration() (*MigrationReport, error) {\n    report := &MigrationReport{\n        Timestamp: time.Now(),\n    }\n    \n    // Scan for any remaining direct usage\n    remaining, err := ScanDirectUsage()\n    if err != nil {\n        return nil, err\n    }\n    \n    if len(remaining.directUsages) > 0 {\n        report.Status = \"INCOMPLETE\"\n        report.RemainingFiles = remaining.directUsages\n    } else {\n        report.Status = \"COMPLETE\"\n    }\n    \n    // Verify wrapper is used consistently\n    wrapperUsage, _ := scanWrapperUsage()\n    report.WrapperAdoption = wrapperUsage\n    \n    return report, nil\n}\n\n// Configuration for library selection\n// internal/pdf/factory/config.go\nfunc UpdateFactoryConfig() error {\n    config := `\npackage factory\n\nimport (\n    \"github.com/yourusername/pdfextract/internal/pdf/wrapper\"\n    \"github.com/yourusername/pdfextract/internal/pdf/wrapper/ledongthuc\"\n    \"github.com/yourusername/pdfextract/internal/pdf/wrapper/pdfcpu\"\n)\n\nvar (\n    defaultLibrary = \"ledongthuc\"\n    libraries = map[string]func() wrapper.PDFLibrary{\n        \"ledongthuc\": func() wrapper.PDFLibrary { return ledongthuc.New() },\n        \"pdfcpu\":     func() wrapper.PDFLibrary { return pdfcpu.New() },\n    }\n)\n\nfunc GetPDFLibrary(opts ...Option) wrapper.PDFLibrary {\n    cfg := &Config{\n        Library: defaultLibrary,\n    }\n    \n    for _, opt := range opts {\n        opt(cfg)\n    }\n    \n    if factory, ok := libraries[cfg.Library]; ok {\n        return factory()\n    }\n    \n    // Fallback to default\n    return libraries[defaultLibrary]()\n}\n`\n    return ioutil.WriteFile(\"internal/pdf/factory/config.go\", []byte(config), 0644)\n}\n`",
        "testStrategy": "Comprehensive testing strategy for wrapper factory pattern refactoring:\n\n1. **Pre-refactoring baseline tests**:\n   - Run all existing tests and document current pass/fail status\n   - Create performance benchmarks for current direct library usage\n   - Document current API behavior for regression testing\n\n2. **Migration verification tests**:\n   - Test automated scanner correctly identifies all direct library usage\n   - Verify no direct imports remain after refactoring\n   - Ensure all PDF operations use wrapper factory pattern\n   - Test that refactored code compiles without errors\n\n3. **Functional equivalence tests**:\n   - Compare output of refactored code with original for same PDFs\n   - Test all PDF operations (open, read, extract, metadata) work identically\n   - Verify error handling behavior remains consistent\n   - Test edge cases and malformed PDFs behave the same\n\n4. **Factory pattern tests**:\n   - Test library switching works correctly via factory configuration\n   - Verify same PDF operations work with different underlying libraries\n   - Test factory returns correct library implementation\n   - Ensure thread-safety of factory pattern\n\n5. **Integration tests**:\n   - Test all command-line tools work after refactoring\n   - Verify streaming parser uses wrapper correctly\n   - Test page range extraction with wrapper\n   - Ensure MediaBox parsing works through wrapper\n\n6. **Performance tests**:\n   - Compare performance before and after refactoring\n   - Measure overhead introduced by wrapper abstraction\n   - Test memory usage remains similar\n   - Benchmark concurrent PDF processing\n\n7. **Regression tests**:\n   - Run full test suite to ensure no functionality broken\n   - Test with PDF corpus including edge cases\n   - Verify all existing features still work\n   - Test error messages and logging unchanged",
        "status": "pending",
        "dependencies": [
          13,
          42,
          43,
          44,
          45,
          46
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 48,
        "title": "Standardize Error Handling Patterns Across PDF Package",
        "description": "Consolidate WrapperError usage throughout the PDF package, establish consistent error types and messages, and implement a unified error handling strategy that provides clear, actionable error information for all PDF operations.",
        "details": "Implement a comprehensive error handling standardization across the entire PDF package to ensure consistent error reporting and handling:\n\n```go\n// internal/pdf/errors/types.go\npackage errors\n\nimport (\n    \"fmt\"\n    \"runtime\"\n)\n\n// Base error types for PDF operations\ntype ErrorCode string\n\nconst (\n    ErrCodeParsing        ErrorCode = \"PDF_PARSE_ERROR\"\n    ErrCodeExtraction     ErrorCode = \"PDF_EXTRACTION_ERROR\"\n    ErrCodeValidation     ErrorCode = \"PDF_VALIDATION_ERROR\"\n    ErrCodeIO             ErrorCode = \"PDF_IO_ERROR\"\n    ErrCodeMemory         ErrorCode = \"PDF_MEMORY_ERROR\"\n    ErrCodeTimeout        ErrorCode = \"PDF_TIMEOUT_ERROR\"\n    ErrCodeUnsupported    ErrorCode = \"PDF_UNSUPPORTED_ERROR\"\n    ErrCodeCorrupted      ErrorCode = \"PDF_CORRUPTED_ERROR\"\n    ErrCodeConfiguration  ErrorCode = \"PDF_CONFIG_ERROR\"\n)\n\n// Standardized WrapperError with enhanced context\ntype WrapperError struct {\n    Code       ErrorCode\n    Message    string\n    Operation  string\n    Details    map[string]interface{}\n    Cause      error\n    StackTrace string\n}\n\nfunc NewWrapperError(code ErrorCode, operation string, message string) *WrapperError {\n    return &WrapperError{\n        Code:       code,\n        Operation:  operation,\n        Message:    message,\n        Details:    make(map[string]interface{}),\n        StackTrace: captureStackTrace(),\n    }\n}\n\nfunc (e *WrapperError) Error() string {\n    if e.Cause != nil {\n        return fmt.Sprintf(\"[%s] %s in %s: %s (caused by: %v)\", \n            e.Code, e.Message, e.Operation, formatDetails(e.Details), e.Cause)\n    }\n    return fmt.Sprintf(\"[%s] %s in %s: %s\", \n        e.Code, e.Message, e.Operation, formatDetails(e.Details))\n}\n\nfunc (e *WrapperError) WithCause(err error) *WrapperError {\n    e.Cause = err\n    return e\n}\n\nfunc (e *WrapperError) WithDetail(key string, value interface{}) *WrapperError {\n    e.Details[key] = value\n    return e\n}\n\n// Error builder for common scenarios\ntype ErrorBuilder struct {\n    code      ErrorCode\n    operation string\n}\n\nfunc For(operation string) *ErrorBuilder {\n    return &ErrorBuilder{operation: operation}\n}\n\nfunc (b *ErrorBuilder) ParseError(format string, args ...interface{}) *WrapperError {\n    return NewWrapperError(ErrCodeParsing, b.operation, fmt.Sprintf(format, args...))\n}\n\nfunc (b *ErrorBuilder) ExtractionError(format string, args ...interface{}) *WrapperError {\n    return NewWrapperError(ErrCodeExtraction, b.operation, fmt.Sprintf(format, args...))\n}\n```\n\n```go\n// internal/pdf/errors/handlers.go\npackage errors\n\nimport (\n    \"context\"\n    \"log\"\n)\n\n// Centralized error handler with logging and metrics\ntype ErrorHandler struct {\n    logger     *log.Logger\n    metrics    MetricsCollector\n    errorHooks []ErrorHook\n}\n\ntype ErrorHook func(err *WrapperError)\n\nfunc (h *ErrorHandler) Handle(ctx context.Context, err error) error {\n    if err == nil {\n        return nil\n    }\n    \n    // Convert to WrapperError if needed\n    wErr, ok := err.(*WrapperError)\n    if !ok {\n        wErr = NewWrapperError(ErrCodeUnknown, \"unknown\", err.Error()).WithCause(err)\n    }\n    \n    // Log with appropriate level\n    h.logError(wErr)\n    \n    // Collect metrics\n    h.metrics.RecordError(wErr.Code, wErr.Operation)\n    \n    // Run hooks (e.g., for alerting)\n    for _, hook := range h.errorHooks {\n        hook(wErr)\n    }\n    \n    return wErr\n}\n\n// Error recovery middleware\nfunc RecoverWithError(operation string) func() {\n    return func() {\n        if r := recover(); r != nil {\n            err := NewWrapperError(ErrCodePanic, operation, \"panic recovered\").\n                WithDetail(\"panic\", r).\n                WithDetail(\"stack\", string(debug.Stack()))\n            \n            // Log critical error\n            log.Printf(\"CRITICAL: %v\", err)\n            \n            // Re-panic with wrapped error\n            panic(err)\n        }\n    }\n}\n```\n\n```go\n// internal/pdf/errors/migration.go\npackage errors\n\nimport (\n    \"go/ast\"\n    \"go/parser\"\n    \"go/token\"\n)\n\n// Tool to help migrate existing error handling\ntype ErrorMigrator struct {\n    replacements map[string]string\n    patterns     []ErrorPattern\n}\n\ntype ErrorPattern struct {\n    Match   string\n    Replace string\n    Code    ErrorCode\n}\n\n// Analyze codebase for error handling patterns\nfunc (m *ErrorMigrator) AnalyzePackage(pkgPath string) (*MigrationReport, error) {\n    report := &MigrationReport{\n        PackagePath: pkgPath,\n        Issues:      []ErrorIssue{},\n    }\n    \n    // Parse Go files\n    fset := token.NewFileSet()\n    pkgs, err := parser.ParseDir(fset, pkgPath, nil, parser.ParseComments)\n    if err != nil {\n        return nil, err\n    }\n    \n    // Find error handling patterns\n    for _, pkg := range pkgs {\n        ast.Inspect(pkg, func(n ast.Node) bool {\n            switch x := n.(type) {\n            case *ast.CallExpr:\n                if isErrorCreation(x) {\n                    issue := m.analyzeErrorCreation(fset, x)\n                    if issue != nil {\n                        report.Issues = append(report.Issues, *issue)\n                    }\n                }\n            }\n            return true\n        })\n    }\n    \n    return report, nil\n}\n```\n\n```go\n// internal/pdf/wrapper/error_integration.go\npackage wrapper\n\nimport (\n    \"github.com/yourusername/pdfextract/internal/pdf/errors\"\n)\n\n// Update wrapper to use standardized errors\nfunc (w *PDFWrapper) extractWithErrorHandling(ctx context.Context, options ExtractOptions) (*ExtractResult, error) {\n    // Set up panic recovery\n    defer errors.RecoverWithError(\"PDFWrapper.Extract\")()\n    \n    // Validate input\n    if err := w.validate(); err != nil {\n        return nil, errors.For(\"PDFWrapper.Extract\").\n            ValidationError(\"invalid PDF wrapper state\").\n            WithCause(err).\n            WithDetail(\"options\", options)\n    }\n    \n    // Perform extraction with timeout\n    resultCh := make(chan *ExtractResult, 1)\n    errCh := make(chan error, 1)\n    \n    go func() {\n        defer errors.RecoverWithError(\"PDFWrapper.Extract.worker\")()\n        \n        result, err := w.doExtract(options)\n        if err != nil {\n            errCh <- errors.For(\"PDFWrapper.Extract\").\n                ExtractionError(\"extraction failed\").\n                WithCause(err).\n                WithDetail(\"page_count\", w.PageCount()).\n                WithDetail(\"file_size\", w.FileSize())\n        } else {\n            resultCh <- result\n        }\n    }()\n    \n    select {\n    case result := <-resultCh:\n        return result, nil\n    case err := <-errCh:\n        return nil, err\n    case <-ctx.Done():\n        return nil, errors.For(\"PDFWrapper.Extract\").\n            TimeoutError(\"extraction timed out\").\n            WithCause(ctx.Err()).\n            WithDetail(\"timeout\", options.Timeout)\n    }\n}\n```\n\n```go\n// internal/pdf/errors/messages.go\npackage errors\n\n// Standardized error messages with user-friendly explanations\nvar ErrorMessages = map[ErrorCode]MessageTemplate{\n    ErrCodeParsing: {\n        Short: \"Failed to parse PDF structure\",\n        Long:  \"The PDF file contains invalid or corrupted structure that prevents parsing. This may be due to file corruption, unsupported PDF version, or non-standard formatting.\",\n        Suggestion: \"Try repairing the PDF with a PDF editor or converting it to a newer format.\",\n    },\n    ErrCodeExtraction: {\n        Short: \"Failed to extract content from PDF\",\n        Long:  \"Content extraction failed due to complex layouts, encrypted content, or unsupported features.\",\n        Suggestion: \"Check if the PDF is password-protected or contains scanned images instead of text.\",\n    },\n    ErrCodeMemory: {\n        Short: \"Insufficient memory to process PDF\",\n        Long:  \"The PDF file is too large or complex to process with available memory.\",\n        Suggestion: \"Try processing fewer pages at a time or increase available memory.\",\n    },\n}\n\ntype MessageTemplate struct {\n    Short      string\n    Long       string\n    Suggestion string\n}\n\nfunc (e *WrapperError) UserMessage() string {\n    if template, ok := ErrorMessages[e.Code]; ok {\n        return fmt.Sprintf(\"%s\\n\\nDetails: %s\\n\\nSuggestion: %s\", \n            template.Short, template.Long, template.Suggestion)\n    }\n    return e.Message\n}\n```",
        "testStrategy": "Comprehensive testing strategy for standardized error handling:\n\n1. **Error Type Coverage Tests**:\n   - Create test cases for each ErrorCode type\n   - Verify error messages follow consistent format\n   - Test error wrapping and cause chain preservation\n   - Validate stack trace capture functionality\n   - Test error serialization for API responses\n\n2. **Error Builder Pattern Tests**:\n   ```go\n   func TestErrorBuilder(t *testing.T) {\n       err := errors.For(\"TestOperation\").\n           ParseError(\"invalid object at offset %d\", 1234).\n           WithDetail(\"object_type\", \"stream\").\n           WithDetail(\"expected\", \"dictionary\")\n       \n       assert.Contains(t, err.Error(), \"PDF_PARSE_ERROR\")\n       assert.Contains(t, err.Error(), \"TestOperation\")\n       assert.Equal(t, \"stream\", err.Details[\"object_type\"])\n   }\n   ```\n\n3. **Panic Recovery Tests**:\n   - Test panic recovery in extraction operations\n   - Verify stack traces are captured correctly\n   - Test nested panic scenarios\n   - Ensure panics are converted to WrapperError\n   - Validate error context is preserved through recovery\n\n4. **Migration Tool Tests**:\n   - Test error pattern detection in existing code\n   - Verify migration suggestions are accurate\n   - Test automated refactoring capabilities\n   - Validate no functional changes during migration\n\n5. **Integration Tests**:\n   - Test error propagation through wrapper factory\n   - Verify consistent error handling across all PDF operations\n   - Test error handling in concurrent scenarios\n   - Validate timeout errors include proper context\n   - Test memory limit errors with large PDFs\n\n6. **User Message Tests**:\n   - Verify all error codes have user-friendly messages\n   - Test message formatting with various detail combinations\n   - Validate suggestions are actionable and helpful\n   - Test localization support (if applicable)\n\n7. **Performance Tests**:\n   - Benchmark error creation overhead\n   - Test error handling impact on extraction performance\n   - Verify no memory leaks in error objects\n   - Test error handler under high load\n\n8. **API Response Tests**:\n   - Test error serialization to JSON\n   - Verify HTTP status codes match error types\n   - Test error response structure consistency\n   - Validate client-side error parsing",
        "status": "pending",
        "dependencies": [
          13,
          42,
          43,
          44,
          45,
          46
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-06-20T03:09:52.219Z",
      "updated": "2025-06-20T17:03:44.378Z",
      "description": "Tasks for master context"
    }
  }
}